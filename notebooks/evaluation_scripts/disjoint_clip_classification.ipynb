{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e82daff1",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bc6c677b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/srv/share/gstoica3/miniconda3/envs/open-mmlab/lib/python3.7/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import pdb\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.optimize\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from torch.optim import SGD, Adam, lr_scheduler\n",
    "import torchvision\n",
    "import torchvision.transforms as T\n",
    "\n",
    "from sys import platform\n",
    "\n",
    "DEVICE = 'mps' if platform == 'darwin' else 'cuda'\n",
    "if DEVICE == 'mps':\n",
    "    DOWNLOAD_PATH = '/Users/georgestoica/Downloads' \n",
    "else:\n",
    "    DOWNLOAD_PATH = '/srv/share/gstoica3/checkpoints/REPAIR/'\n",
    "    \n",
    "torch.autograd.set_grad_enabled(False)\n",
    "\n",
    "from copy import deepcopy\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f3c4fd06",
   "metadata": {},
   "outputs": [],
   "source": [
    "from resnets import resnet20\n",
    "from matching_algs import *\n",
    "from model_matchings import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ad5a5f68",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(model, i):\n",
    "    sd = model.state_dict()\n",
    "    path = os.path.join(\n",
    "        # '/Users/georgestoica/Downloads',\n",
    "        DOWNLOAD_PATH,\n",
    "        '%s.pth.tar' % i\n",
    "    )\n",
    "    torch.save(model.state_dict(), path)\n",
    "\n",
    "def load_model(model, i):\n",
    "    path = os.path.join(\n",
    "        # '/Users/georgestoica/Downloads',\n",
    "        DOWNLOAD_PATH,\n",
    "        '%s.pth.tar' % i\n",
    "    )\n",
    "    sd = torch.load(path, map_location=torch.device(DEVICE))\n",
    "    model.load_state_dict(sd)\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f5b319b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "cifar100_info = {\n",
    "    'dir': '/nethome/gstoica3/research/pytorch-cifar100/data/cifar-100-python',\n",
    "    'classes1': np.arange(50),\n",
    "    'classes2': np.arange(50, 100),\n",
    "    'num_classes': 100,\n",
    "    'split_classes': 50,\n",
    "    'wrapper': torchvision.datasets.CIFAR100\n",
    "}\n",
    "\n",
    "cifar10_info = {\n",
    "    'dir': '/tmp',\n",
    "    'classes1': np.array([3, 2, 0, 6, 4]),\n",
    "    'classes2': np.array([5, 7, 9, 8, 1]),\n",
    "    'num_classes': 10,\n",
    "    'split_classes': 5,\n",
    "    'wrapper': torchvision.datasets.CIFAR10\n",
    "}\n",
    "\n",
    "ds_info = cifar100_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "abb29e55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "CIFAR_MEAN = [125.307, 122.961, 113.8575]\n",
    "CIFAR_STD = [51.5865, 50.847, 51.255]\n",
    "normalize = T.Normalize(np.array(CIFAR_MEAN)/255, np.array(CIFAR_STD)/255)\n",
    "denormalize = T.Normalize(-np.array(CIFAR_MEAN)/np.array(CIFAR_STD), 255/np.array(CIFAR_STD))\n",
    "\n",
    "train_transform = T.Compose([\n",
    "    T.RandomHorizontalFlip(),\n",
    "    T.RandomCrop(32, padding=4),\n",
    "    T.ToTensor(),\n",
    "    normalize,\n",
    "])\n",
    "test_transform = T.Compose([\n",
    "    T.ToTensor(),\n",
    "    normalize,\n",
    "])\n",
    "train_dset = ds_info['wrapper'](root=ds_info['dir'], train=True,\n",
    "                                        download=True, transform=train_transform)\n",
    "test_dset = ds_info['wrapper'](root=ds_info['dir'], train=False,\n",
    "                                        download=True, transform=test_transform)\n",
    "\n",
    "train_aug_loader = torch.utils.data.DataLoader(train_dset, batch_size=500, shuffle=True, num_workers=8)\n",
    "test_loader = torch.utils.data.DataLoader(test_dset, batch_size=500, shuffle=False, num_workers=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ddf8d2f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "50000it [00:12, 3928.50it/s]\n",
      "50000it [00:12, 3935.26it/s]\n",
      "10000it [00:01, 6061.08it/s]\n",
      "10000it [00:01, 6092.10it/s]\n"
     ]
    }
   ],
   "source": [
    "train_aug_loader = torch.utils.data.DataLoader(train_dset, batch_size=500, shuffle=True, num_workers=8)\n",
    "test_loader = torch.utils.data.DataLoader(test_dset, batch_size=500, shuffle=False, num_workers=8)\n",
    "\n",
    "model1_classes= ds_info['classes1']#np.array([3, 2, 0, 6, 4])\n",
    "model2_classes = ds_info['classes2']\n",
    "\n",
    "valid_examples1 = [i for i, (_, label) in tqdm(enumerate(train_dset)) if label in model1_classes]\n",
    "valid_examples2 = [i for i, (_, label) in tqdm(enumerate(train_dset)) if label in model2_classes]\n",
    "\n",
    "assert len(set(valid_examples1).intersection(set(valid_examples2))) == 0, 'sets should be disjoint'\n",
    "\n",
    "train_aug_loader1 = torch.utils.data.DataLoader(\n",
    "    torch.utils.data.Subset(train_dset, valid_examples1), batch_size=500, shuffle=True, num_workers=8\n",
    ")\n",
    "train_aug_loader2 = torch.utils.data.DataLoader(\n",
    "    torch.utils.data.Subset(train_dset, valid_examples2), batch_size=500, shuffle=True, num_workers=8\n",
    ")\n",
    "\n",
    "test_valid_examples1 = [i for i, (_, label) in tqdm(enumerate(test_dset)) if label in model1_classes]\n",
    "test_valid_examples2 = [i for i, (_, label) in tqdm(enumerate(test_dset)) if label in model2_classes]\n",
    "\n",
    "test_loader1 = torch.utils.data.DataLoader(\n",
    "    torch.utils.data.Subset(test_dset, test_valid_examples1), batch_size=500, shuffle=False, num_workers=8\n",
    ")\n",
    "test_loader2 = torch.utils.data.DataLoader(\n",
    "    torch.utils.data.Subset(test_dset, test_valid_examples2), batch_size=500, shuffle=False, num_workers=8\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "468b1405",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n",
      "        18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,\n",
      "        36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49,  0,  1,  2,  3,\n",
      "         4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21,\n",
      "        22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39,\n",
      "        40, 41, 42, 43, 44, 45, 46, 47, 48, 49])\n"
     ]
    }
   ],
   "source": [
    "class_idxs = np.zeros(ds_info['num_classes'], dtype=int)\n",
    "class_idxs[model1_classes] = np.arange(ds_info['split_classes'])\n",
    "class_idxs[model2_classes] = np.arange(ds_info['split_classes'])\n",
    "class_idxs = torch.from_numpy(class_idxs)\n",
    "print(class_idxs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1e4d47ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluates accuracy\n",
    "def evaluate_texthead(model, loader, class_vectors, remap_class_idxs=None, return_confusion=False):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    totals = [0] * class_vectors.shape[0]\n",
    "    corrects = [0] * class_vectors.shape[0]\n",
    "    \n",
    "    with torch.no_grad(), autocast():\n",
    "        for inputs, labels in loader:\n",
    "            encodings = model(inputs.to(DEVICE))\n",
    "            normed_encodings = encodings / encodings.norm(dim=-1, keepdim=True)\n",
    "            outputs = normed_encodings @ class_vectors.T\n",
    "            pred = outputs.argmax(dim=1)\n",
    "            if remap_class_idxs is not None:\n",
    "                correct += (remap_class_idxs[labels].to(DEVICE) == pred).sum().item()\n",
    "            else:\n",
    "                for gt, p in zip(labels, pred):\n",
    "                    totals[gt] += 1\n",
    "                    \n",
    "                    if gt == p:\n",
    "                        correct += 1\n",
    "                        corrects[gt] += 1\n",
    "                \n",
    "            total += inputs.shape[0]\n",
    "    if return_confusion:\n",
    "        return correct / sum(totals), list(map(lambda a: a[0] / a[1], zip(corrects, totals)))\n",
    "    else:\n",
    "        return correct / total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "0f2c3e03",
   "metadata": {},
   "outputs": [],
   "source": [
    "import clip\n",
    "\n",
    "text_inputs = torch.cat([clip.tokenize(f\"a photo of a {c}\") for c in test_dset.classes]).to(DEVICE)\n",
    "model, preprocess = clip.load('ViT-B/32', DEVICE)\n",
    "with torch.no_grad():\n",
    "    text_features = model.encode_text(text_inputs)\n",
    "\n",
    "\n",
    "text_features /= text_features.norm(dim=-1, keepdim=True)\n",
    "class_vecs1 = text_features[model1_classes]\n",
    "class_vecs2 = text_features[model2_classes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a68cb316",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.778\n",
      "0.7758\n"
     ]
    }
   ],
   "source": [
    "model1 = resnet20(w=4, text_head=True).to(DEVICE)\n",
    "model2 = resnet20(w=4, text_head=True).to(DEVICE)\n",
    "load_model(model1, f'resnet20x4_CIFAR50_clses{model1_classes.tolist()}')\n",
    "load_model(model2, f'resnet20x4_CIFAR50_clses{model2_classes.tolist()}')\n",
    "\n",
    "print(evaluate_texthead(model1, test_loader1, class_vecs1, remap_class_idxs=class_idxs))\n",
    "print(evaluate_texthead(model2, test_loader2, class_vecs2, remap_class_idxs=class_idxs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f91d113e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_transform_differences(old_transforms, current_transforms):\n",
    "    if len(old_transforms) == 0:\n",
    "        return {}\n",
    "    transform2norm = {}\n",
    "    for key, old_transform in old_transforms.items():\n",
    "        current_transform = current_transforms[key]\n",
    "        old_align = old_transform.output_align\n",
    "        new_align = current_transform.output_align\n",
    "        cost = old_align.T @ new_align\n",
    "        row_ind, col_idx = scipy.optimize.linear_sum_assignment(cost.detach().cpu().numpy())\n",
    "        permutation = torch.eye(new_align.shape[1], device=old_align.device)[col_idx]\n",
    "        aligned_new = new_align @ permutation\n",
    "#         pdb.set_trace()\n",
    "        norm = torch.norm(old_align - aligned_new).cpu().numpy()\n",
    "        transform2norm[key] = norm\n",
    "    return transform2norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "6e9d5a78",
   "metadata": {},
   "outputs": [],
   "source": [
    "r = 0.5\n",
    "fn = match_tensors_exact_bipartite\n",
    "set_r(r)\n",
    "set_match_fn(fn)\n",
    "\n",
    "match_tensors = match_wrapper(\n",
    "    fn, \n",
    "    backend_alg=match_tensors_exact_bipartite,\n",
    "    interleave=True, \n",
    "    random_perm=False\n",
    ")\n",
    "layer_transform = lambda : LayerTransform(normalize_tensors=True, tensor_merge_type='concat')\n",
    "old_state_dict = {}\n",
    "state_dict = {}\n",
    "old_transforms = defaultdict(lambda: layer_transform())\n",
    "new_transforms = defaultdict(lambda: layer_transform())\n",
    "modelc = resnet20(w=4, text_head=True).to(DEVICE)\n",
    "accuracies = []\n",
    "steps = []\n",
    "distances = []\n",
    "best_info = {'acc': 0., 'dist': np.inf}\n",
    "step = 1\n",
    "is_converged = False\n",
    "prev_distance = np.inf\n",
    "same_window = 5\n",
    "same_span = 0\n",
    "while not is_converged:\n",
    "# for step in tqdm(range(121)):\n",
    "    old_transforms = new_transforms\n",
    "    old_state_dict = deepcopy(state_dict)\n",
    "    new_transforms = merge_resnet20(\n",
    "        state_dict, \n",
    "        model1, \n",
    "        model2, \n",
    "        transforms=deepcopy(old_transforms),\n",
    "        concat_head=False\n",
    "    )\n",
    "    if step == 0:\n",
    "        original_computation = deepcopy(new_transforms)\n",
    "\n",
    "    transform2dist = find_transform_differences(old_transforms, new_transforms)\n",
    "    avg_distance = np.mean(list(transform2dist.values()))\n",
    "    \n",
    "    if abs(avg_distance - prev_distance) <= 1e-5:\n",
    "        same_span += 1\n",
    "    else:\n",
    "        same_span = 0\n",
    "    if same_span >= same_window:\n",
    "        is_converged = True\n",
    "        \n",
    "    prev_distance = avg_distance\n",
    "    if is_converged or step >= 1000:\n",
    "        break\n",
    "    step += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "04c0412c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "3c234100",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3818"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modelc.load_state_dict(state_dict)\n",
    "reset_bn_stats(modelc, loader=train_aug_loader)\n",
    "acc, perclass_acc = evaluate_texthead(\n",
    "    modelc, test_loader, class_vectors=text_features, return_confusion=True\n",
    ")\n",
    "acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "cd30e758",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.13, 0.04, 0.18, 0.32, 0.33, 0.41, 0.31, 0.29, 0.61, 0.48, 0.27, 0.4, 0.33, 0.11, 0.26, 0.32, 0.27, 0.69, 0.52, 0.36, 0.79, 0.08, 0.27, 0.46, 0.65, 0.34, 0.19, 0.13, 0.37, 0.28, 0.3, 0.14, 0.15, 0.4, 0.39, 0.39, 0.56, 0.5, 0.24, 0.44, 0.35, 0.31, 0.58, 0.48, 0.26, 0.19, 0.16, 0.25, 0.65, 0.43, 0.24, 0.49, 0.5, 0.74, 0.54, 0.1, 0.79, 0.45, 0.79, 0.39, 0.37, 0.52, 0.43, 0.34, 0.29, 0.1, 0.41, 0.21, 0.68, 0.62, 0.38, 0.5, 0.4, 0.51, 0.17, 0.36, 0.64, 0.17, 0.59, 0.28, 0.08, 0.72, 0.29, 0.53, 0.14, 0.61, 0.4, 0.56, 0.28, 0.51, 0.46, 0.22, 0.33, 0.19, 0.58, 0.56, 0.14, 0.34, 0.42, 0.46]\n"
     ]
    }
   ],
   "source": [
    "print(perclass_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60b838b2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
