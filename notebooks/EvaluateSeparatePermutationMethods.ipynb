{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cc2907a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/srv/share/gstoica3/miniconda3/envs/open-mmlab/lib/python3.7/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import scipy.optimize\n",
    "import os\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from torch.optim import SGD, lr_scheduler\n",
    "import torchvision\n",
    "import torchvision.transforms as T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ea725609",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(model, i):\n",
    "    sd = model.state_dict()\n",
    "    torch.save(model.state_dict(), os.path.join('/srv/share4/gstoica3/checkpoints/REPAIR/', '%s.pt' % i))\n",
    "\n",
    "def load_model(model, i):\n",
    "    sd = torch.load(os.path.join('/srv/share4/gstoica3/checkpoints/REPAIR', '%s.pt' % i))\n",
    "    model.load_state_dict(sd)\n",
    "    \n",
    "def evaluate(model):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    with torch.no_grad(), autocast():\n",
    "        for inputs, labels in test_loader:\n",
    "            outputs = model(inputs.cuda())\n",
    "            pred = outputs.argmax(dim=1)\n",
    "            correct += (labels.cuda() == pred).sum().item()\n",
    "    return correct / len(test_loader.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "28847535",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to /tmp/cifar-10-python.tar.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████| 170498071/170498071 [00:01<00:00, 89659485.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /tmp/cifar-10-python.tar.gz to /tmp\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "CIFAR_MEAN = [125.307, 122.961, 113.8575]\n",
    "CIFAR_STD = [51.5865, 50.847, 51.255]\n",
    "normalize = T.Normalize(np.array(CIFAR_MEAN)/255, np.array(CIFAR_STD)/255)\n",
    "denormalize = T.Normalize(-np.array(CIFAR_MEAN)/np.array(CIFAR_STD), 255/np.array(CIFAR_STD))\n",
    "\n",
    "train_transform = T.Compose([\n",
    "    T.RandomHorizontalFlip(),\n",
    "    T.RandomCrop(32, padding=4),\n",
    "    T.ToTensor(),\n",
    "    normalize,\n",
    "])\n",
    "test_transform = T.Compose([\n",
    "    T.ToTensor(),\n",
    "    normalize,\n",
    "])\n",
    "train_dset = torchvision.datasets.CIFAR10(root='/tmp', train=True,\n",
    "                                        download=True, transform=train_transform)\n",
    "test_dset = torchvision.datasets.CIFAR10(root='/tmp', train=False,\n",
    "                                        download=True, transform=test_transform)\n",
    "\n",
    "train_aug_loader = torch.utils.data.DataLoader(train_dset, batch_size=500, shuffle=True, num_workers=0)\n",
    "test_loader = torch.utils.data.DataLoader(test_dset, batch_size=500, shuffle=False, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "321b782a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/kuangliu/pytorch-cifar/blob/master/models/vgg.py\n",
    "cfg = {\n",
    "    'VGG11': [64, 'M', 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512, 'M'],\n",
    "    'VGG13': [64, 64, 'M', 128, 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512, 'M'],\n",
    "    'VGG16': [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 'M', 512, 512, 512, 'M', 512, 512, 512, 'M'],\n",
    "    'VGG19': [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 256, 'M', 512, 512, 512, 512, 'M', 512, 512, 512, 512, 'M'],\n",
    "}\n",
    "\n",
    "class VGG(nn.Module):\n",
    "    def __init__(self, vgg_name, w=1):\n",
    "        super(VGG, self).__init__()\n",
    "        self.w = w\n",
    "        self.features = self._make_layers(cfg[vgg_name])\n",
    "        self.classifier = nn.Linear(self.w*512, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.features(x)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.classifier(out)\n",
    "        return out\n",
    "\n",
    "    def _make_layers(self, cfg):\n",
    "        layers = []\n",
    "        in_channels = 3\n",
    "        for x in cfg:\n",
    "            if x == 'M':\n",
    "                layers += [nn.MaxPool2d(kernel_size=2, stride=2)]\n",
    "            else:\n",
    "                layers.append(nn.Conv2d(in_channels if in_channels == 3 else self.w*in_channels,\n",
    "                                     self.w*x, kernel_size=3, padding=1))\n",
    "                layers.append(nn.ReLU(inplace=True))\n",
    "                in_channels = x\n",
    "        layers += [nn.AvgPool2d(kernel_size=1, stride=1)]\n",
    "        return nn.Sequential(*layers)\n",
    "    \n",
    "def vgg11(w=1):\n",
    "    return VGG('VGG11', w).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3137d3cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Given two networks net0, net1 which each output a feature map of shape NxCxWxH,\n",
    "# this will reshape both outputs to (N*W*H)xC\n",
    "# and then compute a CxC correlation matrix between the two\n",
    "def run_corr_matrix(net0, net1):\n",
    "    n = len(train_aug_loader)\n",
    "    with torch.no_grad():\n",
    "        net0.eval()\n",
    "        net1.eval()\n",
    "        for i, (images, _) in enumerate(tqdm(train_aug_loader)):\n",
    "            \n",
    "            img_t = images.float().cuda()\n",
    "            out0 = net0(img_t).double()\n",
    "            out0 = out0.permute(0, 2, 3, 1).reshape(-1, out0.shape[1])\n",
    "            out1 = net1(img_t).double()\n",
    "            out1 = out1.permute(0, 2, 3, 1).reshape(-1, out1.shape[1])\n",
    "\n",
    "            # save batchwise first+second moments and outer product\n",
    "            mean0_b = out0.mean(dim=0)\n",
    "            mean1_b = out1.mean(dim=0)\n",
    "            sqmean0_b = out0.square().mean(dim=0)\n",
    "            sqmean1_b = out1.square().mean(dim=0)\n",
    "            outer_b = (out0.T @ out1) / out0.shape[0]\n",
    "            if i == 0:\n",
    "                mean0 = torch.zeros_like(mean0_b)\n",
    "                mean1 = torch.zeros_like(mean1_b)\n",
    "                sqmean0 = torch.zeros_like(sqmean0_b)\n",
    "                sqmean1 = torch.zeros_like(sqmean1_b)\n",
    "                outer = torch.zeros_like(outer_b)\n",
    "            mean0 += mean0_b / n\n",
    "            mean1 += mean1_b / n\n",
    "            sqmean0 += sqmean0_b / n\n",
    "            sqmean1 += sqmean1_b / n\n",
    "            outer += outer_b / n\n",
    "\n",
    "    cov = outer - torch.outer(mean0, mean1)\n",
    "    std0 = (sqmean0 - mean0**2).sqrt()\n",
    "    std1 = (sqmean1 - mean1**2).sqrt()\n",
    "    outer = (torch.outer(std0, std1) + 1e-4)\n",
    "    outer[outer.isnan()] = 1e-4\n",
    "    corr = cov / outer\n",
    "    assert corr.isnan().sum() == 0, 'corr has nans...'\n",
    "    return corr\n",
    "\n",
    "def get_layer_perm1(corr_mtx):\n",
    "    corr_mtx_a = corr_mtx.cpu().numpy()\n",
    "    row_ind, col_ind = scipy.optimize.linear_sum_assignment(corr_mtx_a, maximize=True)\n",
    "    assert (row_ind == np.arange(len(corr_mtx_a))).all()\n",
    "    perm_map = torch.tensor(col_ind).long()\n",
    "    perm_map = torch.eye(corr_mtx.shape[0], device=corr_mtx.device)[perm_map]\n",
    "    return perm_map\n",
    "\n",
    "def get_bipartite_perm(corr, threshold=-torch.inf):\n",
    "    scores, idx = corr.max(0)\n",
    "    valid_elements = scores >= threshold\n",
    "    idx = torch.where(valid_elements, idx, corr.shape[0])\n",
    "    location_lookup = torch.eye(corr.shape[0]+1, corr.shape[0], device=corr.device)\n",
    "    matches = location_lookup[idx]\n",
    "    totals = matches.sum(0, keepdim=True)\n",
    "    matches = matches / (totals + 1)\n",
    "    return matches.t(), totals\n",
    "\n",
    "# returns the channel-permutation to make layer1's activations most closely\n",
    "# match layer0's.\n",
    "def get_layer_perm(net0, net1):\n",
    "    corr_mtx = run_corr_matrix(net0, net1)\n",
    "    return get_layer_perm1(corr_mtx)\n",
    "\n",
    "def get_layer_bipartite_transform(net0, net1, threshold=-torch.inf):\n",
    "    corr_mtx = run_corr_matrix(net0, net1)\n",
    "    return get_bipartite_perm(corr_mtx, threshold=threshold)\n",
    "\n",
    "# modifies the weight matrices of a convolution and batchnorm\n",
    "# layer given a permutation of the output channels\n",
    "def permute_output(perm_map, layer):\n",
    "    pre_weights = [layer.weight,\n",
    "                   layer.bias]\n",
    "    for w in pre_weights:\n",
    "        if len(w.shape) == 4:\n",
    "            transform = torch.einsum('ab,bcde->acde', perm_map, w)\n",
    "        elif len(w.shape) == 2:\n",
    "            transform = perm_map @ w\n",
    "        else:\n",
    "            transform = w @ perm_map.t()\n",
    "#         assert torch.allclose(w[perm_map.argmax(-1)], transform)\n",
    "        w.data = transform\n",
    "\n",
    "# modifies the weight matrix of a layer for a given permutation of the input channels\n",
    "# works for both conv2d and linear\n",
    "def permute_input(perm_map, layer):\n",
    "    w = layer.weight\n",
    "    if len(w.shape) == 4:\n",
    "        transform = torch.einsum('abcd,be->aecd', w, perm_map.t())\n",
    "    elif len(w.shape) == 2:\n",
    "        transform = w @ perm_map.t()\n",
    "#     assert torch.allclose(w[:, perm_map.argmax(-1)], transform)\n",
    "    w.data = transform\n",
    "\n",
    "def subnet(model, n_layers):\n",
    "    return model.features[:n_layers]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e5c95270",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 0.],\n",
       "        [0., 1.],\n",
       "        [0., 0.]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.eye(3, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f48a9e7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "519b2923",
   "metadata": {},
   "outputs": [],
   "source": [
    "def permute_model(source, target):\n",
    "    source_feats = source.features\n",
    "    target_feats = target.features\n",
    "    n = len(source_feats)\n",
    "    for i in range(n):\n",
    "        if not isinstance(target_feats[i], nn.Conv2d): continue\n",
    "        assert isinstance(target_feats[i+1], nn.ReLU)\n",
    "        perm_map = get_layer_perm(\n",
    "            subnet(source, i+2), subnet(target, i+2)\n",
    "        )\n",
    "        permute_output(perm_map, target_feats[i])\n",
    "        \n",
    "        next_layer = None\n",
    "        for j in range(i+1, n):\n",
    "            if isinstance(target_feats[j], nn.Conv2d):\n",
    "                next_layer = target_feats[j]\n",
    "                break\n",
    "        if next_layer is None:\n",
    "            next_layer = target.classifier\n",
    "        permute_input(perm_map, next_layer)\n",
    "\n",
    "def strip_param_suffix(name):\n",
    "    return name.replace('.weight', '').replace('.bias', '')\n",
    "\n",
    "def get_empty_module_dict(net):\n",
    "    module2Dict = dict()\n",
    "    module_list = []\n",
    "    for key in net.state_dict().keys():\n",
    "        base_name = strip_param_suffix(key)\n",
    "        module2Dict[base_name] = dict()\n",
    "        if base_name not in module_list:\n",
    "            module_list += [base_name]\n",
    "    return module2Dict, module_list\n",
    "\n",
    "def apply_bipartite_transform(source, target, threshold=-torch.inf):\n",
    "    source_feats = source.features\n",
    "    target_feats = target.features\n",
    "    module2Dict, module_list = get_empty_module_dict(target)\n",
    "    k = 0\n",
    "    n = len(source_feats)\n",
    "    for i in range(n):\n",
    "        if not isinstance(target_feats[i], nn.Conv2d): continue\n",
    "        assert isinstance(target_feats[i+1], nn.ReLU)\n",
    "        bipartite_map, layer_totals = get_layer_bipartite_transform(\n",
    "            subnet(source, i+2), subnet(target, i+2), threshold=threshold\n",
    "        )\n",
    "        permute_output(bipartite_map, target_feats[i])\n",
    "        module2Dict[module_list[k]]['output'] = layer_totals\n",
    "#         pdb.set_trace()\n",
    "        next_layer = None\n",
    "        for j in range(i+1, n):\n",
    "            if isinstance(target_feats[j], nn.Conv2d):\n",
    "                next_layer = target_feats[j]\n",
    "                break\n",
    "        if next_layer is None:\n",
    "            next_layer = target.classifier\n",
    "        permute_input(bipartite_map, next_layer)\n",
    "        module2Dict[module_list[k+1]]['input'] = layer_totals\n",
    "        k += 1\n",
    "    return module2Dict\n",
    "\n",
    "def combine_io_masks(io, param):\n",
    "    mask = torch.zeros_like(param, device=param.device)\n",
    "    try:\n",
    "        if 'output' in io:\n",
    "            mask[io['output'].view(-1) == 0] = 1.\n",
    "        if 'input' in io and len(mask.shape) > 1:\n",
    "            mask[:, io['input'].view(-1) == 0] = 1.\n",
    "    except:\n",
    "        pdb.set_trace()\n",
    "    return mask\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f1fbcaf4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.8984, 0.8982)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model0 = vgg11()\n",
    "model1 = vgg11()\n",
    "load_model(model0, 'vgg11_v2')\n",
    "load_model(model1, 'vgg11_v1')\n",
    "\n",
    "evaluate(model0), evaluate(model1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a5d99831",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████| 100/100 [00:12<00:00,  7.70it/s]\n",
      "100%|██████████████████████████████████████████████████| 100/100 [00:12<00:00,  7.71it/s]\n",
      "100%|██████████████████████████████████████████████████| 100/100 [00:13<00:00,  7.68it/s]\n",
      "100%|██████████████████████████████████████████████████| 100/100 [00:13<00:00,  7.64it/s]\n",
      "100%|██████████████████████████████████████████████████| 100/100 [00:13<00:00,  7.65it/s]\n",
      "100%|██████████████████████████████████████████████████| 100/100 [00:13<00:00,  7.66it/s]\n",
      "100%|██████████████████████████████████████████████████| 100/100 [00:13<00:00,  7.63it/s]\n",
      "100%|██████████████████████████████████████████████████| 100/100 [00:13<00:00,  7.62it/s]\n"
     ]
    }
   ],
   "source": [
    "permute_model(model0, model1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c3854fca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8982\n"
     ]
    }
   ],
   "source": [
    "print(evaluate(model1))\n",
    "save_model(model1, 'vgg11_v1_perm1_copy')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4620769",
   "metadata": {},
   "source": [
    "# Merge the two networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9f7f6a45",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mix_weights(net, alpha, key0, key1, module2io=None):\n",
    "    sd0 = torch.load(\n",
    "        os.path.join(\n",
    "            '/srv/share4/gstoica3/checkpoints/REPAIR', \n",
    "            '%s.pt' % key0\n",
    "        )\n",
    "    )\n",
    "    sd1 = torch.load(\n",
    "        os.path.join(\n",
    "            '/srv/share4/gstoica3/checkpoints/REPAIR', \n",
    "            '%s.pt' % key1\n",
    "        )\n",
    "    )\n",
    "    sd_alpha = {}\n",
    "    for k in sd0.keys():\n",
    "        param0 = sd0[k].cuda()\n",
    "        param1 = sd1[k].cuda()\n",
    "        sd_alpha[k] = (1 - alpha) * param0 + alpha * param1\n",
    "        if module2io is not None:\n",
    "            param_base = strip_param_suffix(k)\n",
    "            mask = combine_io_masks(module2io[param_base], param1)\n",
    "            sd_alpha[k][mask == 1] = param0[mask == 1]\n",
    "#     sd_alpha = {k: (1 - alpha) * sd0[k].cuda() + alpha * sd1[k].cuda()\n",
    "#                 for k in sd0.keys()}\n",
    "    net.load_state_dict(sd_alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4410fea5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(α=0): 89.8% \t\t<-- Model A\n",
      "(α=1): 89.8% \t\t<-- Model B\n",
      "(α=0.5): 62.9% \t\t<-- Merged model\n"
     ]
    }
   ],
   "source": [
    "k0 = 'vgg11_v2'\n",
    "k1 = 'vgg11_v1_perm1_copy'\n",
    "model0 = vgg11()\n",
    "model1 = vgg11()\n",
    "model_a = vgg11()\n",
    "mix_weights(model0, 0.0, k0, k1)\n",
    "mix_weights(model1, 1.0, k0, k1)\n",
    "\n",
    "alpha = 0.5\n",
    "mix_weights(model_a, alpha, k0, k1)\n",
    "print('(α=0): %.1f%% \\t\\t<-- Model A' % (100*evaluate(model0)))\n",
    "print('(α=1): %.1f%% \\t\\t<-- Model B' % (100*evaluate(model1)))\n",
    "print('(α=0.5): %.1f%% \\t\\t<-- Merged model' % (100*evaluate(model_a)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6eee5311",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mix_weights_over_alphas(num_times, model_a, k0, k1, module2io=None):\n",
    "    step = 1. / num_times\n",
    "    alphas = np.arange(0., 1. + step, step)\n",
    "    accuracies = []\n",
    "    for alpha in tqdm(alphas):\n",
    "        mix_weights(model_a, alpha, k0, k1, module2io=module2io)\n",
    "        accuracies.append(evaluate(model_a))\n",
    "    return alphas, accuracies\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1b373bca",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████| 11/11 [00:19<00:00,  1.75s/it]\n"
     ]
    }
   ],
   "source": [
    "alphas, permute_accuracies = mix_weights_over_alphas(10, model_a, k0, k1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f0e05480",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.8984,\n",
       " 0.8884,\n",
       " 0.8419,\n",
       " 0.7486,\n",
       " 0.652,\n",
       " 0.6288,\n",
       " 0.6901,\n",
       " 0.7873,\n",
       " 0.8574,\n",
       " 0.8907,\n",
       " 0.8982]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "permute_accuracies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48940dd5",
   "metadata": {},
   "source": [
    "# Apply Bipartite Transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "77bec5e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.8984, 0.8982)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model0 = vgg11()\n",
    "model1 = vgg11()\n",
    "load_model(model0, 'vgg11_v2')\n",
    "load_model(model1, 'vgg11_v1')\n",
    "\n",
    "evaluate(model0), evaluate(model1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1f0a4e55",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████| 100/100 [00:12<00:00,  7.78it/s]\n",
      "100%|██████████████████████████████████████████████████| 100/100 [00:12<00:00,  7.70it/s]\n",
      "100%|██████████████████████████████████████████████████| 100/100 [00:13<00:00,  7.58it/s]\n",
      "100%|██████████████████████████████████████████████████| 100/100 [00:13<00:00,  7.53it/s]\n",
      "100%|██████████████████████████████████████████████████| 100/100 [00:13<00:00,  7.56it/s]\n",
      "100%|██████████████████████████████████████████████████| 100/100 [00:13<00:00,  7.60it/s]\n",
      "100%|██████████████████████████████████████████████████| 100/100 [00:13<00:00,  7.65it/s]\n",
      "100%|██████████████████████████████████████████████████| 100/100 [00:12<00:00,  7.70it/s]\n"
     ]
    }
   ],
   "source": [
    "module2io = apply_bipartite_transform(model0, model1, threshold=-torch.inf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "2f4e252f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "features.0 | input: None, output: torch.Size([1, 64])\n",
      "features.3 | input: torch.Size([1, 64]), output: torch.Size([1, 128])\n",
      "features.6 | input: torch.Size([1, 128]), output: torch.Size([1, 256])\n",
      "features.8 | input: torch.Size([1, 256]), output: torch.Size([1, 256])\n",
      "features.11 | input: torch.Size([1, 256]), output: torch.Size([1, 512])\n",
      "features.13 | input: torch.Size([1, 512]), output: torch.Size([1, 512])\n",
      "features.16 | input: torch.Size([1, 512]), output: torch.Size([1, 512])\n",
      "features.18 | input: torch.Size([1, 512]), output: torch.Size([1, 512])\n",
      "classifier | input: torch.Size([1, 512]), output: None\n"
     ]
    }
   ],
   "source": [
    "for key, io in module2io.items():\n",
    "        input_str = io['input'].shape if 'input' in io else 'None'\n",
    "        output_str = io['output'].shape if 'output' in io else 'None'\n",
    "        print(f'{key} | input: {input_str}, output: {output_str}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2e21505f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1\n"
     ]
    }
   ],
   "source": [
    "print(evaluate(model1))\n",
    "save_model(model1, 'vgg11_v1_bipartite_copy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1dd21e6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(α=0): 89.8% \t\t<-- Model A\n",
      "(α=1): 10.0% \t\t<-- Model B\n",
      "(α=0.5): 83.3% \t\t<-- Merged model\n"
     ]
    }
   ],
   "source": [
    "k0 = 'vgg11_v2'\n",
    "k1 = 'vgg11_v1_bipartite_copy'\n",
    "model0 = vgg11()\n",
    "model1 = vgg11()\n",
    "model_a = vgg11()\n",
    "mix_weights(model0, 0.0, k0, k1)\n",
    "mix_weights(model1, 1.0, k0, k1)\n",
    "\n",
    "alpha = 0.5\n",
    "mix_weights(model_a, alpha, k0, k1, module2io=module2io)\n",
    "print('(α=0): %.1f%% \\t\\t<-- Model A' % (100*evaluate(model0)))\n",
    "print('(α=1): %.1f%% \\t\\t<-- Model B' % (100*evaluate(model1)))\n",
    "print('(α=0.5): %.1f%% \\t\\t<-- Merged model' % (100*evaluate(model_a)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e93a03fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████| 11/11 [00:19<00:00,  1.76s/it]\n"
     ]
    }
   ],
   "source": [
    "alphas, bipartite_accuracies = mix_weights_over_alphas(10, model_a, k0, k1, module2io)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "923d05f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "330ff35b",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(permute_accuracies, label='permutation')\n",
    "plt.plot(bipartite_accuracies, label='bipartite')\n",
    "ax = plt.gca()\n",
    "ax.set_ylim([.7, 1.])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3865d88",
   "metadata": {},
   "outputs": [],
   "source": [
    "bipartite_accuracies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee05c1bd",
   "metadata": {},
   "source": [
    "# Fuse Batch Norms Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "id": "ff4795e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrackLayer(nn.Module):\n",
    "    def __init__(self, layer):\n",
    "        super().__init__()\n",
    "        self.layer = layer\n",
    "        self.bn = nn.BatchNorm2d(layer.out_channels)\n",
    "        \n",
    "    def get_stats(self):\n",
    "        return (self.bn.running_mean, self.bn.running_var.sqrt())\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x1 = self.layer(x)\n",
    "        self.bn(x1)\n",
    "        return x1\n",
    "\n",
    "class ResetLayer(nn.Module):\n",
    "    def __init__(self, layer):\n",
    "        super().__init__()\n",
    "        self.layer = layer\n",
    "        self.bn = nn.BatchNorm2d(layer.out_channels)\n",
    "        \n",
    "    def set_stats(self, goal_mean, goal_std):\n",
    "        self.bn.bias.data = goal_mean\n",
    "        self.bn.weight.data = goal_std\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x1 = self.layer(x)\n",
    "        return self.bn(x1)\n",
    "\n",
    "# adds TrackLayers around every conv layer\n",
    "def make_tracked_net(net):\n",
    "    net1 = vgg11()\n",
    "    net1.load_state_dict(net.state_dict())\n",
    "    for i, layer in enumerate(net1.features):\n",
    "        if isinstance(layer, nn.Conv2d):\n",
    "            net1.features[i] = TrackLayer(layer)\n",
    "    return net1.cuda().eval()\n",
    "\n",
    "# adds ResetLayers around every conv layer\n",
    "def make_repaired_net(net):\n",
    "    net1 = vgg11()\n",
    "    net1.load_state_dict(net.state_dict())\n",
    "    for i, layer in enumerate(net1.features):\n",
    "        if isinstance(layer, nn.Conv2d):\n",
    "            net1.features[i] = ResetLayer(layer)\n",
    "    return net1.cuda().eval()\n",
    "\n",
    "# reset all tracked BN stats against training data\n",
    "def reset_bn_stats(model):\n",
    "    # resetting stats to baseline first as below is necessary for stability\n",
    "    for m in model.modules():\n",
    "        if type(m) == nn.BatchNorm2d:\n",
    "            m.momentum = None # use simple average\n",
    "            m.reset_running_stats()\n",
    "    model.train()\n",
    "    with torch.no_grad(), autocast():\n",
    "        for images, _ in train_aug_loader:\n",
    "            output = model(images.cuda())\n",
    "            \n",
    "def fuse_conv_bn(conv, bn):\n",
    "    fused_conv = torch.nn.Conv2d(conv.in_channels,\n",
    "                                 conv.out_channels,\n",
    "                                 kernel_size=conv.kernel_size,\n",
    "                                 stride=conv.stride,\n",
    "                                 padding=conv.padding,\n",
    "                                 bias=True)\n",
    "\n",
    "    # set weights\n",
    "    w_conv = conv.weight.clone()\n",
    "    bn_std = (bn.eps + bn.running_var).sqrt()\n",
    "    gamma = bn.weight / bn_std\n",
    "    fused_conv.weight.data = (w_conv * gamma.reshape(-1, 1, 1, 1))\n",
    "\n",
    "    # set bias\n",
    "    beta = bn.bias + gamma * (-bn.running_mean + conv.bias)\n",
    "    fused_conv.bias.data = beta\n",
    "    \n",
    "    return fused_conv\n",
    "\n",
    "def fuse_tracked_net(net):\n",
    "    net1 = vgg11()\n",
    "    for i, rlayer in enumerate(net.features):\n",
    "        if isinstance(rlayer, ResetLayer):\n",
    "            fused_conv = fuse_conv_bn(rlayer.layer, rlayer.bn)\n",
    "            net1.features[i].load_state_dict(fused_conv.state_dict())\n",
    "    net1.classifier.load_state_dict(net.classifier.state_dict())\n",
    "    return net1\n",
    "\n",
    "def fuse_computed_batchnorms(wrap0, wrap1, wrap_a, alpha=0.5, module2io=None):\n",
    "    # Iterate through corresponding triples of (TrackLayer, TrackLayer, ResetLayer)\n",
    "    # around conv layers in (model0, model1, model_a).\n",
    "    if module2io is not None:\n",
    "        modules = list(module2io.keys())\n",
    "        idx = 0\n",
    "    for track0, track1, reset_a in zip(wrap0.modules(), wrap1.modules(), wrap_a.modules()): \n",
    "        if not isinstance(track0, TrackLayer):\n",
    "            continue  \n",
    "        assert (isinstance(track0, TrackLayer)\n",
    "                and isinstance(track1, TrackLayer)\n",
    "                and isinstance(reset_a, ResetLayer))\n",
    "\n",
    "        # get neuronal statistics of original networks\n",
    "        mu0, std0 = track0.get_stats()\n",
    "        mu1, std1 = track1.get_stats()\n",
    "        # set the goal neuronal statistics for the merged network \n",
    "        goal_mean = (1 - alpha) * mu0 + alpha * mu1\n",
    "        goal_std = (1 - alpha) * std0 + alpha * std1\n",
    "        if module2io is not None:\n",
    "            io = module2io[modules[idx]]\n",
    "            mask = io['output'].view(-1) == 0.\n",
    "            goal_mean[mask == 1] = mu0[mask == 1]\n",
    "            goal_std[mask == 1] = std0[mask == 1]\n",
    "            idx += 1\n",
    "        reset_a.set_stats(goal_mean, goal_std)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73b2f66b",
   "metadata": {},
   "source": [
    "## Fuse Permutations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "id": "2dd24461",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(α=0): 90.0% \t\t<-- Model A\n",
      "(α=1): 89.6% \t\t<-- Model B\n",
      "(α=0.5): 71.4% \t\t<-- Merged model\n"
     ]
    }
   ],
   "source": [
    "k0 = 'vgg11_v2'\n",
    "k1 = 'vgg11_v1_perm1_copy'\n",
    "model0 = vgg11()\n",
    "model1 = vgg11()\n",
    "model_a = vgg11()\n",
    "mix_weights(model0, 0.0, k0, k1)\n",
    "mix_weights(model1, 1.0, k0, k1)\n",
    "\n",
    "alpha = 0.5\n",
    "mix_weights(model_a, alpha, k0, k1)\n",
    "print('(α=0): %.1f%% \\t\\t<-- Model A' % (100*evaluate(model0)))\n",
    "print('(α=1): %.1f%% \\t\\t<-- Model B' % (100*evaluate(model1)))\n",
    "print('(α=0.5): %.1f%% \\t\\t<-- Merged model' % (100*evaluate(model_a)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "id": "871ec9d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Calculate all neuronal statistics in the endpoint networks\n",
    "wrap0 = make_tracked_net(model0)\n",
    "wrap1 = make_tracked_net(model1)\n",
    "reset_bn_stats(wrap0)\n",
    "reset_bn_stats(wrap1)\n",
    "\n",
    "wrap_a = make_repaired_net(model_a)\n",
    "fuse_computed_batchnorms(wrap0, wrap1, wrap_a)\n",
    "# Estimate mean/vars such that when added BNs are set to eval mode,\n",
    "# neuronal stats will be goal_mean and goal_std.\n",
    "reset_bn_stats(wrap_a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "id": "bad85178",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fuse the rescaling+shift coefficients back into conv layers\n",
    "model_b = fuse_tracked_net(wrap_a)\n",
    "assert abs(evaluate(model_b) - evaluate(wrap_a)) < 0.005"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "id": "fc8f9684",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(α=0): 90.0% \t\t<-- Model A\n",
      "(α=1): 89.6% \t\t<-- Model B\n",
      "(α=0.5): 85.4% \t\t<-- Merged model with REPAIR\n"
     ]
    }
   ],
   "source": [
    "print('(α=0): %.1f%% \\t\\t<-- Model A' % (100*evaluate(model0)))\n",
    "print('(α=1): %.1f%% \\t\\t<-- Model B' % (100*evaluate(model1)))\n",
    "print('(α=0.5): %.1f%% \\t\\t<-- Merged model with REPAIR' % (100*evaluate(model_b)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd694bfa",
   "metadata": {},
   "source": [
    "## Fuse Bipartite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "id": "42d77beb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(α=0): 90.0% \t\t<-- Model A\n",
      "(α=1): 10.0% \t\t<-- Model B\n",
      "(α=0.5): 81.0% \t\t<-- Merged model\n"
     ]
    }
   ],
   "source": [
    "k0 = 'vgg11_v2'\n",
    "k1 = 'vgg11_v1_bipartite_copy'\n",
    "model0 = vgg11()\n",
    "model1 = vgg11()\n",
    "model_a = vgg11()\n",
    "mix_weights(model0, 0.0, k0, k1)\n",
    "mix_weights(model1, 1.0, k0, k1)\n",
    "\n",
    "alpha = 0.5\n",
    "mix_weights(model_a, alpha, k0, k1, module2io=module2io)\n",
    "print('(α=0): %.1f%% \\t\\t<-- Model A' % (100*evaluate(model0)))\n",
    "print('(α=1): %.1f%% \\t\\t<-- Model B' % (100*evaluate(model1)))\n",
    "print('(α=0.5): %.1f%% \\t\\t<-- Merged model' % (100*evaluate(model_a)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "id": "451f1154",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Calculate all neuronal statistics in the endpoint networks\n",
    "wrap0 = make_tracked_net(model0)\n",
    "wrap1 = make_tracked_net(model1)\n",
    "reset_bn_stats(wrap0)\n",
    "reset_bn_stats(wrap1)\n",
    "\n",
    "wrap_a = make_repaired_net(model_a)\n",
    "fuse_computed_batchnorms(wrap0, wrap1, wrap_a)#, module2io=module2io)\n",
    "# Estimate mean/vars such that when added BNs are set to eval mode,\n",
    "# neuronal stats will be goal_mean and goal_std.\n",
    "reset_bn_stats(wrap_a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "id": "0e5750b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(α=0.5): 89.3% \t\t<-- Merged model with REPAIR\n"
     ]
    }
   ],
   "source": [
    "model_b = fuse_tracked_net(wrap_a)\n",
    "print('(α=0.5): %.1f%% \\t\\t<-- Merged model with REPAIR' % (100*evaluate(model_b)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cde3c96",
   "metadata": {},
   "source": [
    "# Evaluate Transformation & Repair Over Multiple Alphas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "id": "c1041e07",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mix_weights_and_repair_over_alphas(num_times, model_a, wrap0, wrap1, k0, k1, module2io=None):\n",
    "    step = 1. / num_times\n",
    "    alphas = np.arange(0., 1. + step, step)\n",
    "    accuracies = []\n",
    "    for alpha in tqdm(alphas):\n",
    "        mix_weights(model_a, alpha, k0, k1, module2io=module2io)\n",
    "        wrap_a = make_repaired_net(model_a)\n",
    "        fuse_computed_batchnorms(wrap0, wrap1, wrap_a)\n",
    "        reset_bn_stats(wrap_a)\n",
    "        model_b = fuse_tracked_net(wrap_a)\n",
    "        accuracies.append(evaluate(model_b))\n",
    "    return alphas, accuracies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b0b5bf0",
   "metadata": {},
   "source": [
    "## Permute + REPAIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "id": "674604d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(α=0): 90.0% \t\t<-- Model A\n",
      "(α=1): 89.6% \t\t<-- Model B\n",
      "(α=0.5): 71.4% \t\t<-- Merged model\n"
     ]
    }
   ],
   "source": [
    "k0 = 'vgg11_v2'\n",
    "k1 = 'vgg11_v1_perm1_copy'\n",
    "model0 = vgg11()\n",
    "model1 = vgg11()\n",
    "model_a = vgg11()\n",
    "mix_weights(model0, 0.0, k0, k1)\n",
    "mix_weights(model1, 1.0, k0, k1)\n",
    "\n",
    "alpha = 0.5\n",
    "mix_weights(model_a, alpha, k0, k1)\n",
    "print('(α=0): %.1f%% \\t\\t<-- Model A' % (100*evaluate(model0)))\n",
    "print('(α=1): %.1f%% \\t\\t<-- Model B' % (100*evaluate(model1)))\n",
    "print('(α=0.5): %.1f%% \\t\\t<-- Merged model' % (100*evaluate(model_a)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "id": "4c12f6da",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Calculate all neuronal statistics in the endpoint networks\n",
    "wrap0 = make_tracked_net(model0)\n",
    "wrap1 = make_tracked_net(model1)\n",
    "reset_bn_stats(wrap0)\n",
    "reset_bn_stats(wrap1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "id": "2b936aeb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████| 11/11 [08:32<00:00, 46.59s/it]\n"
     ]
    }
   ],
   "source": [
    "alphas, permute_repairs = mix_weights_and_repair_over_alphas(10, model_a, wrap0, wrap1, k0, k1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "id": "7022e1b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.8914,\n",
       " 0.8906,\n",
       " 0.8826,\n",
       " 0.8714,\n",
       " 0.859,\n",
       " 0.8544,\n",
       " 0.859,\n",
       " 0.8703,\n",
       " 0.8779,\n",
       " 0.8856,\n",
       " 0.8864]"
      ]
     },
     "execution_count": 253,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "permute_repairs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6df2ef8b",
   "metadata": {},
   "source": [
    "## Bipartite + REPAIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "id": "d1c4ffbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(α=0): 90.0% \t\t<-- Model A\n",
      "(α=1): 10.0% \t\t<-- Model B\n",
      "(α=0.5): 81.0% \t\t<-- Merged model\n"
     ]
    }
   ],
   "source": [
    "k0 = 'vgg11_v2'\n",
    "k1 = 'vgg11_v1_bipartite_copy'\n",
    "model0 = vgg11()\n",
    "model1 = vgg11()\n",
    "model_a = vgg11()\n",
    "mix_weights(model0, 0.0, k0, k1)\n",
    "mix_weights(model1, 1.0, k0, k1)\n",
    "\n",
    "alpha = 0.5\n",
    "mix_weights(model_a, alpha, k0, k1, module2io=module2io)\n",
    "print('(α=0): %.1f%% \\t\\t<-- Model A' % (100*evaluate(model0)))\n",
    "print('(α=1): %.1f%% \\t\\t<-- Model B' % (100*evaluate(model1)))\n",
    "print('(α=0.5): %.1f%% \\t\\t<-- Merged model' % (100*evaluate(model_a)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "id": "5cfe4148",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Calculate all neuronal statistics in the endpoint networks\n",
    "wrap0 = make_tracked_net(model0)\n",
    "wrap1 = make_tracked_net(model1)\n",
    "reset_bn_stats(wrap0)\n",
    "reset_bn_stats(wrap1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "id": "824a802b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████| 11/11 [04:36<00:00, 25.18s/it]\n"
     ]
    }
   ],
   "source": [
    "alphas, bipartite_repairs = mix_weights_and_repair_over_alphas(\n",
    "    10, model_a, wrap0, wrap1, k0, k1, module2io=module2io\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "id": "88de91db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.8958,\n",
       " 0.8959,\n",
       " 0.8984,\n",
       " 0.8977,\n",
       " 0.8969,\n",
       " 0.8934,\n",
       " 0.8896,\n",
       " 0.8821,\n",
       " 0.87,\n",
       " 0.8515,\n",
       " 0.8176]"
      ]
     },
     "execution_count": 257,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bipartite_repairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6a7de04",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
