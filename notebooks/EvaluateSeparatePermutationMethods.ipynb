{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7a9c3c6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import scipy.optimize\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from torch.optim import SGD, lr_scheduler\n",
    "import torchvision\n",
    "import torchvision.transforms as T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b62d982f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(model, i):\n",
    "    sd = model.state_dict()\n",
    "    torch.save(model.state_dict(), '%s.pt' % i)\n",
    "\n",
    "def load_model(model, i):\n",
    "    sd = torch.load('%s.pt' % i)\n",
    "    model.load_state_dict(sd)\n",
    "    \n",
    "def evaluate(model):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    with torch.no_grad(), autocast():\n",
    "        for inputs, labels in test_loader:\n",
    "            outputs = model(inputs.cuda())\n",
    "            pred = outputs.argmax(dim=1)\n",
    "            correct += (labels.cuda() == pred).sum().item()\n",
    "    return correct / len(test_loader.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "61174b69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "CIFAR_MEAN = [125.307, 122.961, 113.8575]\n",
    "CIFAR_STD = [51.5865, 50.847, 51.255]\n",
    "normalize = T.Normalize(np.array(CIFAR_MEAN)/255, np.array(CIFAR_STD)/255)\n",
    "denormalize = T.Normalize(-np.array(CIFAR_MEAN)/np.array(CIFAR_STD), 255/np.array(CIFAR_STD))\n",
    "\n",
    "train_transform = T.Compose([\n",
    "    T.RandomHorizontalFlip(),\n",
    "    T.RandomCrop(32, padding=4),\n",
    "    T.ToTensor(),\n",
    "    normalize,\n",
    "])\n",
    "test_transform = T.Compose([\n",
    "    T.ToTensor(),\n",
    "    normalize,\n",
    "])\n",
    "train_dset = torchvision.datasets.CIFAR10(root='/tmp', train=True,\n",
    "                                        download=True, transform=train_transform)\n",
    "test_dset = torchvision.datasets.CIFAR10(root='/tmp', train=False,\n",
    "                                        download=True, transform=test_transform)\n",
    "\n",
    "train_aug_loader = torch.utils.data.DataLoader(train_dset, batch_size=500, shuffle=True, num_workers=8)\n",
    "test_loader = torch.utils.data.DataLoader(test_dset, batch_size=500, shuffle=False, num_workers=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f6d511f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/kuangliu/pytorch-cifar/blob/master/models/vgg.py\n",
    "cfg = {\n",
    "    'VGG11': [64, 'M', 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512, 'M'],\n",
    "    'VGG13': [64, 64, 'M', 128, 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512, 'M'],\n",
    "    'VGG16': [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 'M', 512, 512, 512, 'M', 512, 512, 512, 'M'],\n",
    "    'VGG19': [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 256, 'M', 512, 512, 512, 512, 'M', 512, 512, 512, 512, 'M'],\n",
    "}\n",
    "\n",
    "class VGG(nn.Module):\n",
    "    def __init__(self, vgg_name, w=1):\n",
    "        super(VGG, self).__init__()\n",
    "        self.w = w\n",
    "        self.features = self._make_layers(cfg[vgg_name])\n",
    "        self.classifier = nn.Linear(self.w*512, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.features(x)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.classifier(out)\n",
    "        return out\n",
    "\n",
    "    def _make_layers(self, cfg):\n",
    "        layers = []\n",
    "        in_channels = 3\n",
    "        for x in cfg:\n",
    "            if x == 'M':\n",
    "                layers += [nn.MaxPool2d(kernel_size=2, stride=2)]\n",
    "            else:\n",
    "                layers.append(nn.Conv2d(in_channels if in_channels == 3 else self.w*in_channels,\n",
    "                                     self.w*x, kernel_size=3, padding=1))\n",
    "                layers.append(nn.ReLU(inplace=True))\n",
    "                in_channels = x\n",
    "        layers += [nn.AvgPool2d(kernel_size=1, stride=1)]\n",
    "        return nn.Sequential(*layers)\n",
    "    \n",
    "def vgg11(w=1):\n",
    "    return VGG('VGG11', w).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "7b9e85de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Given two networks net0, net1 which each output a feature map of shape NxCxWxH,\n",
    "# this will reshape both outputs to (N*W*H)xC\n",
    "# and then compute a CxC correlation matrix between the two\n",
    "def run_corr_matrix(net0, net1):\n",
    "    n = len(train_aug_loader)\n",
    "    with torch.no_grad():\n",
    "        net0.eval()\n",
    "        net1.eval()\n",
    "        for i, (images, _) in enumerate(tqdm(train_aug_loader)):\n",
    "            \n",
    "            img_t = images.float().cuda()\n",
    "            out0 = net0(img_t).double()\n",
    "            out0 = out0.permute(0, 2, 3, 1).reshape(-1, out0.shape[1])\n",
    "            out1 = net1(img_t).double()\n",
    "            out1 = out1.permute(0, 2, 3, 1).reshape(-1, out1.shape[1])\n",
    "\n",
    "            # save batchwise first+second moments and outer product\n",
    "            mean0_b = out0.mean(dim=0)\n",
    "            mean1_b = out1.mean(dim=0)\n",
    "            sqmean0_b = out0.square().mean(dim=0)\n",
    "            sqmean1_b = out1.square().mean(dim=0)\n",
    "            outer_b = (out0.T @ out1) / out0.shape[0]\n",
    "            if i == 0:\n",
    "                mean0 = torch.zeros_like(mean0_b)\n",
    "                mean1 = torch.zeros_like(mean1_b)\n",
    "                sqmean0 = torch.zeros_like(sqmean0_b)\n",
    "                sqmean1 = torch.zeros_like(sqmean1_b)\n",
    "                outer = torch.zeros_like(outer_b)\n",
    "            mean0 += mean0_b / n\n",
    "            mean1 += mean1_b / n\n",
    "            sqmean0 += sqmean0_b / n\n",
    "            sqmean1 += sqmean1_b / n\n",
    "            outer += outer_b / n\n",
    "\n",
    "    cov = outer - torch.outer(mean0, mean1)\n",
    "    std0 = (sqmean0 - mean0**2).sqrt()\n",
    "    std1 = (sqmean1 - mean1**2).sqrt()\n",
    "    corr = cov / (torch.outer(std0, std1) + 1e-4)\n",
    "    return corr\n",
    "\n",
    "def get_layer_perm1(corr_mtx):\n",
    "    corr_mtx_a = corr_mtx.cpu().numpy()\n",
    "    row_ind, col_ind = scipy.optimize.linear_sum_assignment(corr_mtx_a, maximize=True)\n",
    "    assert (row_ind == np.arange(len(corr_mtx_a))).all()\n",
    "    perm_map = torch.tensor(col_ind).long()\n",
    "    perm_map = torch.eye(corr_mtx.shape[0], device=corr_mtx.device)[perm_map]\n",
    "    return perm_map\n",
    "\n",
    "def get_bipartite_perm(corr):\n",
    "    idx = corr.argmax(0)\n",
    "    matches = torch.eye(corr.shape[0], device=corr.device)[idx]\n",
    "    totals = matches.sum(0, keepdim=True)\n",
    "    matches = matches / (totals + 1)\n",
    "    return matches.t(), totals\n",
    "\n",
    "# returns the channel-permutation to make layer1's activations most closely\n",
    "# match layer0's.\n",
    "def get_layer_perm(net0, net1):\n",
    "    corr_mtx = run_corr_matrix(net0, net1)\n",
    "    return get_layer_perm1(corr_mtx)\n",
    "\n",
    "def get_layer_bipartite_transform(net0, net1):\n",
    "    corr_mtx = run_corr_matrix(net0, net1)\n",
    "    return get_bipartite_perm(corr_mtx)\n",
    "\n",
    "# modifies the weight matrices of a convolution and batchnorm\n",
    "# layer given a permutation of the output channels\n",
    "def permute_output(perm_map, layer):\n",
    "    pre_weights = [layer.weight,\n",
    "                   layer.bias]\n",
    "    for w in pre_weights:\n",
    "        if len(w.shape) == 4:\n",
    "            transform = torch.einsum('ab,bcde->acde', perm_map, w)\n",
    "        elif len(w.shape) == 2:\n",
    "            transform = perm_map @ w\n",
    "        else:\n",
    "            transform = w @ perm_map.t()\n",
    "#         assert torch.allclose(w[perm_map.argmax(-1)], transform)\n",
    "        w.data = transform\n",
    "\n",
    "# modifies the weight matrix of a layer for a given permutation of the input channels\n",
    "# works for both conv2d and linear\n",
    "def permute_input(perm_map, layer):\n",
    "    w = layer.weight\n",
    "    if len(w.shape) == 4:\n",
    "        transform = torch.einsum('abcd,be->aecd', w, perm_map.t())\n",
    "    elif len(w.shape) == 2:\n",
    "        transform = w @ perm_map.t()\n",
    "#     assert torch.allclose(w[:, perm_map.argmax(-1)], transform)\n",
    "    w.data = transform\n",
    "\n",
    "def subnet(model, n_layers):\n",
    "    return model.features[:n_layers]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f4607579",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "afd6113c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def permute_model(source, target):\n",
    "    source_feats = source.features\n",
    "    target_feats = target.features\n",
    "    n = len(source_feats)\n",
    "    for i in range(n):\n",
    "        if not isinstance(target_feats[i], nn.Conv2d): continue\n",
    "        assert isinstance(target_feats[i+1], nn.ReLU)\n",
    "        perm_map = get_layer_perm(\n",
    "            subnet(source, i+2), subnet(target, i+2)\n",
    "        )\n",
    "        permute_output(perm_map, target_feats[i])\n",
    "        \n",
    "        next_layer = None\n",
    "        for j in range(i+1, n):\n",
    "            if isinstance(target_feats[j], nn.Conv2d):\n",
    "                next_layer = target_feats[j]\n",
    "                break\n",
    "        if next_layer is None:\n",
    "            next_layer = target.classifier\n",
    "        permute_input(perm_map, next_layer)\n",
    "\n",
    "def strip_param_suffix(name):\n",
    "    return name.replace('.weight', '').replace('.bias', '')\n",
    "\n",
    "def get_empty_module_dict(net):\n",
    "    module2Dict = dict()\n",
    "    module_list = []\n",
    "    for key in net.state_dict().keys():\n",
    "        base_name = strip_param_suffix(key)\n",
    "        module2Dict[base_name] = dict()\n",
    "        if base_name not in module_list:\n",
    "            module_list += [base_name]\n",
    "    return module2Dict, module_list\n",
    "\n",
    "def apply_bipartite_transform(source, target):\n",
    "    source_feats = source.features\n",
    "    target_feats = target.features\n",
    "    module2Dict, module_list = get_empty_module_dict(target)\n",
    "    k = 0\n",
    "    n = len(source_feats)\n",
    "    for i in range(n):\n",
    "        if not isinstance(target_feats[i], nn.Conv2d): continue\n",
    "        assert isinstance(target_feats[i+1], nn.ReLU)\n",
    "        bipartite_map, layer_totals = get_layer_bipartite_transform(\n",
    "            subnet(source, i+2), subnet(target, i+2)\n",
    "        )\n",
    "        permute_output(bipartite_map, target_feats[i])\n",
    "        module2Dict[module_list[k]]['output'] = layer_totals\n",
    "#         pdb.set_trace()\n",
    "        next_layer = None\n",
    "        for j in range(i+1, n):\n",
    "            if isinstance(target_feats[j], nn.Conv2d):\n",
    "                next_layer = target_feats[j]\n",
    "                break\n",
    "        if next_layer is None:\n",
    "            next_layer = target.classifier\n",
    "        permute_input(bipartite_map, next_layer)\n",
    "        module2Dict[module_list[k+1]]['input'] = layer_totals\n",
    "        k += 1\n",
    "    return module2Dict\n",
    "\n",
    "def combine_io_masks(io, param):\n",
    "    mask = torch.zeros_like(param, device=param.device)\n",
    "    try:\n",
    "        if 'output' in io:\n",
    "            mask[io['output'].view(-1) == 0] = 1.\n",
    "        if 'input' in io and len(mask.shape) > 1:\n",
    "            mask[:, io['input'].view(-1) == 0] = 1.\n",
    "    except:\n",
    "        pdb.set_trace()\n",
    "    return mask\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "ae6adb19",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.9001, 0.8964)"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model0 = vgg11()\n",
    "model1 = vgg11()\n",
    "load_model(model0, 'vgg11_v2')\n",
    "load_model(model1, 'vgg11_v1')\n",
    "\n",
    "evaluate(model0), evaluate(model1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "c8608478",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [00:04<00:00, 20.82it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [00:04<00:00, 20.61it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [00:04<00:00, 20.08it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [00:05<00:00, 19.69it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [00:05<00:00, 19.85it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [00:05<00:00, 19.77it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [00:05<00:00, 19.56it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [00:05<00:00, 19.30it/s]\n"
     ]
    }
   ],
   "source": [
    "permute_model(model0, model1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "19043762",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8964\n"
     ]
    }
   ],
   "source": [
    "print(evaluate(model1))\n",
    "save_model(model1, 'vgg11_v2_perm2_copy')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06e112de",
   "metadata": {},
   "source": [
    "# Merge the two networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "58bf0c00",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mix_weights(net, alpha, key0, key1, module2io=None):\n",
    "    sd0 = torch.load('%s.pt' % key0)\n",
    "    sd1 = torch.load('%s.pt' % key1)\n",
    "    sd_alpha = {}\n",
    "    for k in sd0.keys():\n",
    "        param0 = sd0[k].cuda()\n",
    "        param1 = sd1[k].cuda()\n",
    "        sd_alpha[k] = (1 - alpha) * param0 + alpha * param1\n",
    "        if module2io is not None:\n",
    "            param_base = strip_param_suffix(k)\n",
    "            mask = combine_io_masks(module2io[param_base], param1)\n",
    "            sd_alpha[k][mask == 1] = param0[mask == 1]\n",
    "#     sd_alpha = {k: (1 - alpha) * sd0[k].cuda() + alpha * sd1[k].cuda()\n",
    "#                 for k in sd0.keys()}\n",
    "    net.load_state_dict(sd_alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "382d9b00",
   "metadata": {},
   "outputs": [],
   "source": [
    "k0 = 'vgg11_v2_perm2_copy'\n",
    "k1 = 'vgg11_v2'\n",
    "model0 = vgg11()\n",
    "model1 = vgg11()\n",
    "model_a = vgg11()\n",
    "mix_weights(model0, 0.0, k0, k1)\n",
    "mix_weights(model1, 1.0, k0, k1)\n",
    "\n",
    "alpha = 0.5\n",
    "mix_weights(model_a, alpha, k0, k1)\n",
    "print('(α=0): %.1f%% \\t\\t<-- Model A' % (100*evaluate(model0)))\n",
    "print('(α=1): %.1f%% \\t\\t<-- Model B' % (100*evaluate(model1)))\n",
    "print('(α=0.5): %.1f%% \\t\\t<-- Merged model' % (100*evaluate(model_a)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "891185ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mix_weights_over_alphas(num_times, model_a, k0, k1, module2io=None):\n",
    "    step = 1. / num_times\n",
    "    alphas = np.arange(0., 1. + step, step)\n",
    "    accuracies = []\n",
    "    for alpha in tqdm(alphas):\n",
    "        mix_weights(model_a, alpha, k0, k1, module2io=module2io)\n",
    "        accuracies.append(evaluate(model_a))\n",
    "    return alphas, accuracies\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eafe7db8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "alphas, permute_accuracies = mix_weights_over_alphas(10, model_a, k0, k1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ca9c648",
   "metadata": {},
   "outputs": [],
   "source": [
    "permute_accuracies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebbd6600",
   "metadata": {},
   "source": [
    "# Apply Bipartite Transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "61c3a97a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.9001, 0.8964)"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model0 = vgg11()\n",
    "model1 = vgg11()\n",
    "load_model(model0, 'vgg11_v2')\n",
    "load_model(model1, 'vgg11_v1')\n",
    "\n",
    "evaluate(model0), evaluate(model1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "d9850c43",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [00:04<00:00, 20.83it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [00:04<00:00, 20.61it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [00:04<00:00, 20.00it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [00:04<00:00, 20.19it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [00:05<00:00, 19.80it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [00:05<00:00, 19.73it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [00:05<00:00, 19.70it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [00:05<00:00, 19.62it/s]\n"
     ]
    }
   ],
   "source": [
    "module2io = apply_bipartite_transform(model0, model1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "069f2dcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "features.0 | input: None, output: torch.Size([1, 64])\n",
      "features.3 | input: torch.Size([1, 64]), output: torch.Size([1, 128])\n",
      "features.6 | input: torch.Size([1, 128]), output: torch.Size([1, 256])\n",
      "features.8 | input: torch.Size([1, 256]), output: torch.Size([1, 256])\n",
      "features.11 | input: torch.Size([1, 256]), output: torch.Size([1, 512])\n",
      "features.13 | input: torch.Size([1, 512]), output: torch.Size([1, 512])\n",
      "features.16 | input: torch.Size([1, 512]), output: torch.Size([1, 512])\n",
      "features.18 | input: torch.Size([1, 512]), output: torch.Size([1, 512])\n",
      "classifier | input: torch.Size([1, 512]), output: None\n"
     ]
    }
   ],
   "source": [
    "for key, io in module2io.items():\n",
    "        input_str = io['input'].shape if 'input' in io else 'None'\n",
    "        output_str = io['output'].shape if 'output' in io else 'None'\n",
    "        print(f'{key} | input: {input_str}, output: {output_str}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "ae3fbc20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1\n"
     ]
    }
   ],
   "source": [
    "print(evaluate(model1))\n",
    "save_model(model1, 'vgg11_v2_bipartite2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "656cf9e4",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'vgg11_v2_bipartite2.pt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_21001/1521454877.py\u001b[0m in \u001b[0;36m<cell line: 6>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mmodel1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvgg11\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mmodel_a\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvgg11\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mmix_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0mmix_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_21001/3307067750.py\u001b[0m in \u001b[0;36mmix_weights\u001b[0;34m(net, alpha, key0, key1, module2io)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmix_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodule2io\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0msd0\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'%s.pt'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mkey0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0msd1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'%s.pt'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mkey1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0msd_alpha\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msd0\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, weights_only, **pickle_load_args)\u001b[0m\n\u001b[1;32m    769\u001b[0m         \u001b[0mpickle_load_args\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'encoding'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    770\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 771\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0m_open_file_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mopened_file\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    772\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0m_is_zipfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopened_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    773\u001b[0m             \u001b[0;31m# The zipfile reader is going to advance the current file position.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36m_open_file_like\u001b[0;34m(name_or_buffer, mode)\u001b[0m\n\u001b[1;32m    268\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_open_file_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    269\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0m_is_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 270\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_open_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    271\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    272\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m'w'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, name, mode)\u001b[0m\n\u001b[1;32m    249\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0m_open_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_opener\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 251\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_open_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    252\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    253\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__exit__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'vgg11_v2_bipartite2.pt'"
     ]
    }
   ],
   "source": [
    "k0 = 'vgg11_v2'\n",
    "k1 = 'vgg11_v2_bipartite2'\n",
    "model0 = vgg11()\n",
    "model1 = vgg11()\n",
    "model_a = vgg11()\n",
    "mix_weights(model0, 0.0, k0, k1)\n",
    "mix_weights(model1, 1.0, k0, k1)\n",
    "\n",
    "alpha = 0.5\n",
    "mix_weights(model_a, alpha, k0, k1, module2io=module2io)\n",
    "print('(α=0): %.1f%% \\t\\t<-- Model A' % (100*evaluate(model0)))\n",
    "print('(α=1): %.1f%% \\t\\t<-- Model B' % (100*evaluate(model1)))\n",
    "print('(α=0.5): %.1f%% \\t\\t<-- Merged model' % (100*evaluate(model_a)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "149f6c6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████| 11/11 [00:11<00:00,  1.06s/it]\n"
     ]
    }
   ],
   "source": [
    "alphas, bipartite_accuracies = mix_weights_over_alphas(10, model_a, k0, k1, module2io)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "e081ae4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Matplotlib is building the font cache; this may take a moment.\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59d935d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(permute_accuracies, label='permutation')\n",
    "plt.plot(bipartite_accuracies, label='bipartite')\n",
    "ax = plt.gca()\n",
    "ax.set_ylim([.7, 1.])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "485f4916",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.8964,\n",
       " 0.8964,\n",
       " 0.8874,\n",
       " 0.8721,\n",
       " 0.8433,\n",
       " 0.7943,\n",
       " 0.7204,\n",
       " 0.6208,\n",
       " 0.5002,\n",
       " 0.387,\n",
       " 0.3103]"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bipartite_accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "807ea1c9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_pytorch_p39)",
   "language": "python",
   "name": "conda_pytorch_p39"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
