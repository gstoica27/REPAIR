{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6adf371a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/srv/share/gstoica3/miniconda3/envs/open-mmlab/lib/python3.7/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import pdb\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.optimize\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from torch.optim import SGD, Adam, lr_scheduler\n",
    "import torchvision\n",
    "import torchvision.transforms as T\n",
    "\n",
    "from sys import platform\n",
    "\n",
    "DEVICE = 'mps' if platform == 'darwin' else 'cuda'\n",
    "if DEVICE == 'mps':\n",
    "    DOWNLOAD_PATH = '/Users/georgestoica/Downloads' \n",
    "else:\n",
    "    DOWNLOAD_PATH = '/srv/share/gstoica3/checkpoints/REPAIR/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "591af424",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(model, i):\n",
    "    sd = model.state_dict()\n",
    "    path = os.path.join(\n",
    "        DOWNLOAD_PATH,\n",
    "        '%s.pth.tar' % i\n",
    "    )\n",
    "    torch.save(model.state_dict(), path)\n",
    "\n",
    "def load_model(model, i):\n",
    "    path = os.path.join(\n",
    "        DOWNLOAD_PATH,\n",
    "        '%s.pth.tar' % i\n",
    "    )\n",
    "    sd = torch.load(path, map_location=torch.device(DEVICE))\n",
    "    model.load_state_dict(sd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "734d2142",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "CIFAR_MEAN = [125.307, 122.961, 113.8575]\n",
    "CIFAR_STD = [51.5865, 50.847, 51.255]\n",
    "normalize = T.Normalize(np.array(CIFAR_MEAN)/255, np.array(CIFAR_STD)/255)\n",
    "denormalize = T.Normalize(-np.array(CIFAR_MEAN)/np.array(CIFAR_STD), 255/np.array(CIFAR_STD))\n",
    "\n",
    "train_transform = T.Compose([\n",
    "    T.RandomHorizontalFlip(),\n",
    "    T.RandomCrop(32, padding=4),\n",
    "    T.ToTensor(),\n",
    "    normalize,\n",
    "])\n",
    "test_transform = T.Compose([\n",
    "    T.ToTensor(),\n",
    "    normalize,\n",
    "])\n",
    "train_dset = torchvision.datasets.CIFAR10(root='/tmp', train=True,\n",
    "                                        download=True, transform=train_transform)\n",
    "test_dset = torchvision.datasets.CIFAR10(root='/tmp', train=False,\n",
    "                                        download=True, transform=test_transform)\n",
    "\n",
    "train_aug_loader = torch.utils.data.DataLoader(train_dset, batch_size=500, shuffle=True, num_workers=8)\n",
    "test_loader = torch.utils.data.DataLoader(test_dset, batch_size=500, shuffle=False, num_workers=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "837ea18c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.nn.init as init\n",
    "\n",
    "def _weights_init(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if isinstance(m, nn.Linear) or isinstance(m, nn.Conv2d):\n",
    "        init.kaiming_normal_(m.weight)\n",
    "\n",
    "class LambdaLayer(nn.Module):\n",
    "    def __init__(self, lambd):\n",
    "        super(LambdaLayer, self).__init__()\n",
    "        self.lambd = lambd\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.lambd(x)\n",
    "\n",
    "\n",
    "class BasicBlock(nn.Module):\n",
    "    expansion = 1\n",
    "\n",
    "    def __init__(self, in_planes, planes, stride=1):\n",
    "        super(BasicBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_planes != planes:\n",
    "#             self.shortcut = LambdaLayer(lambda x:\n",
    "#                                         F.pad(x[:, :, ::2, ::2], (0, 0, 0, 0, planes//4, planes//4), \"constant\", 0))\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=False),\n",
    "                nn.BatchNorm2d(planes)\n",
    "            )\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.bn2(self.conv2(out))\n",
    "        out += self.shortcut(x)\n",
    "        out = F.relu(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "class ResNet(nn.Module):\n",
    "    def __init__(self, block, num_blocks, w=1, num_classes=10, text_head=False):\n",
    "        super(ResNet, self).__init__()\n",
    "        self.in_planes = w*16\n",
    "\n",
    "        self.conv1 = nn.Conv2d(3, w*16, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(w*16)\n",
    "        self.layer1 = self._make_layer(block, w*16, num_blocks[0], stride=1)\n",
    "        self.layer2 = self._make_layer(block, w*32, num_blocks[1], stride=2)\n",
    "        self.layer3 = self._make_layer(block, w*64, num_blocks[2], stride=2)\n",
    "        if text_head:\n",
    "            num_classes = 512\n",
    "        self.linear = nn.Linear(w*64, num_classes)\n",
    "\n",
    "        self.apply(_weights_init)\n",
    "\n",
    "    def _make_layer(self, block, planes, num_blocks, stride):\n",
    "        strides = [stride] + [1]*(num_blocks-1)\n",
    "        layers = []\n",
    "        for stride in strides:\n",
    "            layers.append(block(self.in_planes, planes, stride))\n",
    "            self.in_planes = planes * block.expansion\n",
    "\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.layer1(out)\n",
    "        out = self.layer2(out)\n",
    "        out = self.layer3(out)\n",
    "        out = F.avg_pool2d(out, out.size()[3])\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.linear(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "def resnet20(w=1, text_head=False):\n",
    "    return ResNet(BasicBlock, [3, 3, 3], w=w, text_head=text_head)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "93a80b44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluates accuracy\n",
    "def evaluate(model, loader=test_loader, return_confusion=False):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    confusion_matrix = np.zeros((10, 10))\n",
    "    with torch.no_grad(), autocast():\n",
    "        for inputs, labels in loader:\n",
    "            outputs = model(inputs.to(DEVICE))\n",
    "            pred = outputs.argmax(dim=1)\n",
    "            correct += (labels.to(DEVICE) == pred).sum().item()\n",
    "            confusion_matrix[labels.cpu().numpy(), pred.cpu().numpy()] += 1\n",
    "    confusion_matrix /= confusion_matrix.sum(-1, keepdims=True)\n",
    "    if return_confusion:\n",
    "        return correct, confusion_matrix\n",
    "    else:\n",
    "        return correct\n",
    "\n",
    "# evaluates loss\n",
    "def evaluate1(model, loader=test_loader):\n",
    "    model.eval()\n",
    "    losses = []\n",
    "    with torch.no_grad(), autocast():\n",
    "        for inputs, labels in loader:\n",
    "            outputs = model(inputs.to(DEVICE))\n",
    "            loss = F.cross_entropy(outputs, labels.to(DEVICE))\n",
    "            losses.append(loss.item())\n",
    "    return np.array(losses).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c5cc307f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use the train loader with data augmentation as this gives better results\n",
    "def reset_bn_stats(model, epochs=1, loader=train_aug_loader):\n",
    "    # resetting stats to baseline first as below is necessary for stability\n",
    "    for m in model.modules():\n",
    "        if type(m) == nn.BatchNorm2d:\n",
    "            m.momentum = None # use simple average\n",
    "            m.reset_running_stats()\n",
    "    # run a single train epoch with augmentations to recalc stats\n",
    "    model.train()\n",
    "    for _ in range(epochs):\n",
    "        with torch.no_grad(), autocast():\n",
    "            for images, _ in loader:\n",
    "                output = model(images.to(DEVICE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "64e80d31",
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_nothing(x, mode=\"mean\"):\n",
    "    return x\n",
    "\n",
    "def bipartite_soft_matching(\n",
    "    metric: torch.Tensor,\n",
    "    r: float,\n",
    "    class_token: bool = False,\n",
    "    distill_token: bool = False,\n",
    "):\n",
    "    \"\"\"\n",
    "    Applies ToMe with a balanced matching set (50%, 50%).\n",
    "    Input size is [batch, tokens, channels].\n",
    "    r indicates the ratio of tokens to remove (max 50% of tokens).\n",
    "    Extra args:\n",
    "     - class_token: Whether or not there's a class token.\n",
    "     - distill_token: Whether or not there's also a distillation token.\n",
    "    When enabled, the class token and distillation tokens won't get merged.\n",
    "    \"\"\"\n",
    "    protected = 0\n",
    "    if class_token:\n",
    "        protected += 1\n",
    "    if distill_token:\n",
    "        protected += 1\n",
    "\n",
    "    # We can only reduce by a maximum of 50% tokens\n",
    "    t = metric.shape[1]\n",
    "    r = int(r * t)\n",
    "    r = min(r, (t - protected) // 2)\n",
    "\n",
    "    if r <= 0:\n",
    "        return do_nothing, do_nothing\n",
    "\n",
    "    with torch.no_grad():\n",
    "        metric = metric / metric.norm(dim=-1, keepdim=True)\n",
    "        a, b = metric[..., ::2, :], metric[..., 1::2, :]\n",
    "        scores = a @ b.transpose(-1, -2)\n",
    "        \n",
    "#         scores = torch.cov(torch.cat([a, b], dim=1)[0])[None, :r, r:t]\n",
    "#         scores = -torch.cdist(a, b)\n",
    "\n",
    "        if class_token:\n",
    "            scores[..., 0, :] = -math.inf\n",
    "        if distill_token:\n",
    "            scores[..., :, 0] = -math.inf\n",
    "\n",
    "        node_max, node_idx = scores.max(dim=-1)\n",
    "        edge_idx = node_max.argsort(dim=-1, descending=True)[..., None]\n",
    "\n",
    "        unm_idx = edge_idx[..., r:, :]  # Unmerged Tokens\n",
    "        src_idx = edge_idx[..., :r, :]  # Merged Tokens\n",
    "        dst_idx = node_idx[..., None].gather(dim=-2, index=src_idx)\n",
    "\n",
    "        if class_token:\n",
    "            # Sort to ensure the class token is at the start\n",
    "            unm_idx = unm_idx.sort(dim=1)[0]\n",
    "\n",
    "    def merge(x: torch.Tensor, mode=\"mean\") -> torch.Tensor:\n",
    "        src, dst = x[..., ::2, :], x[..., 1::2, :]\n",
    "        n, t1, c = src.shape\n",
    "        unm = src.gather(dim=-2, index=unm_idx.expand(n, t1 - r, c))\n",
    "        src = src.gather(dim=-2, index=src_idx.expand(n, r, c))\n",
    "        dst = dst.scatter_reduce(-2, dst_idx.expand(n, r, c), src, reduce=mode)\n",
    "\n",
    "        if distill_token:\n",
    "            return torch.cat([unm[:, :1], dst[:, :1], unm[:, 1:], dst[:, 1:]], dim=1)\n",
    "        else:\n",
    "            return torch.cat([unm, dst], dim=1)\n",
    "\n",
    "    def unmerge(x: torch.Tensor) -> torch.Tensor:\n",
    "        unm_len = unm_idx.shape[1]\n",
    "        unm, dst = x[..., :unm_len, :], x[..., unm_len:, :]\n",
    "        n, _, c = unm.shape\n",
    "\n",
    "        src = dst.gather(dim=-2, index=dst_idx.expand(n, r, c))\n",
    "\n",
    "        out = torch.zeros(n, metric.shape[1], c, device=x.device, dtype=x.dtype)\n",
    "\n",
    "        out[..., 1::2, :] = dst\n",
    "        out.scatter_(dim=-2, index=(2 * unm_idx).expand(n, unm_len, c), src=unm)\n",
    "        out.scatter_(dim=-2, index=(2 * src_idx).expand(n, r, c), src=src)\n",
    "\n",
    "        return out\n",
    "\n",
    "    return merge, unmerge\n",
    "\n",
    "def permutation_matching(metric, r):\n",
    "    with torch.no_grad():\n",
    "        metric = metric / metric.norm(dim=-1, keepdim=True)\n",
    "        a, b = metric[..., ::2, :], metric[..., 1::2, :]\n",
    "        scores = -(a @ b.transpose(-1, -2))\n",
    "#     pdb.set_trace()\n",
    "    row_ind, col_ind = scipy.optimize.linear_sum_assignment(scores[0].cpu().numpy().T)\n",
    "    row_ind = torch.from_numpy(col_ind)[None, :, None].to(metric.device)\n",
    "    \n",
    "#     print(row_ind, col_ind)\n",
    "    \n",
    "    def merge(x: torch.Tensor, mode=\"mean\") -> torch.Tensor:\n",
    "        src, dst = x[..., ::2, :], x[..., 1::2, :]\n",
    "        n, t1, c = src.shape\n",
    "        \n",
    "        src = src.gather(dim=-2, index=row_ind.expand(n, t1, c))\n",
    "        \n",
    "        if mode == \"sum\":\n",
    "            return dst * 0. + src\n",
    "        elif mode == \"mean\":\n",
    "            return (dst * 0. + src) #/ 2\n",
    "        else:\n",
    "            return 1 / 0\n",
    "    \n",
    "    def unmerge(x):\n",
    "        pdb.set_trace()\n",
    "        n, r, c = x.shape\n",
    "        out = torch.zeros(n, metric.shape[1], c, device=x.device, dtype=x.dtype)\n",
    "\n",
    "        out[..., 1::2, :] = x\n",
    "        out.scatter_(dim=-2, index=(2 * row_ind).expand(n, r, c), src=x)\n",
    "\n",
    "        return out\n",
    "    \n",
    "    return merge, unmerge\n",
    "\n",
    "# bipartite_soft_matching = permutation_matching\n",
    "\n",
    "def svd_matching(metric, r):\n",
    "    U, S, V = torch.svd(metric[0])\n",
    "    return U[None, :, :r], (S @ V.T)[None, :r, :]\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "373711b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def strip_param_suffix(name):\n",
    "    return name.replace('.weight', '').replace('.bias', '')\n",
    "\n",
    "def naively_compress_model(model, ratio):\n",
    "    new_sd = {}\n",
    "    for key, val in model.state_dict().items():\n",
    "        shape = val.shape\n",
    "        if len(shape) == 4:\n",
    "            val = val.flatten(2).permute(2, 0, 1)\n",
    "            merge, unmerge = bipartite_soft_matching(val, r=ratio)\n",
    "            val = unmerge(merge(val))\n",
    "            if not key.startswith('conv1'):\n",
    "                val = val.transpose(2, 1)\n",
    "                merge, unmerge = bipartite_soft_matching(val, r=ratio)\n",
    "                val = unmerge(merge(val)).transpose(2, 1)\n",
    "            val = val.permute(1, 2, 0).reshape(*shape)\n",
    "        elif len(shape) == 2:\n",
    "            val = val[None]\n",
    "            merge, unmerge = bipartite_soft_matching(val, r=0)\n",
    "            val = unmerge(merge(val)).transpose(2, 1)\n",
    "            merge, unmerge = bipartite_soft_matching(val, r=ratio)\n",
    "            val = unmerge(merge(val)).transpose(2, 1)[0]\n",
    "        elif len(shape) == 1:\n",
    "            val = val[None, :, None]\n",
    "            merge, unmerge = bipartite_soft_matching(val, r=ratio)\n",
    "            val = unmerge(merge(val))[0, :, 0]\n",
    "        new_sd[key] = val\n",
    "    return new_sd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1ed5b700",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9536, 9510)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modela = resnet20(w=4).to(DEVICE)\n",
    "modelb = resnet20(w=4).to(DEVICE)\n",
    "load_model(modela, 'resnet20x4_v1')\n",
    "load_model(modelb, 'resnet20x4_v2')\n",
    "\n",
    "evaluate(modela), evaluate(modelb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7f2f6fcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_mats(merge, unmerge, n, t, r):\n",
    "    \n",
    "    merge_mat   = merge(torch.eye(t, device=DEVICE)[None, ...].expand(n, t, t))\n",
    "    unmerge_mat = unmerge(torch.eye(t-r, device=DEVICE)[None, ...].expand(n, t-r, t-r))\n",
    "    return merge_mat, unmerge_mat\n",
    "\n",
    "def prep_conv(weight):\n",
    "    return weight.flatten(1)[None, ...]\n",
    "\n",
    "def unprep_conv(weight, k=3):\n",
    "    weight = weight[0]\n",
    "    o, *_ = weight.shape\n",
    "    return weight.reshape(o, -1, k, k)\n",
    "\n",
    "def make_mats(merge, unmerge, n, t, r):\n",
    "    merge_mat   = merge(torch.eye(t, device=DEVICE)[None, ...].expand(n, t, t))\n",
    "    unmerge_mat = unmerge(torch.eye(t-r, device=DEVICE)[None, ...].expand(n, t-r, t-r))\n",
    "    return merge_mat, unmerge_mat\n",
    "\n",
    "state_dict = {}\n",
    "\n",
    "def interleave_vals(tensor1, tensor2, dim=1):\n",
    "    # Assume tensor is of shape [B,H,D]\n",
    "    return torch.cat((tensor1.unsqueeze(dim+1), tensor2.unsqueeze(dim+1)), dim=dim+1).flatten(dim, dim+1)\n",
    "#     return torch.cat((tensor1, tensor2), dim=dim)\n",
    "\n",
    "def unterleave_vals(tensor):\n",
    "    return tensor[:, ::2, :], tensor[:, 1::2, :]\n",
    "#     return tensor.chunk(2, dim=1)\n",
    "\n",
    "\n",
    "def merge_inconv(state_dict, prefix, a_conv, b_conv):\n",
    "    a_c1 = prep_conv(a_conv.weight)\n",
    "    b_c1 = prep_conv(b_conv.weight)\n",
    "    \n",
    "    c_c1 = interleave_vals(a_c1, b_c1)\n",
    "    merge, unmerge = permutation_matching(c_c1, r=0.5)\n",
    "    \n",
    "    _, t, _ = c_c1.shape\n",
    "    r = int(0.5*t)\n",
    "    \n",
    "    _, out_unmerge = make_mats(merge, unmerge, 1, t, r)\n",
    "    \n",
    "    c_c1 = unprep_conv(merge(c_c1))\n",
    "    \n",
    "    state_dict[prefix + \".weight\"] = c_c1\n",
    "    \n",
    "    return merge, out_unmerge\n",
    "\n",
    "def merge_conv(state_dict, prefix, a_conv, b_conv, unmerge, out_merge=None, eps=1e-7):\n",
    "    def move_kernel_to_output(x): # [out, in, h, w]\n",
    "        return x.transpose(0, 1).flatten(1)[None, ...] # output: [1, in, w*h*out]\n",
    "    \n",
    "    out, _in, k, _ = a_conv.weight.shape\n",
    "    \n",
    "    a_c1 = move_kernel_to_output(a_conv.weight)\n",
    "    b_c1 = move_kernel_to_output(b_conv.weight)\n",
    "    \n",
    "    unmerge_a, unmerge_b = unterleave_vals(unmerge)\n",
    "#     pdb.set_trace()\n",
    "    a_c1 = unmerge_a.transpose(-1, -2) @ a_c1\n",
    "    b_c1 = unmerge_b.transpose(-1, -2) @ b_c1\n",
    "    \n",
    "    def move_kernel_to_input(x):\n",
    "        return x.transpose(1, 2).reshape(1, -1, k*k*x.shape[1]) # [1, out, h*w*in]\n",
    "    \n",
    "    c_c1 = interleave_vals(move_kernel_to_input(a_c1), move_kernel_to_input(b_c1))\n",
    "    \n",
    "    if out_merge is None:\n",
    "        out_merge, out_unmerge = permutation_matching(c_c1, r=0.5)\n",
    "    else:\n",
    "        out_unmerge = None\n",
    "    \n",
    "    _, t, _ = c_c1.shape\n",
    "    r = int(0.5*t)\n",
    "    \n",
    "    if out_unmerge is not None:\n",
    "        _, out_unmerge = make_mats(out_merge, out_unmerge, 1, t, r)\n",
    "    \n",
    "    c_c1 = out_merge(c_c1)\n",
    "    c_c1 = c_c1.reshape(1, out, k, k, _in)[0].permute(0, 3, 1, 2)\n",
    "    \n",
    "    state_dict[prefix + \".weight\"] = c_c1\n",
    "    return out_merge, out_unmerge\n",
    "\n",
    "\n",
    "def merge_bn(state_dict, prefix, a_bn, b_bn, merge):\n",
    "    # weight, bias, running_mean, running_var\n",
    "    \n",
    "    a_stats = torch.stack([a_bn.weight, a_bn.bias, a_bn.running_mean], dim=1)[None, ...]\n",
    "    b_stats = torch.stack([b_bn.weight, b_bn.bias, b_bn.running_mean], dim=1)[None, ...]\n",
    "    \n",
    "    c_stats = interleave_vals(a_stats, b_stats)\n",
    "    c_stats = merge(c_stats)[0]\n",
    "    \n",
    "    c_weight, c_bias, c_mean = c_stats.unbind(dim=-1)\n",
    "    \n",
    "    c_var = interleave_vals(a_bn.running_var[None, :, None], b_bn.running_var[None, :, None])\n",
    "    \n",
    "    ones = c_var * 0 + 1\n",
    "    c_denom = merge(ones, mode=\"sum\")\n",
    "    c_var = merge(c_var, mode=\"sum\")\n",
    "    c_var = c_var / (c_denom ** 2)\n",
    "    c_var = c_var[0, :, 0]\n",
    "    \n",
    "    state_dict[prefix + \".weight\"] = c_weight\n",
    "    state_dict[prefix + \".bias\"] = c_bias\n",
    "    state_dict[prefix + \".running_mean\"] = c_mean\n",
    "    state_dict[prefix + \".running_var\"] = c_var\n",
    "\n",
    "def merge_block(state_dict, prefix, a, b, conv1_merge, conv1_unmerge):\n",
    "    c1_merge, c1_unmerge = merge_conv(state_dict, prefix + \".conv1\", a.conv1, b.conv1, conv1_unmerge)\n",
    "    merge_bn(state_dict, prefix + \".bn1\", a.bn1, b.bn1, c1_merge)\n",
    "    c2_merge, c2_unmerge = merge_conv(state_dict, prefix + \".conv2\", a.conv2, b.conv2, c1_unmerge, out_merge=conv1_merge)\n",
    "    merge_bn(state_dict, prefix + \".bn2\", a.bn2, b.bn2, c2_merge)\n",
    "    \n",
    "def merge_block_shortcut(state_dict, prefix, a, b, conv1_merge, conv1_unmerge):\n",
    "    c1_merge, c1_unmerge = merge_conv(state_dict, prefix + \".conv1\", a.conv1, b.conv1, conv1_unmerge)\n",
    "    merge_bn(state_dict, prefix + \".bn1\", a.bn1, b.bn1, c1_merge)\n",
    "    c2_merge, c2_unmerge = merge_conv(state_dict, prefix + \".conv2\", a.conv2, b.conv2, c1_unmerge)\n",
    "    merge_bn(state_dict, prefix + \".bn2\", a.bn2, b.bn2, c2_merge)\n",
    "    \n",
    "    s_merge, s_unmerge = merge_conv(state_dict, prefix + \".shortcut.0\", a.shortcut[0], b.shortcut[0], conv1_unmerge, out_merge=c2_merge)\n",
    "    merge_bn(state_dict, prefix + \".shortcut.1\", a.shortcut[1], b.shortcut[1], s_merge)\n",
    "    return s_merge, c2_unmerge\n",
    "\n",
    "class conv_wrapper:\n",
    "    def __init__(self, linear):\n",
    "        self.weight = linear.weight[:, :, None, None]\n",
    "\n",
    "def merge_resnet20(state_dict, a, b): #, merge_output=False):\n",
    "    conv1_merge, conv1_unmerge = merge_inconv(state_dict, \"conv1\", a.conv1, b.conv1)\n",
    "    merge_bn(state_dict, \"bn1\", a.bn1, b.bn1, conv1_merge)\n",
    "    \n",
    "    for i in range(3):\n",
    "        merge_block(state_dict, f\"layer1.{i}\", a.layer1[i], b.layer1[i], conv1_merge, conv1_unmerge)\n",
    "    \n",
    "    conv1_merge, conv1_unmerge = merge_block_shortcut(state_dict, \"layer2.0\", a.layer2[0], b.layer2[0], conv1_merge, conv1_unmerge)\n",
    "    for i in range(1, 3):\n",
    "        merge_block(state_dict, f\"layer2.{i}\", a.layer2[i], b.layer2[i], conv1_merge, conv1_unmerge)\n",
    "        \n",
    "    conv1_merge, conv1_unmerge = merge_block_shortcut(state_dict, \"layer3.0\", a.layer3[0], b.layer3[0], conv1_merge, conv1_unmerge)\n",
    "    for i in range(1, 3):\n",
    "        merge_block(state_dict, f\"layer3.{i}\", a.layer3[i], b.layer3[i], conv1_merge, conv1_unmerge)\n",
    "\n",
    "#     if merge_output:\n",
    "    merge_conv(state_dict, \"linear\", conv_wrapper(a.linear), conv_wrapper(b.linear), conv1_unmerge)\n",
    "    state_dict[\"linear.weight\"] = state_dict[\"linear.weight\"][:, :, 0, 0]\n",
    "#     else:\n",
    "#         c_linear = interleave_vals(a.linear.weight.mT[None, ...], b.linear.weight.mT[None, ...])\n",
    "#         c_linear = conv1_merge(c_linear)[0].mT\n",
    "#         state_dict[\"linear.weight\"] = c_linear\n",
    "    state_dict[\"linear.bias\"] = (a.linear.bias + b.linear.bias) / 2\n",
    "\n",
    "# in_merge, _ = bipartite_soft_matching(torch.rand(1, 128, 20, device=DEVICE), r=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "438bf548",
   "metadata": {},
   "outputs": [],
   "source": [
    "def permutation_matching(metric, r):\n",
    "    with torch.no_grad():\n",
    "        metric = metric / metric.norm(dim=-1, keepdim=True)\n",
    "        a, b = metric[..., ::2, :], metric[..., 1::2, :]\n",
    "        scores = -(a @ b.transpose(-1, -2))\n",
    "#     pdb.set_trace()\n",
    "    row_ind, col_ind = scipy.optimize.linear_sum_assignment(scores[0].cpu().numpy().T)\n",
    "    row_ind = torch.from_numpy(col_ind)[None, :, None].to(metric.device)\n",
    "    \n",
    "#     print(row_ind, col_ind)\n",
    "    \n",
    "    def merge(x: torch.Tensor, mode=\"mean\") -> torch.Tensor:\n",
    "        src, dst = x[..., ::2, :], x[..., 1::2, :]\n",
    "        n, t1, c = src.shape\n",
    "        \n",
    "        src = src.gather(dim=-2, index=row_ind.expand(n, t1, c))\n",
    "        \n",
    "        if mode == \"sum\":\n",
    "            return dst * 0. + src\n",
    "        elif mode == \"mean\":\n",
    "            return (dst * 0. + src) #/ 2\n",
    "        else:\n",
    "            return 1 / 0\n",
    "    \n",
    "    def unmerge(x):\n",
    "        n, r, c = x.shape\n",
    "        out = torch.zeros(n, metric.shape[1], c, device=x.device, dtype=x.dtype)\n",
    "\n",
    "        out[..., 1::2, :] = x #torch.zeros_like(x, device=DEVICE)\n",
    "        out.scatter_(dim=-2, index=(2 * row_ind).expand(n, r, c), src=x)\n",
    "\n",
    "        return out\n",
    "    \n",
    "    return merge, unmerge\n",
    "\n",
    "def merge_conv(state_dict, prefix, a_conv, b_conv, unmerge, out_merge=None, eps=1e-7):\n",
    "    def move_kernel_to_output(x): # [out, in, h, w]\n",
    "        return x.transpose(0, 1).flatten(1)[None, ...] # output: [1, in, w*h*out]\n",
    "    \n",
    "    out, _in, k, _ = a_conv.weight.shape\n",
    "    \n",
    "    a_c1 = move_kernel_to_output(a_conv.weight)\n",
    "    b_c1 = move_kernel_to_output(b_conv.weight)\n",
    "    \n",
    "    unmerge_a, unmerge_b = unterleave_vals(unmerge)\n",
    "    a_c1 = unmerge_a @ a_c1\n",
    "    b_c1 = unmerge_b @ b_c1\n",
    "    \n",
    "    def move_kernel_to_input(x):\n",
    "        return x.transpose(1, 2).reshape(1, -1, k*k*x.shape[1]) # [1, out, h*w*in]\n",
    "    \n",
    "    c_c1 = interleave_vals(move_kernel_to_input(a_c1), move_kernel_to_input(b_c1))\n",
    "    \n",
    "    if out_merge is None:\n",
    "        out_merge, out_unmerge = permutation_matching(c_c1, r=0.5)\n",
    "    else:\n",
    "        out_unmerge = None\n",
    "    \n",
    "    _, t, _ = c_c1.shape\n",
    "    r = int(0.5*t)\n",
    "    \n",
    "    if out_unmerge is not None:\n",
    "        _, out_unmerge = make_mats(out_merge, out_unmerge, 1, t, r)\n",
    "    \n",
    "    c_c1 = out_merge(c_c1)\n",
    "    c_c1 = c_c1.reshape(1, out, k, k, _in)[0].permute(0, 3, 1, 2)\n",
    "    \n",
    "    state_dict[prefix + \".weight\"] = c_c1\n",
    "    return out_merge, out_unmerge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "087e6317",
   "metadata": {},
   "outputs": [],
   "source": [
    "is_zero = lambda x: torch.allclose(x, torch.zeros((x.shape[1], x.shape[2]), device=DEVICE))\n",
    "is_I = lambda x: torch.allclose(x, torch.eye(x.shape[1], device=DEVICE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efe3a982",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "state_dict = {}\n",
    "modelc = resnet20(w=4).to(DEVICE)\n",
    "# merge_conv(state_dict, \"layer1.0.conv1\", modela.layer1[0].conv1, modelb.layer1[0].conv1, in_merge)\n",
    "merge_resnet20(state_dict, modelb, modela)\n",
    "modelc.load_state_dict(state_dict)\n",
    "# reset_bn_stats(modelc)\n",
    "evaluate(modelc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8657b6c7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bea39e2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
