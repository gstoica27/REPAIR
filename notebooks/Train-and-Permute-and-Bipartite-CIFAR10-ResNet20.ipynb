{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train-and-Permute-CIFAR10-ResNet20\n",
    "\n",
    "In the following notebook we train two standard 4x-width ResNet20s on CIFAR-10, and then generate a neuron-matching/permutation between the two models. Our permutation-search method matches neurons based on the correlation between their activations.\n",
    "\n",
    "We then interpolate between the two matched networks, and evaluate the accuracy+loss with and without resetting BatchNorm statistics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.optimize\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from torch.optim import SGD, Adam, lr_scheduler\n",
    "import torchvision\n",
    "import torchvision.transforms as T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(model, i):\n",
    "    sd = model.state_dict()\n",
    "    torch.save(model.state_dict(), '/Users/georgestoica/Downloads/%s.pth.tar' % i)\n",
    "\n",
    "def load_model(model, i):\n",
    "    sd = torch.load('/Users/georgestoica/Downloads/%s.pth.tar' % i, map_location=torch.device('mps'))\n",
    "    model.load_state_dict(sd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to /tmp/cifar-10-python.tar.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "84518f3937e143bcb74d390fcf9fe3e8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/170498071 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /tmp/cifar-10-python.tar.gz to /tmp\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "CIFAR_MEAN = [125.307, 122.961, 113.8575]\n",
    "CIFAR_STD = [51.5865, 50.847, 51.255]\n",
    "normalize = T.Normalize(np.array(CIFAR_MEAN)/255, np.array(CIFAR_STD)/255)\n",
    "denormalize = T.Normalize(-np.array(CIFAR_MEAN)/np.array(CIFAR_STD), 255/np.array(CIFAR_STD))\n",
    "\n",
    "train_transform = T.Compose([\n",
    "    T.RandomHorizontalFlip(),\n",
    "    T.RandomCrop(32, padding=4),\n",
    "    T.ToTensor(),\n",
    "    normalize,\n",
    "])\n",
    "test_transform = T.Compose([\n",
    "    T.ToTensor(),\n",
    "    normalize,\n",
    "])\n",
    "train_dset = torchvision.datasets.CIFAR10(root='/tmp', train=True,\n",
    "                                        download=True, transform=train_transform)\n",
    "test_dset = torchvision.datasets.CIFAR10(root='/tmp', train=False,\n",
    "                                        download=True, transform=test_transform)\n",
    "\n",
    "train_aug_loader = torch.utils.data.DataLoader(train_dset, batch_size=500, shuffle=True, num_workers=8)\n",
    "test_loader = torch.utils.data.DataLoader(test_dset, batch_size=500, shuffle=False, num_workers=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluates accuracy\n",
    "def evaluate(model, loader=test_loader):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    with torch.no_grad(), autocast():\n",
    "        for inputs, labels in loader:\n",
    "            outputs = model(inputs.to('mps'))\n",
    "            pred = outputs.argmax(dim=1)\n",
    "            correct += (labels.to('mps') == pred).sum().item()\n",
    "    return correct\n",
    "\n",
    "# evaluates loss\n",
    "def evaluate1(model, loader=test_loader):\n",
    "    model.eval()\n",
    "    losses = []\n",
    "    with torch.no_grad(), autocast():\n",
    "        for inputs, labels in loader:\n",
    "            outputs = model(inputs.to('mps'))\n",
    "            loss = F.cross_entropy(outputs, labels.to('mps'))\n",
    "            losses.append(loss.item())\n",
    "    return np.array(losses).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.nn.init as init\n",
    "\n",
    "def _weights_init(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if isinstance(m, nn.Linear) or isinstance(m, nn.Conv2d):\n",
    "        init.kaiming_normal_(m.weight)\n",
    "\n",
    "class LambdaLayer(nn.Module):\n",
    "    def __init__(self, lambd):\n",
    "        super(LambdaLayer, self).__init__()\n",
    "        self.lambd = lambd\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.lambd(x)\n",
    "\n",
    "\n",
    "class BasicBlock(nn.Module):\n",
    "    expansion = 1\n",
    "\n",
    "    def __init__(self, in_planes, planes, stride=1):\n",
    "        super(BasicBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_planes != planes:\n",
    "#             self.shortcut = LambdaLayer(lambda x:\n",
    "#                                         F.pad(x[:, :, ::2, ::2], (0, 0, 0, 0, planes//4, planes//4), \"constant\", 0))\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=False),\n",
    "                nn.BatchNorm2d(planes)\n",
    "            )\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.bn2(self.conv2(out))\n",
    "        out += self.shortcut(x)\n",
    "        out = F.relu(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "class ResNet(nn.Module):\n",
    "    def __init__(self, block, num_blocks, w=1, num_classes=10):\n",
    "        super(ResNet, self).__init__()\n",
    "        self.in_planes = w*16\n",
    "\n",
    "        self.conv1 = nn.Conv2d(3, w*16, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(w*16)\n",
    "        self.layer1 = self._make_layer(block, w*16, num_blocks[0], stride=1)\n",
    "        self.layer2 = self._make_layer(block, w*32, num_blocks[1], stride=2)\n",
    "        self.layer3 = self._make_layer(block, w*64, num_blocks[2], stride=2)\n",
    "        self.linear = nn.Linear(w*64, num_classes)\n",
    "\n",
    "        self.apply(_weights_init)\n",
    "\n",
    "    def _make_layer(self, block, planes, num_blocks, stride):\n",
    "        strides = [stride] + [1]*(num_blocks-1)\n",
    "        layers = []\n",
    "        for stride in strides:\n",
    "            layers.append(block(self.in_planes, planes, stride))\n",
    "            self.in_planes = planes * block.expansion\n",
    "\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.layer1(out)\n",
    "        out = self.layer2(out)\n",
    "        out = self.layer3(out)\n",
    "        out = F.avg_pool2d(out, out.size()[3])\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.linear(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "def resnet20(w=1):\n",
    "    return ResNet(BasicBlock, [3, 3, 3], w=w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train and save two models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(save_key):\n",
    "    model = resnet20(w=4).to('mps')\n",
    "    optimizer = SGD(model.parameters(), lr=0.4, momentum=0.9, weight_decay=5e-4)\n",
    "    # optimizer = Adam(model.parameters(), lr=0.05)\n",
    "    \n",
    "    # Adam seems to perform worse than SGD for training ResNets on CIFAR-10.\n",
    "    # To make Adam work, we find that we need a very high learning rate: 0.05 (50x the default)\n",
    "    # At this LR, Adam gives 1.0-1.5% worse accuracy than SGD.\n",
    "    \n",
    "    # It is not yet clear whether the increased interpolation barrier for Adam-trained networks\n",
    "    # is simply due to the increased test loss of said networks relative to those trained with SGD.\n",
    "    # We include the option of using Adam in this notebook to explore this question.\n",
    "\n",
    "    EPOCHS = 100\n",
    "    ne_iters = len(train_aug_loader)\n",
    "    lr_schedule = np.interp(np.arange(1+EPOCHS*ne_iters), [0, 5*ne_iters, EPOCHS*ne_iters], [0, 1, 0])\n",
    "    scheduler = lr_scheduler.LambdaLR(optimizer, lr_schedule.__getitem__)\n",
    "\n",
    "    scaler = GradScaler()\n",
    "    loss_fn = CrossEntropyLoss()\n",
    "    \n",
    "    losses = []\n",
    "    for _ in tqdm(range(EPOCHS)):\n",
    "        for i, (inputs, labels) in enumerate(train_aug_loader):\n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "            with autocast():\n",
    "                outputs = model(inputs.to('mps'))\n",
    "                loss = loss_fn(outputs, labels.to('mps'))\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            scheduler.step()\n",
    "            losses.append(loss.item())\n",
    "    print(evaluate(model))\n",
    "    save_model(model, save_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/100 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "element 0 of tensors does not require grad and does not have a grad_fn",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-203-94e8b46eca81>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'resnet20x4_v4'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'resnet20x4_v5'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-202-0e2a9bed7d89>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(save_key)\u001b[0m\n\u001b[1;32m     27\u001b[0m                 \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'mps'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m                 \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'mps'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m             \u001b[0mscaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscale\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m             \u001b[0mscaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m             \u001b[0mscaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    487\u001b[0m             )\n\u001b[1;32m    488\u001b[0m         torch.autograd.backward(\n\u001b[0;32m--> 489\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    490\u001b[0m         )\n\u001b[1;32m    491\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    197\u001b[0m     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[1;32m    198\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m         allow_unreachable=True, accumulate_grad=True)  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    200\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m def grad(\n",
      "\u001b[0;31mRuntimeError\u001b[0m: element 0 of tensors does not require grad and does not have a grad_fn"
     ]
    }
   ],
   "source": [
    "train('resnet20x4_v4')\n",
    "train('resnet20x4_v5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### matching code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# given two networks net0, net1 which each output a feature map of shape NxCxWxH\n",
    "# this will reshape both outputs to (N*W*H)xC\n",
    "# and then compute a CxC correlation matrix between the outputs of the two networks\n",
    "def run_corr_matrix(net0, net1, epochs=1, norm=True, loader=train_aug_loader):\n",
    "    n = epochs*len(loader)\n",
    "    mean0 = mean1 = std0 = std1 = None\n",
    "    with torch.no_grad():\n",
    "        net0.eval()\n",
    "        net1.eval()\n",
    "        for _ in range(epochs):\n",
    "            for i, (images, _) in enumerate(tqdm(loader)):\n",
    "                img_t = images.float().to('mps')\n",
    "                out0 = net0(img_t)\n",
    "                out0 = out0.reshape(out0.shape[0], out0.shape[1], -1).permute(0, 2, 1)\n",
    "                out0 = out0.reshape(-1, out0.shape[2])#.double()\n",
    "\n",
    "                out1 = net1(img_t)\n",
    "                out1 = out1.reshape(out1.shape[0], out1.shape[1], -1).permute(0, 2, 1)\n",
    "                out1 = out1.reshape(-1, out1.shape[2])#.double()\n",
    "\n",
    "                mean0_b = out0.mean(dim=0)\n",
    "                mean1_b = out1.mean(dim=0)\n",
    "                std0_b = out0.std(dim=0)\n",
    "                std1_b = out1.std(dim=0)\n",
    "                outer_b = (out0.T @ out1) / out0.shape[0]\n",
    "\n",
    "                if i == 0:\n",
    "                    mean0 = torch.zeros_like(mean0_b)\n",
    "                    mean1 = torch.zeros_like(mean1_b)\n",
    "                    std0 = torch.zeros_like(std0_b)\n",
    "                    std1 = torch.zeros_like(std1_b)\n",
    "                    outer = torch.zeros_like(outer_b)\n",
    "                mean0 += mean0_b / n\n",
    "                mean1 += mean1_b / n\n",
    "                std0 += std0_b / n\n",
    "                std1 += std1_b / n\n",
    "                outer += outer_b / n\n",
    "\n",
    "    cov = outer - torch.outer(mean0, mean1)\n",
    "    if norm:\n",
    "        corr = cov / (torch.outer(std0, std1) + 1e-4)\n",
    "        return corr\n",
    "    else:\n",
    "        return cov"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_perm_map(corr_mtx):\n",
    "    # sort the (i, j) channel pairs by correlation\n",
    "    nchan = corr_mtx.shape[0]\n",
    "    triples = [(i, j, corr_mtx[i, j].item()) for i in range(nchan) for j in range(nchan)]\n",
    "    triples = sorted(triples, key=lambda p: -p[2])\n",
    "    # greedily find a matching\n",
    "    perm_d = {}\n",
    "    for i, j, c in triples:\n",
    "        if not (i in perm_d.keys() or j in perm_d.values()):\n",
    "            perm_d[i] = j\n",
    "    perm_map = torch.tensor([perm_d[i] for i in range(nchan)])\n",
    "\n",
    "    # qual_map will be a permutation of the indices in the order\n",
    "    # of the quality / degree of correlation between the neurons found in the permutation.\n",
    "    # this just for visualization purposes.\n",
    "    qual_l = [corr_mtx[i, perm_map[i]].item() for i in range(nchan)]\n",
    "    qual_map = torch.tensor(sorted(range(nchan), key=lambda i: -qual_l[i]))\n",
    "\n",
    "    return perm_map, qual_map\n",
    "\n",
    "def get_layer_perm1(corr_mtx, method='max_weight', vizz=False, prune_threshold=-torch.inf):\n",
    "    if method == 'greedy':\n",
    "        perm_map, qual_map = compute_perm_map(corr_mtx)\n",
    "        if vizz:\n",
    "            corr_mtx_viz = (corr_mtx[qual_map].T[perm_map[qual_map]]).T\n",
    "            viz(corr_mtx_viz)\n",
    "    elif method == 'max_weight':\n",
    "        corr_mtx_a = corr_mtx.cpu().numpy()\n",
    "        row_ind, col_ind = scipy.optimize.linear_sum_assignment(corr_mtx_a, maximize=True)\n",
    "        assert (row_ind == np.arange(len(corr_mtx_a))).all()\n",
    "        perm_map = torch.tensor(col_ind).long()\n",
    "        perm_map = torch.eye(corr_mtx.shape[0], device=corr_mtx.device)[perm_map]\n",
    "    else:\n",
    "        raise Exception('Unknown method: %s' % method)\n",
    "    \n",
    "#     pdb.set_trace()\n",
    "    pruned_elements = torch.from_numpy(\n",
    "        corr_mtx_a[row_ind, col_ind] >= prune_threshold\n",
    "    ).to(perm_map.device).to(torch.float32)\n",
    "    return perm_map, pruned_elements\n",
    "\n",
    "# returns the channel-permutation to make layer1's activations most closely\n",
    "# match layer0's.\n",
    "def get_layer_perm(net0, net1, method='max_weight', vizz=False, prune_threshold=-torch.inf):\n",
    "    corr_mtx = run_corr_matrix(net0, net1)\n",
    "    return get_layer_perm1(corr_mtx, method, vizz, prune_threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "# modifies the weight matrices of a convolution and batchnorm\n",
    "# layer given a permutation of the output channels\n",
    "def permute_output(perm_map, conv, bn):\n",
    "    pre_weights = [\n",
    "        conv.weight,\n",
    "        bn.weight,\n",
    "        bn.bias,\n",
    "        bn.running_mean,\n",
    "        bn.running_var,\n",
    "    ]\n",
    "    for w in pre_weights:\n",
    "        if len(w.shape) == 4:\n",
    "            transform = torch.einsum('ab,bcde->acde', perm_map, w)\n",
    "        elif len(w.shape) == 2:\n",
    "            transform = perm_map @ w\n",
    "        else:\n",
    "            transform = w @ perm_map.t()\n",
    "#         assert torch.allclose(w[perm_map.argmax(-1)], transform)\n",
    "        w.data = transform\n",
    "#         w.data = w[perm_map]\n",
    "\n",
    "# modifies the weight matrix of a convolution layer for a given\n",
    "# permutation of the input channels\n",
    "def permute_input(perm_map, after_convs):\n",
    "    if not isinstance(after_convs, list):\n",
    "        after_convs = [after_convs]\n",
    "    post_weights = [c.weight for c in after_convs]\n",
    "    for w in post_weights:\n",
    "        if len(w.shape) == 4:\n",
    "            transform = torch.einsum('abcd,be->aecd', w, perm_map.t())\n",
    "        elif len(w.shape) == 2:\n",
    "            transform = w @ perm_map.t()\n",
    "    #     assert torch.allclose(w[:, perm_map.argmax(-1)], transform)\n",
    "        w.data = transform\n",
    "#         w.data = w[:, perm_map, :, :]\n",
    "\n",
    "def permute_cls_output(perm_map, linear):\n",
    "    for w in [linear.weight, linear.bias]:\n",
    "        w.data = perm_map @ w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pdb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Find neuron-permutation for each layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9540, 9521)"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model0 = resnet20(w=4).to('mps')\n",
    "model1 = resnet20(w=4).to('mps')\n",
    "load_model(model0, 'resnet20x4_v2')\n",
    "load_model(model1, 'resnet20x4_v1')\n",
    "\n",
    "evaluate(model0), evaluate(model1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## residual streams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "prune_threshold = -torch.inf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "module2io = defaultdict(lambda: dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:33<00:00,  3.00it/s]\n"
     ]
    }
   ],
   "source": [
    "class Subnet(nn.Module):\n",
    "    def __init__(self, model):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "    def forward(self, x):\n",
    "        self = self.model\n",
    "        x = F.relu(self.bn1(self.conv1(x)))\n",
    "        x = self.layer1(x)\n",
    "        return x\n",
    "# corr = run_corr_matrix(Subnet(model0), Subnet(model1))\n",
    "# perm_map1 = get_layer_perm1(corr)\n",
    "perm_map, collapse_totals = get_layer_perm(Subnet(model0), Subnet(model1), prune_threshold=prune_threshold)\n",
    "permute_output(perm_map, model1.conv1, model1.bn1)\n",
    "permute_output(perm_map, model1.layer1[0].conv2, model1.layer1[0].bn2)\n",
    "permute_output(perm_map, model1.layer1[1].conv2, model1.layer1[1].bn2)\n",
    "permute_output(perm_map, model1.layer1[2].conv2, model1.layer1[2].bn2)\n",
    "permute_input(perm_map, [model1.layer1[0].conv1, model1.layer1[1].conv1, model1.layer1[2].conv1])\n",
    "permute_input(perm_map, [model1.layer2[0].conv1, model1.layer2[0].shortcut[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "module2io['conv1']['output'] = collapse_totals\n",
    "module2io['bn1']['output'] = collapse_totals\n",
    "module2io['layer1.0.conv2']['output'] = collapse_totals\n",
    "module2io['layer1.0.bn2']['output'] = collapse_totals\n",
    "module2io['layer1.1.conv2']['output'] = collapse_totals\n",
    "module2io['layer1.1.bn2']['output'] = collapse_totals\n",
    "module2io['layer1.2.conv2']['output'] = collapse_totals\n",
    "module2io['layer1.2.bn2']['output'] = collapse_totals\n",
    "\n",
    "module2io['layer1.0.conv1']['input'] = collapse_totals\n",
    "module2io['layer1.1.conv1']['input'] = collapse_totals\n",
    "module2io['layer1.2.conv1']['input'] = collapse_totals\n",
    "module2io['layer2.0.conv1']['input'] = collapse_totals\n",
    "module2io['layer2.0.shortcut.0']['input'] = collapse_totals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:22<00:00,  4.42it/s]\n"
     ]
    }
   ],
   "source": [
    "class Subnet(nn.Module):\n",
    "    def __init__(self, model):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "    def forward(self, x):\n",
    "        self = self.model\n",
    "        x = F.relu(self.bn1(self.conv1(x)))\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        return x\n",
    "\n",
    "perm_map, collapse_totals = get_layer_perm(Subnet(model0), Subnet(model1), prune_threshold=prune_threshold)\n",
    "permute_output(perm_map, model1.layer2[0].conv2, model1.layer2[0].bn2)\n",
    "permute_output(perm_map, model1.layer2[0].shortcut[0], model1.layer2[0].shortcut[1])\n",
    "permute_output(perm_map, model1.layer2[1].conv2, model1.layer2[1].bn2)\n",
    "permute_output(perm_map, model1.layer2[2].conv2, model1.layer2[2].bn2)\n",
    "\n",
    "permute_input(perm_map, [model1.layer2[1].conv1, model1.layer2[2].conv1])\n",
    "permute_input(perm_map, [model1.layer3[0].conv1, model1.layer3[0].shortcut[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "module2io['layer2.0.conv2']['output'] = collapse_totals\n",
    "module2io['layer2.0.bn2']['output'] = collapse_totals\n",
    "module2io['layer2.0.shortcut.0']['output'] = collapse_totals\n",
    "module2io['layer2.0.shortcut.1']['output'] = collapse_totals\n",
    "module2io['layer2.1.conv2']['output'] = collapse_totals\n",
    "module2io['layer2.1.bn2']['output'] = collapse_totals\n",
    "module2io['layer2.2.conv2']['output'] = collapse_totals\n",
    "module2io['layer2.2.bn2']['output'] = collapse_totals\n",
    "\n",
    "module2io['layer2.1.conv1']['input'] = collapse_totals\n",
    "module2io['layer2.2.conv1']['input'] = collapse_totals\n",
    "module2io['layer3.0.conv1']['input'] = collapse_totals\n",
    "module2io['layer3.0.shortcut.0']['input'] = collapse_totals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1.], device='mps:0')"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "collapse_totals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:28<00:00,  3.55it/s]\n"
     ]
    }
   ],
   "source": [
    "class Subnet(nn.Module):\n",
    "    def __init__(self, model):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "    def forward(self, x):\n",
    "        self = self.model\n",
    "        x = F.relu(self.bn1(self.conv1(x)))\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        return x\n",
    "\n",
    "perm_map, collapse_totals = get_layer_perm(Subnet(model0), Subnet(model1), prune_threshold=prune_threshold)\n",
    "permute_output(perm_map, model1.layer3[0].conv2, model1.layer3[0].bn2)\n",
    "permute_output(perm_map, model1.layer3[0].shortcut[0], model1.layer3[0].shortcut[1])\n",
    "permute_output(perm_map, model1.layer3[1].conv2, model1.layer3[1].bn2)\n",
    "permute_output(perm_map, model1.layer3[2].conv2, model1.layer3[2].bn2)\n",
    "\n",
    "permute_input(perm_map, [model1.layer3[1].conv1, model1.layer3[2].conv1])\n",
    "model1.linear.weight.data = model1.linear.weight @ perm_map.t()# [:, perm_map]\n",
    "# w @ perm_map.t()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([256, 256]), torch.Size([10, 256]))"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "perm_map.shape, model1.linear.weight.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "module2io['layer3.0.conv2']['output'] = collapse_totals\n",
    "module2io['layer3.0.bn2']['output'] = collapse_totals\n",
    "module2io['layer3.0.shortcut.0']['output'] = collapse_totals\n",
    "module2io['layer3.0.shortcut.1']['output'] = collapse_totals\n",
    "module2io['layer3.1.conv2']['output'] = collapse_totals\n",
    "module2io['layer3.1.bn2']['output'] = collapse_totals\n",
    "module2io['layer3.2.conv2']['output'] = collapse_totals\n",
    "module2io['layer3.2.bn2']['output'] = collapse_totals\n",
    "\n",
    "module2io['layer3.1.conv1']['input'] = collapse_totals\n",
    "module2io['layer3.2.conv1']['input'] = collapse_totals\n",
    "module2io['linear']['input'] = collapse_totals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "# collapse_totals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## blocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Subnet(nn.Module):\n",
    "    def __init__(self, model, nb=9):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "        self.blocks = []\n",
    "        self.blocks += list(model.layer1)\n",
    "        self.blocks += list(model.layer2)\n",
    "        self.blocks += list(model.layer3)\n",
    "        self.blocks = nn.Sequential(*self.blocks)\n",
    "        self.bn1 = model.bn1\n",
    "        self.conv1 = model.conv1\n",
    "        self.linear = model.linear\n",
    "        self.nb = nb\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.bn1(self.conv1(x)))\n",
    "        x = self.blocks[:self.nb](x)\n",
    "        block = self.blocks[self.nb]\n",
    "        x = block.conv1(x)\n",
    "        x = block.bn1(x)\n",
    "        x = F.relu(x)\n",
    "        return x\n",
    "\n",
    "blocks1 = []\n",
    "blocks1 += list(model1.layer1)\n",
    "blocks1 += list(model1.layer2)\n",
    "blocks1 += list(model1.layer3)\n",
    "blocks1 = nn.Sequential(*blocks1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "block_idx2name = {\n",
    "    0: 'layer1.0',\n",
    "    1: 'layer1.1',\n",
    "    2: 'layer1.2',\n",
    "    3: 'layer2.0',\n",
    "    4: 'layer2.1',\n",
    "    5: 'layer2.2',\n",
    "    6: 'layer3.0',\n",
    "    7: 'layer3.1',\n",
    "    8: 'layer3.2'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:26<00:00,  3.81it/s]\n",
      "100%|██████████| 100/100 [00:28<00:00,  3.46it/s]\n",
      "100%|██████████| 100/100 [00:31<00:00,  3.18it/s]\n",
      "100%|██████████| 100/100 [00:15<00:00,  6.52it/s]\n",
      "100%|██████████| 100/100 [00:18<00:00,  5.26it/s]\n",
      "100%|██████████| 100/100 [00:22<00:00,  4.43it/s]\n",
      "100%|██████████| 100/100 [00:21<00:00,  4.56it/s]\n",
      "100%|██████████| 100/100 [00:24<00:00,  4.00it/s]\n",
      "100%|██████████| 100/100 [00:26<00:00,  3.71it/s]\n"
     ]
    }
   ],
   "source": [
    "for nb, (block_idx, layer_name) in zip(range(9), block_idx2name.items()):\n",
    "    perm_map, collapse_totals = get_layer_perm(Subnet(model0, nb=nb), Subnet(model1, nb=nb))\n",
    "    block = blocks1[nb]\n",
    "    permute_output(perm_map, block.conv1, block.bn1)\n",
    "    permute_input(perm_map, [block.conv2])\n",
    "    \n",
    "    module2io[layer_name + '.conv1']['output'] = collapse_totals\n",
    "    module2io[layer_name + '.bn1']['output'] = collapse_totals\n",
    "    module2io[layer_name + '.conv2']['output'] = collapse_totals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Permute Bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:27<00:00,  3.64it/s]\n"
     ]
    }
   ],
   "source": [
    "perm_map, collapse_totals = get_layer_perm(model0, model1, prune_threshold=prune_threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "permute_cls_output(perm_map, model1.linear)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "module2io['linear']['output'] = collapse_totals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_model(model1, f'resnet20x4_v1_perm1_{prune_threshold}threshold')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate the interpolated network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use the train loader with data augmentation as this gives better results\n",
    "def reset_bn_stats(model, epochs=1, loader=train_aug_loader):\n",
    "    # resetting stats to baseline first as below is necessary for stability\n",
    "    for m in model.modules():\n",
    "        if type(m) == nn.BatchNorm2d:\n",
    "            m.momentum = None # use simple average\n",
    "            m.reset_running_stats()\n",
    "    # run a single train epoch with augmentations to recalc stats\n",
    "    model.train()\n",
    "    for _ in range(epochs):\n",
    "        with torch.no_grad(), autocast():\n",
    "            for images, _ in loader:\n",
    "                output = model(images.to('mps'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "def strip_param_suffix(name):\n",
    "    return name.replace('.weight', '').replace('.bias', '')\n",
    "\n",
    "def combine_io_masks(io, param):\n",
    "    mask = torch.zeros_like(param, device=param.device)\n",
    "    try:\n",
    "        if 'output' in io:\n",
    "            mask[io['output'].view(-1) == 0] = 1.\n",
    "        if 'input' in io and len(mask.shape) > 1:\n",
    "            mask[:, io['input'].view(-1) == 0] = 1.\n",
    "    except:\n",
    "        pdb.set_trace()\n",
    "    return mask\n",
    "\n",
    "def mix_weights(model, alpha, key0, key1, module2io=None):\n",
    "    sd0 = torch.load('/Users/georgestoica/Downloads/%s.pth.tar' % key0, map_location=torch.device('mps'))\n",
    "    sd1 = torch.load('/Users/georgestoica/Downloads/%s.pth.tar' % key1, map_location=torch.device('mps'))\n",
    "    sd_alpha = {}\n",
    "    for k in sd0.keys():\n",
    "        param0 = sd0[k].to('mps')\n",
    "        param1 = sd1[k].to('mps')\n",
    "        sd_alpha[k] = (1 - alpha) * param0 + alpha * param1\n",
    "        if module2io is not None:\n",
    "            param_base = strip_param_suffix(k)\n",
    "#             pdb.set_trace()\n",
    "            mask = combine_io_masks(module2io[param_base], param1)\n",
    "            sd_alpha[k][mask == 1] = param0[mask == 1].to(sd_alpha[k].dtype)\n",
    "#     sd_alpha = {k: (1 - alpha) * sd0[k].to('mps') + alpha * sd1[k].to('mps')\n",
    "#                 for k in sd0.keys()}\n",
    "    model.load_state_dict(sd_alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pre-reset:\n",
      "Accuracy=18.51%, Loss=2.478\n",
      "Post-reset:\n",
      "Accuracy=90.95%, Loss=0.329\n"
     ]
    }
   ],
   "source": [
    "model_a = resnet20(w=4).to('mps') # W_\\alpha\n",
    "mix_weights(model_a, 0.5, 'resnet20x4_v2', f'resnet20x4_v1_perm1_{prune_threshold}threshold', module2io=module2io)\n",
    "\n",
    "print('Pre-reset:')\n",
    "print('Accuracy=%.2f%%, Loss=%.3f' % (evaluate(model_a)/100, evaluate1(model_a)))\n",
    "\n",
    "reset_bn_stats(model_a)\n",
    "print('Post-reset:')\n",
    "print('Accuracy=%.2f%%, Loss=%.3f' % (evaluate(model_a)/100, evaluate1(model_a)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Find neuron-bipartitations for each layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9540, 9521, 9521)"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model0 = resnet20(w=4).to('mps')\n",
    "model1 = resnet20(w=4).to('mps')\n",
    "model_to_alter = resnet20(w=4).to('mps')\n",
    "load_model(model0, 'resnet20x4_v2')\n",
    "load_model(model1, 'resnet20x4_v1')\n",
    "load_model(model_to_alter, 'resnet20x4_v1')\n",
    "\n",
    "evaluate(model0), evaluate(model1), evaluate(model_to_alter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.allclose(model1.conv1.weight, model_to_alter.conv1.weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = -torch.inf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_to_alter = model1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bipartite_perm(corr, prune_threshold=-torch.inf):\n",
    "    scores, idx = corr.max(0)\n",
    "    valid_elements = scores >= prune_threshold\n",
    "    idx = torch.where(valid_elements, idx, corr.shape[0])\n",
    "    location_lookup = torch.eye(corr.shape[0]+1, corr.shape[0], device=corr.device)\n",
    "    matches = location_lookup[idx]\n",
    "    totals = matches.sum(0, keepdim=True)\n",
    "    matches = matches / (totals + 1)\n",
    "    return matches.t(), totals\n",
    "\n",
    "def get_layer_bipartite_transform(net0, net1, prune_threshold=-torch.inf):\n",
    "    corr_mtx = run_corr_matrix(net0, net1)\n",
    "    return get_bipartite_perm(corr_mtx, prune_threshold=prune_threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "module2io = defaultdict(lambda: dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model1.state_dict().keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## residual streams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_first_part(\n",
    "    model0, \n",
    "    model1, \n",
    "    opt_change_model=None, \n",
    "    transform_fn=get_layer_bipartite_transform,\n",
    "    module2io=defaultdict(lambda: dict()),\n",
    "    prune_threshold=-torch.inf\n",
    "):\n",
    "    if opt_change_model is None:\n",
    "        model_to_alter = model1\n",
    "    else:\n",
    "        model_to_alter = opt_change_model\n",
    "    \n",
    "    class Subnet(nn.Module):\n",
    "        def __init__(self, model):\n",
    "            super().__init__()\n",
    "            self.model = model\n",
    "        def forward(self, x):\n",
    "            self = self.model\n",
    "            x = F.relu(self.bn1(self.conv1(x)))\n",
    "            x = self.layer1(x)\n",
    "            return x\n",
    "    # corr = run_corr_matrix(Subnet(model0), Subnet(model1))\n",
    "    # perm_map1 = get_layer_perm1(corr)\n",
    "    perm_map, collapse_totals = transform_fn(Subnet(model0), Subnet(model1), prune_threshold=prune_threshold)\n",
    "    permute_output(perm_map, model_to_alter.conv1, model_to_alter.bn1)\n",
    "    permute_output(perm_map, model_to_alter.layer1[0].conv2, model_to_alter.layer1[0].bn2)\n",
    "    permute_output(perm_map, model_to_alter.layer1[1].conv2, model_to_alter.layer1[1].bn2)\n",
    "    permute_output(perm_map, model_to_alter.layer1[2].conv2, model_to_alter.layer1[2].bn2)\n",
    "    permute_input(perm_map, \n",
    "                  [\n",
    "                      model_to_alter.layer1[0].conv1, \n",
    "                      model_to_alter.layer1[1].conv1, \n",
    "                      model_to_alter.layer1[2].conv1\n",
    "                  ]\n",
    "                 )\n",
    "    permute_input(perm_map, [model_to_alter.layer2[0].conv1, model_to_alter.layer2[0].shortcut[0]])\n",
    "    \n",
    "    module2io['conv1']['output'] = collapse_totals\n",
    "    module2io['bn1']['output'] = collapse_totals\n",
    "    module2io['layer1.0.conv2']['output'] = collapse_totals\n",
    "    module2io['layer1.0.bn2']['output'] = collapse_totals\n",
    "    module2io['layer1.1.conv2']['output'] = collapse_totals\n",
    "    module2io['layer1.1.bn2']['output'] = collapse_totals\n",
    "    module2io['layer1.2.conv2']['output'] = collapse_totals\n",
    "    module2io['layer1.2.bn2']['output'] = collapse_totals\n",
    "\n",
    "    module2io['layer1.0.conv1']['input'] = collapse_totals\n",
    "    module2io['layer1.1.conv1']['input'] = collapse_totals\n",
    "    module2io['layer1.2.conv1']['input'] = collapse_totals\n",
    "    module2io['layer2.0.conv1']['input'] = collapse_totals\n",
    "    module2io['layer2.0.shortcut.0']['input'] = collapse_totals\n",
    "    return module2io"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:32<00:00,  3.03it/s]\n"
     ]
    }
   ],
   "source": [
    "class Subnet(nn.Module):\n",
    "    def __init__(self, model):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "    def forward(self, x):\n",
    "        self = self.model\n",
    "        x = F.relu(self.bn1(self.conv1(x)))\n",
    "        x = self.layer1(x)\n",
    "        return x\n",
    "# corr = run_corr_matrix(Subnet(model0), Subnet(model1))\n",
    "# perm_map1 = get_layer_perm1(corr)\n",
    "perm_map, collapse_totals = get_layer_bipartite_transform(Subnet(model0), Subnet(model1), threshold)\n",
    "permute_output(perm_map, model_to_alter.conv1, model_to_alter.bn1)\n",
    "permute_output(perm_map, model_to_alter.layer1[0].conv2, model_to_alter.layer1[0].bn2)\n",
    "permute_output(perm_map, model_to_alter.layer1[1].conv2, model_to_alter.layer1[1].bn2)\n",
    "permute_output(perm_map, model_to_alter.layer1[2].conv2, model_to_alter.layer1[2].bn2)\n",
    "permute_input(perm_map, [model_to_alter.layer1[0].conv1, model_to_alter.layer1[1].conv1, model_to_alter.layer1[2].conv1])\n",
    "permute_input(perm_map, [model_to_alter.layer2[0].conv1, model_to_alter.layer2[0].shortcut[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "module2io['conv1']['output'] = collapse_totals\n",
    "module2io['bn1']['output'] = collapse_totals\n",
    "module2io['layer1.0.conv2']['output'] = collapse_totals\n",
    "module2io['layer1.0.bn2']['output'] = collapse_totals\n",
    "module2io['layer1.1.conv2']['output'] = collapse_totals\n",
    "module2io['layer1.1.bn2']['output'] = collapse_totals\n",
    "module2io['layer1.2.conv2']['output'] = collapse_totals\n",
    "module2io['layer1.2.bn2']['output'] = collapse_totals\n",
    "\n",
    "module2io['layer1.0.conv1']['input'] = collapse_totals\n",
    "module2io['layer1.1.conv1']['input'] = collapse_totals\n",
    "module2io['layer1.2.conv1']['input'] = collapse_totals\n",
    "module2io['layer2.0.conv1']['input'] = collapse_totals\n",
    "module2io['layer2.0.shortcut.0']['input'] = collapse_totals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 64]), torch.Size([64, 3, 3, 3]))"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "module2io['conv1']['output'].shape, model1.conv1.weight.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 64, 3, 3])"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_to_alter.layer1[0].conv1.weight.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "# module2io = transform_first_part(\n",
    "#     model0,\n",
    "#     model1,\n",
    "#     opt_change_model=model_to_alter,\n",
    "#     module2io=module2io,\n",
    "#     prune_threshold=-torch.inf\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:22<00:00,  4.45it/s]\n"
     ]
    }
   ],
   "source": [
    "class Subnet(nn.Module):\n",
    "    def __init__(self, model):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "    def forward(self, x):\n",
    "        self = self.model\n",
    "        x = F.relu(self.bn1(self.conv1(x)))\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        return x\n",
    "\n",
    "perm_map, collapse_totals = get_layer_bipartite_transform(Subnet(model0), Subnet(model1), threshold)\n",
    "permute_output(perm_map, model_to_alter.layer2[0].conv2, model_to_alter.layer2[0].bn2)\n",
    "permute_output(perm_map, model_to_alter.layer2[0].shortcut[0], model_to_alter.layer2[0].shortcut[1])\n",
    "permute_output(perm_map, model_to_alter.layer2[1].conv2, model_to_alter.layer2[1].bn2)\n",
    "permute_output(perm_map, model_to_alter.layer2[2].conv2, model_to_alter.layer2[2].bn2)\n",
    "\n",
    "permute_input(perm_map, [model_to_alter.layer2[1].conv1, model_to_alter.layer2[2].conv1])\n",
    "permute_input(perm_map, [model_to_alter.layer3[0].conv1, model_to_alter.layer3[0].shortcut[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "module2io['layer2.0.conv2']['output'] = collapse_totals\n",
    "module2io['layer2.0.bn2']['output'] = collapse_totals\n",
    "module2io['layer2.0.shortcut.0']['output'] = collapse_totals\n",
    "module2io['layer2.0.shortcut.1']['output'] = collapse_totals\n",
    "module2io['layer2.1.conv2']['output'] = collapse_totals\n",
    "module2io['layer2.1.bn2']['output'] = collapse_totals\n",
    "module2io['layer2.2.conv2']['output'] = collapse_totals\n",
    "module2io['layer2.2.bn2']['output'] = collapse_totals\n",
    "\n",
    "module2io['layer2.1.conv1']['input'] = collapse_totals\n",
    "module2io['layer2.2.conv1']['input'] = collapse_totals\n",
    "module2io['layer3.0.conv1']['input'] = collapse_totals\n",
    "module2io['layer3.0.shortcut.0']['input'] = collapse_totals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1., 3., 3., 0., 0., 0., 3., 0., 1., 0., 2., 0., 1., 1., 0., 1., 1.,\n",
       "         1., 5., 2., 0., 2., 0., 0., 1., 0., 0., 2., 0., 0., 1., 0., 2., 1., 0.,\n",
       "         1., 2., 3., 0., 1., 1., 0., 1., 3., 1., 1., 3., 1., 1., 0., 2., 1., 2.,\n",
       "         2., 0., 1., 1., 1., 1., 0., 2., 0., 1., 2., 1., 0., 1., 1., 1., 2., 1.,\n",
       "         0., 0., 0., 2., 2., 0., 1., 0., 1., 2., 0., 1., 2., 0., 1., 1., 2., 1.,\n",
       "         2., 0., 1., 0., 1., 1., 0., 1., 3., 2., 0., 0., 0., 0., 2., 1., 2., 1.,\n",
       "         1., 0., 0., 1., 1., 1., 1., 2., 1., 2., 0., 3., 0., 1., 2., 0., 0., 1.,\n",
       "         1., 0.]], device='mps:0')"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "collapse_totals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:27<00:00,  3.58it/s]\n"
     ]
    }
   ],
   "source": [
    "class Subnet(nn.Module):\n",
    "    def __init__(self, model):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "    def forward(self, x):\n",
    "        self = self.model\n",
    "        x = F.relu(self.bn1(self.conv1(x)))\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        return x\n",
    "\n",
    "perm_map, collapse_totals = get_layer_bipartite_transform(Subnet(model0), Subnet(model1), threshold)\n",
    "permute_output(perm_map, model_to_alter.layer3[0].conv2, model_to_alter.layer3[0].bn2)\n",
    "permute_output(perm_map, model_to_alter.layer3[0].shortcut[0], model_to_alter.layer3[0].shortcut[1])\n",
    "permute_output(perm_map, model_to_alter.layer3[1].conv2, model_to_alter.layer3[1].bn2)\n",
    "permute_output(perm_map, model_to_alter.layer3[2].conv2, model_to_alter.layer3[2].bn2)\n",
    "\n",
    "permute_input(perm_map, [model_to_alter.layer3[1].conv1, model_to_alter.layer3[2].conv1])\n",
    "model_to_alter.linear.weight.data = model_to_alter.linear.weight @ perm_map.t()# [:, perm_map]\n",
    "# w @ perm_map.t()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "module2io['layer3.0.conv2']['output'] = collapse_totals\n",
    "module2io['layer3.0.bn2']['output'] = collapse_totals\n",
    "module2io['layer3.0.shortcut.0']['output'] = collapse_totals\n",
    "module2io['layer3.0.shortcut.1']['output'] = collapse_totals\n",
    "module2io['layer3.1.conv2']['output'] = collapse_totals\n",
    "module2io['layer3.1.bn2']['output'] = collapse_totals\n",
    "module2io['layer3.2.conv2']['output'] = collapse_totals\n",
    "module2io['layer3.2.bn2']['output'] = collapse_totals\n",
    "\n",
    "module2io['layer3.1.conv1']['input'] = collapse_totals\n",
    "module2io['layer3.2.conv1']['input'] = collapse_totals\n",
    "module2io['linear']['input'] = collapse_totals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "# perm_map.shape, model1.linear.weight.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.,  0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  1.,  3.,  0.,\n",
       "          1.,  0.,  0.,  1.,  1.,  1.,  0.,  0.,  0.,  0.,  0.,  1.,  0.,  2.,\n",
       "          3.,  0.,  1.,  0.,  0.,  0.,  2.,  0.,  0.,  0.,  0.,  0.,  8.,  2.,\n",
       "          0.,  0.,  0.,  0.,  0.,  2.,  0.,  3.,  0.,  1.,  1.,  1.,  4.,  1.,\n",
       "          0.,  0.,  0.,  1.,  1.,  1.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  1.,\n",
       "          0.,  1.,  0.,  5.,  6.,  4.,  0.,  1.,  0.,  0.,  0.,  1.,  0.,  0.,\n",
       "          0.,  0.,  1.,  1.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "          4.,  0.,  2.,  1.,  1.,  1.,  0.,  0.,  0.,  0.,  2.,  3.,  0.,  0.,\n",
       "          0.,  0.,  0.,  1.,  1.,  2.,  0.,  1.,  0.,  0.,  2.,  0.,  3.,  0.,\n",
       "          4.,  0.,  1.,  0.,  1.,  0.,  2.,  1.,  2.,  0.,  0.,  2.,  0.,  1.,\n",
       "          0.,  0.,  0.,  1.,  1.,  0.,  2.,  1.,  2.,  0.,  0.,  0.,  0.,  0.,\n",
       "          1.,  0.,  0.,  0.,  0.,  1.,  1.,  7.,  2.,  1.,  6., 11.,  0.,  1.,\n",
       "          3.,  2.,  1.,  0.,  0.,  0.,  0.,  4.,  6., 10.,  0.,  1.,  0.,  0.,\n",
       "          0.,  0.,  1.,  0.,  0.,  3.,  5.,  0.,  0.,  0.,  0.,  0.,  0.,  1.,\n",
       "          1.,  0.,  0.,  1.,  1.,  2.,  0.,  0.,  3.,  0.,  3.,  2.,  1.,  0.,\n",
       "          0.,  3.,  0.,  0.,  3.,  0.,  8.,  0.,  1.,  1.,  1.,  1.,  0.,  5.,\n",
       "          0.,  0.,  2.,  1.,  1.,  0.,  1.,  0.,  3.,  5.,  0.,  0.,  2.,  0.,\n",
       "          1.,  9.,  0.,  1.,  0.,  0.,  2.,  2.,  1.,  0.,  2.,  1.,  0.,  0.,\n",
       "          3.,  0.,  0.,  1.]], device='mps:0')"
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "collapse_totals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## blocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Subnet(nn.Module):\n",
    "    def __init__(self, model, nb=9):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "        self.blocks = []\n",
    "        self.blocks += list(model.layer1)\n",
    "        self.blocks += list(model.layer2)\n",
    "        self.blocks += list(model.layer3)\n",
    "        self.blocks = nn.Sequential(*self.blocks)\n",
    "        self.bn1 = model.bn1\n",
    "        self.conv1 = model.conv1\n",
    "        self.linear = model.linear\n",
    "        self.nb = nb\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.bn1(self.conv1(x)))\n",
    "        x = self.blocks[:self.nb](x)\n",
    "        block = self.blocks[self.nb]\n",
    "        x = block.conv1(x)\n",
    "        x = block.bn1(x)\n",
    "        x = F.relu(x)\n",
    "        return x\n",
    "\n",
    "blocks1 = []\n",
    "blocks1 += list(model_to_alter.layer1)\n",
    "blocks1 += list(model_to_alter.layer2)\n",
    "blocks1 += list(model_to_alter.layer3)\n",
    "blocks1 = nn.Sequential(*blocks1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "block_idx2name = {\n",
    "    0: 'layer1.0',\n",
    "    1: 'layer1.1',\n",
    "    2: 'layer1.2',\n",
    "    3: 'layer2.0',\n",
    "    4: 'layer2.1',\n",
    "    5: 'layer2.2',\n",
    "    6: 'layer3.0',\n",
    "    7: 'layer3.1',\n",
    "    8: 'layer3.2'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:25<00:00,  3.90it/s]\n",
      "100%|██████████| 100/100 [00:28<00:00,  3.53it/s]\n",
      "100%|██████████| 100/100 [00:31<00:00,  3.20it/s]\n",
      "100%|██████████| 100/100 [00:15<00:00,  6.50it/s]\n",
      "100%|██████████| 100/100 [00:19<00:00,  5.19it/s]\n",
      "100%|██████████| 100/100 [00:22<00:00,  4.50it/s]\n",
      "100%|██████████| 100/100 [00:22<00:00,  4.47it/s]\n",
      "100%|██████████| 100/100 [00:25<00:00,  3.94it/s]\n",
      "100%|██████████| 100/100 [00:27<00:00,  3.67it/s]\n"
     ]
    }
   ],
   "source": [
    "for nb, (block_idx, layer_name) in zip(range(9), block_idx2name.items()):\n",
    "    perm_map, collapse_totals = get_layer_bipartite_transform(\n",
    "        Subnet(model0, nb=nb), Subnet(model1, nb=nb), threshold\n",
    "    )\n",
    "    block = blocks1[nb]\n",
    "    permute_output(perm_map, block.conv1, block.bn1)\n",
    "    permute_input(perm_map, [block.conv2])\n",
    "    \n",
    "    module2io[layer_name + '.conv1']['output'] = collapse_totals\n",
    "    module2io[layer_name + '.bn1']['output'] = collapse_totals\n",
    "    module2io[layer_name + '.conv2']['output'] = collapse_totals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Permute Bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:28<00:00,  3.56it/s]\n"
     ]
    }
   ],
   "source": [
    "perm_map, collapse_totals = get_layer_bipartite_transform(model0, model1, threshold)\n",
    "permute_cls_output(perm_map, model1.linear)\n",
    "module2io['linear']['output'] = collapse_totals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_model(model_to_alter, f'resnet20x4_v1_bipartite_{threshold}threshold')#_change_model1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate the interpolated network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "def strip_param_suffix(name):\n",
    "    return name.replace('.weight', '').replace('.bias', '')\n",
    "\n",
    "def combine_io_masks(io, param):\n",
    "    mask = torch.zeros_like(param, device=param.device)\n",
    "    try:\n",
    "        if 'output' in io:\n",
    "            mask[io['output'].view(-1) == 0] = 1.\n",
    "        if 'input' in io and len(mask.shape) > 1:\n",
    "            mask[:, io['input'].view(-1) == 0] = 1.\n",
    "    except:\n",
    "        pdb.set_trace()\n",
    "    return mask\n",
    "\n",
    "def mix_weights(model, alpha, key0, key1, module2io=None):\n",
    "    sd0 = torch.load('/Users/georgestoica/Downloads/%s.pth.tar' % key0, map_location=torch.device('mps'))\n",
    "    sd1 = torch.load('/Users/georgestoica/Downloads/%s.pth.tar' % key1, map_location=torch.device('mps'))\n",
    "    sd_alpha = {}\n",
    "    for k in sd0.keys():\n",
    "        param0 = sd0[k].to('mps')\n",
    "        param1 = sd1[k].to('mps')\n",
    "        sd_alpha[k] = (1 - alpha) * param0 + alpha * param1\n",
    "        if module2io is not None:\n",
    "            param_base = strip_param_suffix(k)\n",
    "#             pdb.set_trace()\n",
    "            mask = combine_io_masks(module2io[param_base], param1)\n",
    "            sd_alpha[k][mask == 1] = param0[mask == 1].to(sd_alpha[k].dtype)\n",
    "#     sd_alpha = {k: (1 - alpha) * sd0[k].to('mps') + alpha * sd1[k].to('mps')\n",
    "#                 for k in sd0.keys()}\n",
    "    model.load_state_dict(sd_alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "# threshold = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pre-reset:\n",
      "Accuracy=30.03%, Loss=1.950\n",
      "Post-reset:\n",
      "Accuracy=93.43%, Loss=0.210\n"
     ]
    }
   ],
   "source": [
    "model_a = resnet20(w=4).to('mps') # W_alpha\n",
    "mix_weights(\n",
    "    model_a, \n",
    "    0.5, \n",
    "    'resnet20x4_v2', \n",
    "    f'resnet20x4_v1_bipartite_{threshold}threshold',#_change_model1', \n",
    "    module2io=module2io\n",
    ")\n",
    "\n",
    "print('Pre-reset:')\n",
    "print('Accuracy=%.2f%%, Loss=%.3f' % (evaluate(model_a)/100, evaluate1(model_a)))\n",
    "\n",
    "reset_bn_stats(model_a)\n",
    "print('Post-reset:')\n",
    "print('Accuracy=%.2f%%, Loss=%.3f' % (evaluate(model_a)/100, evaluate1(model_a)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ENAS-pytorch",
   "language": "python",
   "name": "enas-pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
