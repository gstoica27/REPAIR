{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3fa1c00f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/srv/share/gstoica3/miniconda3/envs/open-mmlab/lib/python3.7/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<torch.autograd.grad_mode.set_grad_enabled at 0x7fb4330bd050>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import pdb\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.optimize\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from torch.optim import SGD, Adam, lr_scheduler\n",
    "import torchvision\n",
    "import torchvision.transforms as T\n",
    "\n",
    "from sys import platform\n",
    "\n",
    "DEVICE = 'mps' if platform == 'darwin' else 'cuda'\n",
    "if DEVICE == 'mps':\n",
    "    DOWNLOAD_PATH = '/Users/georgestoica/Downloads' \n",
    "else:\n",
    "    DOWNLOAD_PATH = '/srv/share/gstoica3/checkpoints/REPAIR/'\n",
    "    \n",
    "torch.autograd.set_grad_enabled(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "20839263",
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d9223b1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(model, i):\n",
    "    sd = model.state_dict()\n",
    "    path = os.path.join(\n",
    "        DOWNLOAD_PATH,\n",
    "        '%s.pth.tar' % i\n",
    "    )\n",
    "    torch.save(model.state_dict(), path)\n",
    "\n",
    "def load_model(model, i):\n",
    "    path = os.path.join(\n",
    "        DOWNLOAD_PATH,\n",
    "        '%s.pth.tar' % i\n",
    "    )\n",
    "    sd = torch.load(path, map_location=torch.device(DEVICE))\n",
    "    model.load_state_dict(sd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "255cce97",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.nn.init as init\n",
    "\n",
    "def _weights_init(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if isinstance(m, nn.Linear) or isinstance(m, nn.Conv2d):\n",
    "        init.kaiming_normal_(m.weight)\n",
    "\n",
    "class LambdaLayer(nn.Module):\n",
    "    def __init__(self, lambd):\n",
    "        super(LambdaLayer, self).__init__()\n",
    "        self.lambd = lambd\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.lambd(x)\n",
    "\n",
    "\n",
    "class BasicBlock(nn.Module):\n",
    "    expansion = 1\n",
    "\n",
    "    def __init__(self, in_planes, planes, stride=1):\n",
    "        super(BasicBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_planes != planes:\n",
    "#             self.shortcut = LambdaLayer(lambda x:\n",
    "#                                         F.pad(x[:, :, ::2, ::2], (0, 0, 0, 0, planes//4, planes//4), \"constant\", 0))\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=False),\n",
    "                nn.BatchNorm2d(planes)\n",
    "            )\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.bn2(self.conv2(out))\n",
    "        out += self.shortcut(x)\n",
    "        out = F.relu(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "class ResNet(nn.Module):\n",
    "    def __init__(self, block, num_blocks, w=1, num_classes=10, text_head=False):\n",
    "        super(ResNet, self).__init__()\n",
    "        self.in_planes = int(w*16)\n",
    "\n",
    "        self.conv1 = nn.Conv2d(3, int(w*16), kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(int(w*16))\n",
    "        self.layer1 = self._make_layer(block, int(w*16), num_blocks[0], stride=1)\n",
    "        self.layer2 = self._make_layer(block, int(w*32), num_blocks[1], stride=2)\n",
    "        self.layer3 = self._make_layer(block, int(w*64), num_blocks[2], stride=2)\n",
    "        if text_head:\n",
    "            num_classes = 512\n",
    "        self.linear = nn.Linear(int(w*64), num_classes)\n",
    "\n",
    "        self.apply(_weights_init)\n",
    "\n",
    "    def _make_layer(self, block, planes, num_blocks, stride):\n",
    "        strides = [stride] + [1]*(num_blocks-1)\n",
    "        layers = []\n",
    "        for stride in strides:\n",
    "            layers.append(block(self.in_planes, planes, stride))\n",
    "            self.in_planes = planes * block.expansion\n",
    "\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.layer1(out)\n",
    "        out = self.layer2(out)\n",
    "        out = self.layer3(out)\n",
    "        out = F.avg_pool2d(out, out.size()[3])\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.linear(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "def resnet20(w=1, text_head=False):\n",
    "    return ResNet(BasicBlock, [3, 3, 3], w=w, text_head=text_head)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11f8f68e",
   "metadata": {},
   "source": [
    "# Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c64aa77d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "CIFAR_MEAN = [125.307, 122.961, 113.8575]\n",
    "CIFAR_STD = [51.5865, 50.847, 51.255]\n",
    "normalize = T.Normalize(np.array(CIFAR_MEAN)/255, np.array(CIFAR_STD)/255)\n",
    "denormalize = T.Normalize(-np.array(CIFAR_MEAN)/np.array(CIFAR_STD), 255/np.array(CIFAR_STD))\n",
    "\n",
    "train_transform = T.Compose([\n",
    "    T.RandomHorizontalFlip(),\n",
    "    T.RandomCrop(32, padding=4),\n",
    "    T.ToTensor(),\n",
    "    normalize,\n",
    "])\n",
    "test_transform = T.Compose([\n",
    "    T.ToTensor(),\n",
    "    normalize,\n",
    "])\n",
    "train_dset = torchvision.datasets.CIFAR100(\n",
    "    root='/nethome/gstoica3/research/pytorch-cifar100/data/cifar-100-python', \n",
    "    train=True,\n",
    "    download=True, transform=train_transform\n",
    ")\n",
    "test_dset = torchvision.datasets.CIFAR100(\n",
    "    root='/nethome/gstoica3/research/pytorch-cifar100/data/cifar-100-python',\n",
    "    train=False,\n",
    "    download=True, \n",
    "    transform=test_transform\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "32ffa4a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_aug_loader = torch.utils.data.DataLoader(train_dset, batch_size=500, shuffle=True, num_workers=8)\n",
    "test_loader = torch.utils.data.DataLoader(test_dset, batch_size=500, shuffle=False, num_workers=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b3ee517b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "50000it [00:14, 3551.07it/s]\n",
      "50000it [00:14, 3542.16it/s]\n",
      "10000it [00:01, 5353.69it/s]\n",
      "10000it [00:01, 5458.16it/s]\n"
     ]
    }
   ],
   "source": [
    "model1_classes= np.arange(50) # np.array([3, 2, 0, 6, 4])\n",
    "model2_classes = np.arange(50, 100) # np.array([5, 7, 9, 8, 1])\n",
    "\n",
    "valid_examples1 = [i for i, (_, label) in tqdm(enumerate(train_dset)) if label in model1_classes]\n",
    "valid_examples2 = [i for i, (_, label) in tqdm(enumerate(train_dset)) if label in model2_classes]\n",
    "\n",
    "assert len(set(valid_examples1).intersection(set(valid_examples2))) == 0, 'sets should be disjoint'\n",
    "\n",
    "train_aug_loader1 = torch.utils.data.DataLoader(\n",
    "    torch.utils.data.Subset(train_dset, valid_examples1), batch_size=500, shuffle=True, num_workers=8\n",
    ")\n",
    "train_aug_loader2 = torch.utils.data.DataLoader(\n",
    "    torch.utils.data.Subset(train_dset, valid_examples2), batch_size=500, shuffle=True, num_workers=8\n",
    ")\n",
    "\n",
    "test_valid_examples1 = [i for i, (_, label) in tqdm(enumerate(test_dset)) if label in model1_classes]\n",
    "test_valid_examples2 = [i for i, (_, label) in tqdm(enumerate(test_dset)) if label in model2_classes]\n",
    "\n",
    "test_loader1 = torch.utils.data.DataLoader(\n",
    "    torch.utils.data.Subset(test_dset, test_valid_examples1), batch_size=500, shuffle=False, num_workers=8\n",
    ")\n",
    "test_loader2 = torch.utils.data.DataLoader(\n",
    "    torch.utils.data.Subset(test_dset, test_valid_examples2), batch_size=500, shuffle=False, num_workers=8\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4f4e65ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n",
       "        18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,\n",
       "        36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49,  0,  1,  2,  3,\n",
       "         4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21,\n",
       "        22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39,\n",
       "        40, 41, 42, 43, 44, 45, 46, 47, 48, 49])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_idxs = np.zeros(100, dtype=int)\n",
    "class_idxs[model1_classes] = np.arange(50)\n",
    "class_idxs[model2_classes] = np.arange(50)\n",
    "class_idxs = torch.from_numpy(class_idxs)\n",
    "class_idxs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f950449a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import clip\n",
    "\n",
    "text_inputs = torch.cat([clip.tokenize(f\"a photo of a {c}\") for c in test_dset.classes]).to(DEVICE)\n",
    "model, preprocess = clip.load('ViT-B/32', DEVICE)\n",
    "with torch.no_grad():\n",
    "    text_features = model.encode_text(text_inputs)\n",
    "\n",
    "text_features /= text_features.norm(dim=-1, keepdim=True)\n",
    "class_vecs1 = text_features[model1_classes]\n",
    "class_vecs2 = text_features[model2_classes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "eb5f3218",
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluates accuracy\n",
    "def evaluate_texthead(model, loader, class_vectors, remap_class_idxs=None, return_confusion=False):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    confusion = np.zeros((100, 100))\n",
    "    \n",
    "    totals = [0] * class_vectors.shape[0]\n",
    "    corrects = [0] * class_vectors.shape[0]\n",
    "    \n",
    "    with torch.no_grad(), autocast():\n",
    "        for inputs, labels in loader:\n",
    "            encodings = model(inputs.to(DEVICE))\n",
    "            normed_encodings = encodings / encodings.norm(dim=-1, keepdim=True)\n",
    "            outputs = normed_encodings @ class_vectors.T\n",
    "            pred = outputs.argmax(dim=1)\n",
    "            if remap_class_idxs is not None:\n",
    "                correct += (remap_class_idxs[labels].to(DEVICE) == pred).sum().item()\n",
    "            else:\n",
    "                for gt, p in zip(labels, pred):\n",
    "                    totals[gt] += 1\n",
    "                    \n",
    "                    if gt == p:\n",
    "                        correct += 1\n",
    "                        corrects[gt] += 1\n",
    "                \n",
    "#                 correct += (labels.to(DEVICE) == pred).sum().item()\n",
    "                \n",
    "            confusion[labels.cpu().numpy(), pred.cpu().numpy()] += 1\n",
    "            total += inputs.shape[0]\n",
    "    if return_confusion:\n",
    "        return correct / sum(totals), list(map(lambda a: a[0] / a[1], zip(corrects, totals)))\n",
    "    else:\n",
    "        return correct / total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "10b3639d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use the train loader with data augmentation as this gives better results\n",
    "def reset_bn_stats(model, epochs=1, loader=train_aug_loader):\n",
    "    # resetting stats to baseline first as below is necessary for stability\n",
    "    for m in model.modules():\n",
    "        if type(m) == nn.BatchNorm2d:\n",
    "            m.momentum = None # use simple average\n",
    "            m.reset_running_stats()\n",
    "    # run a single train epoch with augmentations to recalc stats\n",
    "    model.train()\n",
    "    for _ in range(epochs):\n",
    "        with torch.no_grad(), autocast():\n",
    "            for images, _ in loader:\n",
    "                output = model(images.to(DEVICE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a178f7b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bipartite_soft_matching(\n",
    "    metric: torch.Tensor,\n",
    "    r: float,\n",
    "    class_token: bool = False,\n",
    "    distill_token: bool = False,\n",
    "):\n",
    "    \"\"\"\n",
    "    Applies ToMe with a balanced matching set (50%, 50%).\n",
    "    Input size is [batch, tokens, channels].\n",
    "    r indicates the ratio of tokens to remove (max 50% of tokens).\n",
    "    Extra args:\n",
    "     - class_token: Whether or not there's a class token.\n",
    "     - distill_token: Whether or not there's also a distillation token.\n",
    "    When enabled, the class token and distillation tokens won't get merged.\n",
    "    \"\"\"\n",
    "    protected = 0\n",
    "    if class_token:\n",
    "        protected += 1\n",
    "    if distill_token:\n",
    "        protected += 1\n",
    "\n",
    "    # We can only reduce by a maximum of 50% tokens\n",
    "    t = metric.shape[1]\n",
    "    r = int(r * t)\n",
    "    r = min(r, (t - protected) // 2)\n",
    "\n",
    "    if r <= 0:\n",
    "        return do_nothing, do_nothing\n",
    "\n",
    "    with torch.no_grad():\n",
    "        metric = metric / metric.norm(dim=-1, keepdim=True)\n",
    "        a, b = metric.chunk(2, dim=-2)\n",
    "        scores = a @ b.transpose(-1, -2)\n",
    "\n",
    "        if class_token:\n",
    "            scores[..., 0, :] = -math.inf\n",
    "        if distill_token:\n",
    "            scores[..., :, 0] = -math.inf\n",
    "\n",
    "        node_max, node_idx = scores.max(dim=-1)\n",
    "        edge_idx = node_max.argsort(dim=-1, descending=True)[..., None]\n",
    "\n",
    "        unm_idx = edge_idx[..., r:, :]  # Unmerged Tokens\n",
    "        src_idx = edge_idx[..., :r, :]  # Merged Tokens\n",
    "        dst_idx = node_idx[..., None].gather(dim=-2, index=src_idx)\n",
    "\n",
    "        if class_token:\n",
    "            # Sort to ensure the class token is at the start\n",
    "            unm_idx = unm_idx.sort(dim=1)[0]\n",
    "\n",
    "    def merge(x: torch.Tensor, mode=\"mean\") -> torch.Tensor:\n",
    "        src, dst = x.chunk(2, dim=-2)\n",
    "        n, t1, c = src.shape\n",
    "        unm = src.gather(dim=-2, index=unm_idx.expand(n, t1 - r, c))\n",
    "        src = src.gather(dim=-2, index=src_idx.expand(n, r, c))\n",
    "        dst = dst.scatter_reduce(-2, dst_idx.expand(n, r, c), src, reduce=mode)\n",
    "\n",
    "        if distill_token:\n",
    "            return torch.cat([unm[:, :1], dst[:, :1], unm[:, 1:], dst[:, 1:]], dim=1)\n",
    "        else:\n",
    "            return torch.cat([unm, dst], dim=1)\n",
    "\n",
    "    def unmerge(x: torch.Tensor) -> torch.Tensor:\n",
    "        unm_len = unm_idx.shape[1]\n",
    "        unm, dst = x[..., :unm_len, :], x[..., unm_len:, :]\n",
    "        n, _, c = unm.shape\n",
    "\n",
    "        src = dst.gather(dim=-2, index=dst_idx.expand(n, r, c))\n",
    "\n",
    "        out = torch.zeros(n, metric.shape[1], c, device=x.device, dtype=x.dtype)\n",
    "\n",
    "        out[..., dst.shape[-2]:, :] = dst\n",
    "        out.scatter_(dim=-2, index=(unm_idx).expand(n, unm_len, c), src=unm)\n",
    "        out.scatter_(dim=-2, index=(src_idx).expand(n, r, c), src=src)\n",
    "\n",
    "        return out\n",
    "\n",
    "    return merge, unmerge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9fcf3324",
   "metadata": {},
   "outputs": [],
   "source": [
    "def concat_mats(args, dim=0):\n",
    "    return torch.cat(args, dim=dim)\n",
    "\n",
    "def unconcat_mat(tensor, dim=0):\n",
    "    return torch.chunk(tensor, chunks=2, dim=dim)\n",
    "\n",
    "def match_tensors_permute(hull_tensor, eps=1e-7, interleave=False, random_perm=False):\n",
    "    \"\"\"\n",
    "    hull_tensor: [2O,I]\n",
    "    \"\"\"\n",
    "    O, I = hull_tensor.shape\n",
    "    O //= 2\n",
    "    \n",
    "    interleave_mat = torch.eye(2*O, device=hull_tensor.device)\n",
    "    if interleave:\n",
    "        A1, A2, B1, B2 = interleave_mat.chunk(4, dim=0)\n",
    "        interleave_mat = torch.cat([A1, B1, A2, B2], dim=0)\n",
    "        interleave_mat = interleave_mat.view(2, O, 2*O).transpose(0, 1).reshape(2*O, 2*O)\n",
    "#         interleave_mat = interleave_mat[torch.randperm(2*O, device=hull_tensor.device)]\n",
    "    \n",
    "    hull_tensor = interleave_mat @ hull_tensor\n",
    "    \n",
    "    hull_tensor = hull_tensor / (hull_tensor.norm(dim=-1, keepdim=True) + eps)\n",
    "    A, B = unconcat_mat(hull_tensor, dim=0)\n",
    "    scores = -(A @ B.T)\n",
    "    \n",
    "    O_eye = torch.eye(O, device=hull_tensor.device)\n",
    "    \n",
    "    try:\n",
    "        row_idx, col_idx = scipy.optimize.linear_sum_assignment(scores.cpu().numpy())\n",
    "    except ValueError:\n",
    "        pdb.set_trace()\n",
    "    \n",
    "    A_perm = O_eye[torch.from_numpy(row_idx)]#[perm]\n",
    "    B_perm = O_eye[torch.from_numpy(col_idx)]#[perm]\n",
    "    \n",
    "    if random_perm:\n",
    "        perm = torch.randperm(O, device=A.device)\n",
    "        A_perm = A_perm[perm]\n",
    "        B_perm = B_perm[perm]\n",
    "    \n",
    "    merge = (torch.cat((A_perm, B_perm), dim=1) / 2.) @ interleave_mat\n",
    "    unmerge = interleave_mat.T @ (torch.cat((A_perm.T, B_perm.T), dim=0))\n",
    "    return merge, unmerge\n",
    "\n",
    "\n",
    "def match_tensors_tome(hull_tensor, eps=1e-7, interleave=False, random_perm=False):\n",
    "    \"\"\"\n",
    "    hull_tensor: [2O,I]\n",
    "    \"\"\"\n",
    "    O, I = hull_tensor.shape\n",
    "    O //= 2\n",
    "    \n",
    "    big_eye = torch.eye(2*O, device=hull_tensor.device)\n",
    "    small_eye = torch.eye(O, device=hull_tensor.device)\n",
    "    \n",
    "    interleave_mat = big_eye\n",
    "    if interleave:\n",
    "        A1, A2, B1, B2 = interleave_mat.chunk(4, dim=0)\n",
    "        interleave_mat = torch.cat([A1, B1, A2, B2], dim=0)\n",
    "    \n",
    "    \n",
    "    hull_tensor = interleave_mat @ hull_tensor\n",
    "    \n",
    "    merge, unmerge = bipartite_soft_matching(hull_tensor[None], 0.5)\n",
    "    \n",
    "    merge_mat = merge(big_eye[None])[0] @ interleave_mat\n",
    "    unmerge_mat = interleave_mat.T @ unmerge(small_eye[None])[0]\n",
    "    return merge_mat, unmerge_mat\n",
    "\n",
    "def match_tensors_svd(hull_tensor, use_S=True):\n",
    "    # We can only reduce by a maximum of 50% tokens\n",
    "    t = hull_tensor.shape[0]\n",
    "    r = int(.5 * t)\n",
    "    r = min(r, (t) // 2)\n",
    "    with torch.no_grad():\n",
    "        hull_tensor = hull_tensor / hull_tensor.norm(dim=-1, keepdim=True)\n",
    "        scores = hull_tensor @ hull_tensor.transpose(-1, -2)\n",
    "        U, S, V = torch.svd(scores)\n",
    "        \n",
    "        U_r = U[:, :r]\n",
    "        S_r = torch.diag(S[:r]) if use_S else torch.eye(r, device=DEVICE)\n",
    "        V_r = V[:, :r]\n",
    "    merge_mat = U_r\n",
    "    unmerge_mat = S_r @ V_r.mT\n",
    "    return merge_mat.T, unmerge_mat.T\n",
    "\n",
    "# match_tensors = match_tensors\n",
    "\n",
    "def match_wrapper(fn, interleave=False, random_perm=False):\n",
    "    return lambda x: fn(x, interleave=interleave, random_perm=random_perm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "925b9980",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerTransform(dict):\n",
    "    def __init__(self, normalize_tensors=False):\n",
    "        super(LayerTransform, self).__init__()\n",
    "        self.output_align = None\n",
    "        self.next_input_align = None\n",
    "        self.normalize_tensors = normalize_tensors\n",
    "    \n",
    "    def compute_transform(self):\n",
    "        inputs = list(self.values())\n",
    "        if self.normalize_tensors:\n",
    "            for idx, inp in enumerate(inputs):\n",
    "                inputs[idx] = F.normalize(inp, dim=-1)\n",
    "        self.output_align, self.next_input_align = match_tensors(concat_mats(inputs, dim=-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8796a855",
   "metadata": {},
   "outputs": [],
   "source": [
    "def unflatten(x, k=3):\n",
    "    O, IHW = x.shape\n",
    "    return x.view(O, -1, k, k)\n",
    "\n",
    "def merge_first_convs(state_dict, prefix, a_conv, b_conv, output_transform):\n",
    "    flatten_conv = lambda x: x.flatten(1)\n",
    "    a_w = flatten_conv(a_conv.weight)\n",
    "    b_w = flatten_conv(b_conv.weight)\n",
    "    ab_w = concat_mats((a_w, b_w), dim=0)\n",
    "    output_transform[prefix] = ab_w\n",
    "    output_transform.compute_transform()\n",
    "    # merge_mat, unmerge_mat = match_tensors(ab_w)\n",
    "    c_w = output_transform.output_align @ ab_w\n",
    "    state_dict[prefix + '.weight'] = unflatten(c_w, a_conv.weight.shape[-1])\n",
    "    return output_transform\n",
    "\n",
    "def merge_bn(state_dict, prefix, a_bn, b_bn, output_transform):\n",
    "    staterify = lambda bn: torch.stack((bn.weight, bn.bias, bn.running_mean), dim=1)\n",
    "    unstaterify = lambda stats: stats.unbind(-1)\n",
    "    \n",
    "    a_stats = staterify(a_bn)\n",
    "    b_stats = staterify(b_bn)\n",
    "    ab_stats = concat_mats((a_stats, b_stats), dim=0)\n",
    "    c_stats = output_transform.output_align @ ab_stats\n",
    "    c_weight, c_bias, c_mean = unstaterify(c_stats)\n",
    "    ab_var = concat_mats((a_bn.running_var[..., None], b_bn.running_var[...,None]))\n",
    "    var_out_transform = output_transform.output_align.square()\n",
    "    c_var = (var_out_transform @ ab_var).reshape(-1)\n",
    "    state_dict[prefix + '.weight'] = c_weight\n",
    "    state_dict[prefix + '.bias'] = c_bias\n",
    "    state_dict[prefix + '.running_mean'] = c_mean\n",
    "    state_dict[prefix + '.running_var'] = c_var\n",
    "    pass\n",
    "\n",
    "def block_diagonalize_tensors(tensor1, tensor2):\n",
    "    zerooos = torch.zeros_like(tensor1)\n",
    "    block_diagonal = concat_mats(\n",
    "        (\n",
    "            concat_mats((tensor1, zerooos), dim=1),\n",
    "            concat_mats((zerooos, tensor2), dim=1),\n",
    "        ),\n",
    "        dim=0\n",
    "    )\n",
    "    return block_diagonal\n",
    "\n",
    "def merge_hidden_conv(\n",
    "    state_dict, prefix, a_conv, b_conv, \n",
    "    input_transform, output_transform, \n",
    "    recompute_output=False\n",
    "):\n",
    "    O, I, H, W = a_conv.weight.shape\n",
    "    # Align output spaces for global iterations\n",
    "#     if output_transform.output_align is not None:\n",
    "#         output_block_diagonal_ab = block_diagonalize_tensors(\n",
    "#             a_conv.weight.flatten(2),\n",
    "#             b_conv.weight.flatten(2)\n",
    "#         )\n",
    "#         ab_output_aligned = output_transform.output_align @ output_block_diagonal_ab.flatten(1)\n",
    "#         ab_output_aligned = ab_output_aligned.reshape(O, 2 * I, H*W).transpose(1, 0).flatten(1)\n",
    "#         input_transform[prefix] = ab_output_aligned\n",
    "    \n",
    "    get_I_by_O_by_HW = lambda x: x.permute(1, 0, 2, 3).flatten(2)\n",
    "    \n",
    "    a_I_by_O_by_HW = get_I_by_O_by_HW(a_conv.weight)\n",
    "    b_I_by_O_by_HW = get_I_by_O_by_HW(b_conv.weight)\n",
    "    \n",
    "    dummy_zerooooo = torch.zeros_like(b_I_by_O_by_HW)\n",
    "    ab_block_diago = concat_mats(\n",
    "        (\n",
    "            concat_mats((a_I_by_O_by_HW, dummy_zerooooo), dim=1),\n",
    "            concat_mats((dummy_zerooooo, b_I_by_O_by_HW), dim=1)\n",
    "        ),\n",
    "        dim=0\n",
    "    )\n",
    "    \n",
    "    # [I,2I]x[2I,2OHW]->[I,2OHW]\n",
    "    ab_input_aligned = input_transform.next_input_align.T @ ab_block_diago.flatten(1)\n",
    "    ab_input_aligned = ab_input_aligned.\\\n",
    "    reshape(I, 2 * O, H*W).\\\n",
    "    transpose(1, 0).\\\n",
    "    flatten(1) # [I,2O,HW]->[2O,I,HW]->[2O,IHW]\n",
    "    output_transform[prefix] = ab_input_aligned\n",
    "    if recompute_output:\n",
    "        output_transform.compute_transform()\n",
    "    c_flat = output_transform.output_align @ ab_input_aligned\n",
    "    state_dict[prefix + '.weight'] = unflatten(c_flat, a_conv.weight.shape[-1])\n",
    "    \n",
    "    return output_transform\n",
    "\n",
    "def merge_linear(\n",
    "    state_dict, prefix, a_linear, \n",
    "    b_linear, input_transform, \n",
    "    output_transform, \n",
    "    recompute_output=False\n",
    "):\n",
    "    class conv_wrapper:\n",
    "        def __init__(self, linear):\n",
    "            self.weight = linear.weight[:, :, None, None]\n",
    "    \n",
    "    output_transform = merge_hidden_conv(\n",
    "        state_dict, prefix, conv_wrapper(a_linear), \n",
    "        conv_wrapper(b_linear), input_transform, \n",
    "        output_transform, recompute_output=recompute_output\n",
    "    )\n",
    "    state_dict[prefix + '.weight'] = state_dict[prefix + '.weight'][..., 0, 0]\n",
    "    state_dict[prefix + '.bias'] = output_transform.output_align @ concat_mats((a_linear.bias, b_linear.bias))\n",
    "    return output_transform\n",
    "    \n",
    "def merge_block(\n",
    "    state_dict, prefix, a_block, b_block, \n",
    "    input_transform, intra_transform,\n",
    "    output_transform=None, shortcut=False\n",
    "):\n",
    "    conv1_transform = merge_hidden_conv(\n",
    "        state_dict, prefix + '.conv1', a_block.conv1, b_block.conv1, \n",
    "        input_transform, intra_transform, recompute_output=True\n",
    "    )\n",
    "    merge_bn(state_dict, prefix + '.bn1', a_block.bn1, b_block.bn1, conv1_transform)\n",
    "    \n",
    "    \n",
    "    conv2_transform = merge_hidden_conv(\n",
    "        state_dict, \n",
    "        prefix + '.conv2', \n",
    "        a_block.conv2, \n",
    "        b_block.conv2, \n",
    "        conv1_transform,\n",
    "        output_transform,\n",
    "        recompute_output=shortcut\n",
    "    )\n",
    "    merge_bn(state_dict, prefix + '.bn2', a_block.bn2, b_block.bn2, conv2_transform)\n",
    "    \n",
    "    if shortcut:\n",
    "        shortcut_transform = merge_hidden_conv(\n",
    "            state_dict, \n",
    "            prefix + '.shortcut.0', \n",
    "            a_block.shortcut[0], \n",
    "            b_block.shortcut[0], \n",
    "            input_transform,\n",
    "            output_transform=conv2_transform\n",
    "        )\n",
    "        merge_bn(\n",
    "            state_dict, \n",
    "            prefix + '.shortcut.1', \n",
    "            a_block.shortcut[1], \n",
    "            b_block.shortcut[1], \n",
    "            shortcut_transform\n",
    "        )\n",
    "    \n",
    "    return conv2_transform\n",
    "\n",
    "hard_pass = lambda : None\n",
    "\n",
    "def merge_resnet20(state_dict, a, b, transforms):\n",
    "    transforms['conv1'] = merge_first_convs(\n",
    "        state_dict, 'conv1', a.conv1, b.conv1, \n",
    "        output_transform=transforms['conv1']\n",
    "    )\n",
    "    merge_bn(state_dict, 'bn1', a.bn1, b.bn1, transforms['conv1'])\n",
    "    \n",
    "    for i in range(3):\n",
    "        merge_block(\n",
    "            state_dict, f'layer1.{i}', a.layer1[i], b.layer1[i], \n",
    "            input_transform=transforms['conv1'], \n",
    "            intra_transform=transforms[f'block1.{i}'],\n",
    "            output_transform=transforms['conv1'],\n",
    "            shortcut=False\n",
    "        )\n",
    "    \n",
    "    transforms['block2'] = merge_block(\n",
    "        state_dict, 'layer2.0', a.layer2[0], b.layer2[0], \n",
    "        input_transform=transforms['conv1'], \n",
    "        intra_transform=transforms[f'block2.0'],\n",
    "        output_transform=transforms['block2'],\n",
    "        shortcut=True\n",
    "    )\n",
    "    \n",
    "    for i in range(1, 3):\n",
    "        merge_block(\n",
    "            state_dict, f'layer2.{i}', a.layer2[i], b.layer2[i], \n",
    "            input_transform=transforms['block2'], \n",
    "            intra_transform=transforms[f'block2.{i}'],\n",
    "            output_transform=transforms['block2'],\n",
    "            shortcut=False\n",
    "        )\n",
    "        \n",
    "    transforms['block3'] = merge_block(\n",
    "        state_dict, 'layer3.0', a.layer3[0], b.layer3[0], \n",
    "        input_transform=transforms['block2'], \n",
    "        intra_transform=transforms[f'block3.0'],\n",
    "        output_transform=transforms['block3'],\n",
    "        shortcut=True\n",
    "    )\n",
    "    for i in range(1, 3):\n",
    "        merge_block(\n",
    "            state_dict, f'layer3.{i}', a.layer3[i], b.layer3[i], \n",
    "            input_transform=transforms['block3'], \n",
    "            intra_transform=transforms[f'block3.{i}'],\n",
    "            output_transform=transforms['block3'],\n",
    "            shortcut=False\n",
    "        )\n",
    "        \n",
    "    output_align_identity = torch.eye(a.linear.weight.shape[0], device=a.linear.weight.device)\n",
    "    output_align_mat = torch.cat((output_align_identity/2, output_align_identity/2), dim=1)\n",
    "    transforms['linear'].output_align = output_align_mat\n",
    "    transforms['linear'] = merge_linear(\n",
    "        state_dict, 'linear', a.linear, b.linear, \n",
    "        transforms['block3'], transforms['linear'],\n",
    "        recompute_output=False\n",
    "    )\n",
    "    \n",
    "    return transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3a0361d",
   "metadata": {},
   "source": [
    "# Merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "258f1764",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.778\n",
      "0.7758\n"
     ]
    }
   ],
   "source": [
    "model1 = resnet20(w=4, text_head=True).to(DEVICE)\n",
    "model2 = resnet20(w=4, text_head=True).to(DEVICE)\n",
    "load_model(model1, f'resnet20x4_CIFAR50_clses{model1_classes.tolist()}')\n",
    "load_model(model2, f'resnet20x4_CIFAR50_clses{model2_classes.tolist()}')\n",
    "\n",
    "print(evaluate_texthead(model1, test_loader1, class_vecs1, remap_class_idxs=class_idxs))\n",
    "print(evaluate_texthead(model2, test_loader2, class_vecs2, remap_class_idxs=class_idxs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "ee8826b5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.2358\n",
      "1 0.2704\n",
      "2 0.2969\n",
      "3 0.3169\n",
      "4 0.3115\n",
      "5 0.2968\n",
      "6 0.2903\n",
      "7 0.308\n",
      "8 0.2965\n",
      "9 0.3137\n",
      "10 0.299\n",
      "11 0.3004\n",
      "12 0.3002\n",
      "13 0.2819\n",
      "14 0.2977\n",
      "15 0.2981\n",
      "16 0.2997\n",
      "17 0.3303\n",
      "18 0.307\n",
      "19 0.2957\n",
      "20 0.3049\n",
      "21 0.3056\n",
      "22 0.3222\n",
      "23 0.322\n",
      "24 0.3254\n",
      "25 0.313\n",
      "26 0.3165\n",
      "27 0.2969\n",
      "28 0.3247\n",
      "29 0.3149\n",
      "30 0.303\n",
      "31 0.3002\n",
      "32 0.3023\n",
      "33 0.3202\n",
      "34 0.3083\n",
      "35 0.325\n",
      "36 0.3216\n",
      "37 0.3208\n",
      "38 0.312\n",
      "39 0.3204\n",
      "40 0.3162\n",
      "41 0.3164\n",
      "42 0.3179\n",
      "43 0.3192\n",
      "44 0.3085\n",
      "45 0.2998\n",
      "46 0.3081\n",
      "47 0.3263\n",
      "48 0.301\n",
      "49 0.3313\n",
      "50 0.2892\n",
      "51 0.3055\n",
      "52 0.3168\n",
      "53 0.3228\n",
      "54 0.3166\n",
      "55 0.3056\n",
      "56 0.3169\n",
      "57 0.3156\n",
      "58 0.3163\n",
      "59 0.3327\n",
      "60 0.3014\n",
      "61 0.3064\n",
      "62 0.3093\n",
      "63 0.3168\n",
      "64 0.3166\n",
      "65 0.3206\n",
      "66 0.2967\n",
      "67 0.3151\n",
      "68 0.316\n",
      "69 0.3303\n",
      "70 0.3439\n",
      "71 0.3264\n",
      "72 0.325\n",
      "73 0.3059\n",
      "74 0.3169\n",
      "75 0.3088\n",
      "76 0.321\n",
      "77 0.3071\n",
      "78 0.3166\n",
      "79 0.3163\n",
      "80 0.3341\n",
      "81 0.2984\n",
      "82 0.3175\n",
      "83 0.3155\n",
      "84 0.3102\n",
      "85 0.3104\n",
      "86 0.3173\n",
      "87 0.3056\n",
      "88 0.3104\n",
      "89 0.3264\n",
      "90 0.3254\n",
      "91 0.3232\n",
      "92 0.3265\n",
      "93 0.3116\n",
      "94 0.3256\n",
      "95 0.3109\n",
      "96 0.3126\n",
      "97 0.316\n",
      "98 0.3195\n",
      "99 0.2955\n"
     ]
    }
   ],
   "source": [
    "# Can choose between:\n",
    "# match_tensors_tome: ToMe with or without interleaving\n",
    "# match_tensors_permute: \n",
    "# match_tensors_svd\n",
    "match_tensors = match_wrapper(match_tensors_permute, interleave=False, random_perm=False)\n",
    "layer_transform = lambda : LayerTransform(normalize_tensors=False)\n",
    "state_dict = {}\n",
    "old_transforms = defaultdict(lambda: layer_transform())\n",
    "new_transforms = defaultdict(lambda: layer_transform())\n",
    "modelc = resnet20(w=4, text_head=True).to(DEVICE)\n",
    "best_info = {'acc': 0.}\n",
    "step = 1\n",
    "# while not is_converged(old_transforms, new_transforms):\n",
    "for step in range(100):\n",
    "    old_transforms = new_transforms\n",
    "    new_transforms = merge_resnet20(state_dict, model2, model1, transforms=deepcopy(old_transforms))\n",
    "    modelc.load_state_dict(state_dict)\n",
    "    reset_bn_stats(modelc)\n",
    "    acc, perclass_acc = evaluate_texthead(\n",
    "        modelc, test_loader, class_vectors=text_features, return_confusion=True\n",
    "    )\n",
    "    if acc > best_info['acc']:\n",
    "        best_info['acc'] = acc\n",
    "        best_info['perclass_acc'] = perclass_acc\n",
    "    print(step, acc)\n",
    "    step += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "7863cfe7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3439\n",
      "[0.02, 0.02, 0.0, 0.09, 0.31, 0.22, 0.18, 0.28, 0.55, 0.38, 0.25, 0.01, 0.36, 0.38, 0.27, 0.17, 0.42, 0.46, 0.27, 0.13, 0.87, 0.33, 0.13, 0.64, 0.18, 0.22, 0.09, 0.28, 0.34, 0.44, 0.53, 0.16, 0.26, 0.33, 0.42, 0.3, 0.62, 0.23, 0.2, 0.33, 0.29, 0.3, 0.24, 0.57, 0.15, 0.2, 0.13, 0.57, 0.38, 0.35, 0.07, 0.45, 0.48, 0.63, 0.52, 0.09, 0.67, 0.59, 0.65, 0.56, 0.48, 0.41, 0.66, 0.09, 0.22, 0.26, 0.28, 0.05, 0.74, 0.27, 0.29, 0.29, 0.12, 0.2, 0.48, 0.36, 0.76, 0.1, 0.38, 0.53, 0.08, 0.38, 0.52, 0.25, 0.16, 0.44, 0.51, 0.61, 0.43, 0.66, 0.4, 0.2, 0.33, 0.14, 0.8, 0.43, 0.17, 0.09, 0.72, 0.54]\n"
     ]
    }
   ],
   "source": [
    "print(best_info['acc'])\n",
    "print(best_info['perclass_acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "258da18f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
