{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ac77b920",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/srv/share/gstoica3/miniconda3/envs/open-mmlab/lib/python3.7/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<torch.autograd.grad_mode.set_grad_enabled at 0x7f6f44443510>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import pdb\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.optimize\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from torch.optim import SGD, Adam, lr_scheduler\n",
    "import torchvision\n",
    "import torchvision.transforms as T\n",
    "\n",
    "from sys import platform\n",
    "\n",
    "DEVICE = 'mps' if platform == 'darwin' else 'cuda'\n",
    "if DEVICE == 'mps':\n",
    "    DOWNLOAD_PATH = '/Users/georgestoica/Downloads' \n",
    "else:\n",
    "    DOWNLOAD_PATH = '/srv/share/gstoica3/checkpoints/REPAIR/'\n",
    "    \n",
    "torch.autograd.set_grad_enabled(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eecce0b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "24850364",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(model, i):\n",
    "    sd = model.state_dict()\n",
    "    path = os.path.join(\n",
    "        DOWNLOAD_PATH,\n",
    "        '%s.pth.tar' % i\n",
    "    )\n",
    "    torch.save(model.state_dict(), path)\n",
    "\n",
    "def load_model(model, i):\n",
    "    path = os.path.join(\n",
    "        DOWNLOAD_PATH,\n",
    "        '%s.pth.tar' % i\n",
    "    )\n",
    "    sd = torch.load(path, map_location=torch.device(DEVICE))\n",
    "    model.load_state_dict(sd)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f77d8acf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "CIFAR_MEAN = [125.307, 122.961, 113.8575]\n",
    "CIFAR_STD = [51.5865, 50.847, 51.255]\n",
    "normalize = T.Normalize(np.array(CIFAR_MEAN)/255, np.array(CIFAR_STD)/255)\n",
    "denormalize = T.Normalize(-np.array(CIFAR_MEAN)/np.array(CIFAR_STD), 255/np.array(CIFAR_STD))\n",
    "\n",
    "train_transform = T.Compose([\n",
    "    T.RandomHorizontalFlip(),\n",
    "    T.RandomCrop(32, padding=4),\n",
    "    T.ToTensor(),\n",
    "    normalize,\n",
    "])\n",
    "test_transform = T.Compose([\n",
    "    T.ToTensor(),\n",
    "    normalize,\n",
    "])\n",
    "train_dset = torchvision.datasets.CIFAR100(\n",
    "    root='/nethome/gstoica3/research/pytorch-cifar100/data/cifar-100-python', \n",
    "    train=True,\n",
    "    download=True, transform=train_transform\n",
    ")\n",
    "test_dset = torchvision.datasets.CIFAR100(\n",
    "    root='/nethome/gstoica3/research/pytorch-cifar100/data/cifar-100-python',\n",
    "    train=False,\n",
    "    download=True, \n",
    "    transform=test_transform\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2714eefc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "50000it [00:13, 3664.91it/s]\n",
      "50000it [00:13, 3657.42it/s]\n",
      "10000it [00:01, 5495.08it/s]\n",
      "10000it [00:01, 5537.26it/s]\n"
     ]
    }
   ],
   "source": [
    "train_aug_loader = torch.utils.data.DataLoader(train_dset, batch_size=500, shuffle=True, num_workers=8)\n",
    "test_loader = torch.utils.data.DataLoader(test_dset, batch_size=500, shuffle=False, num_workers=8)\n",
    "\n",
    "model1_classes= np.arange(50) # np.array([3, 2, 0, 6, 4])\n",
    "model2_classes = np.arange(50, 100) # np.array([5, 7, 9, 8, 1])\n",
    "\n",
    "valid_examples1 = [i for i, (_, label) in tqdm(enumerate(train_dset)) if label in model1_classes]\n",
    "valid_examples2 = [i for i, (_, label) in tqdm(enumerate(train_dset)) if label in model2_classes]\n",
    "\n",
    "assert len(set(valid_examples1).intersection(set(valid_examples2))) == 0, 'sets should be disjoint'\n",
    "\n",
    "train_aug_loader1 = torch.utils.data.DataLoader(\n",
    "    torch.utils.data.Subset(train_dset, valid_examples1), batch_size=500, shuffle=True, num_workers=8\n",
    ")\n",
    "train_aug_loader2 = torch.utils.data.DataLoader(\n",
    "    torch.utils.data.Subset(train_dset, valid_examples2), batch_size=500, shuffle=True, num_workers=8\n",
    ")\n",
    "\n",
    "test_valid_examples1 = [i for i, (_, label) in tqdm(enumerate(test_dset)) if label in model1_classes]\n",
    "test_valid_examples2 = [i for i, (_, label) in tqdm(enumerate(test_dset)) if label in model2_classes]\n",
    "\n",
    "test_loader1 = torch.utils.data.DataLoader(\n",
    "    torch.utils.data.Subset(test_dset, test_valid_examples1), batch_size=500, shuffle=False, num_workers=8\n",
    ")\n",
    "test_loader2 = torch.utils.data.DataLoader(\n",
    "    torch.utils.data.Subset(test_dset, test_valid_examples2), batch_size=500, shuffle=False, num_workers=8\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "02430130",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n",
       "        18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,\n",
       "        36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49,  0,  1,  2,  3,\n",
       "         4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21,\n",
       "        22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39,\n",
       "        40, 41, 42, 43, 44, 45, 46, 47, 48, 49])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_idxs = np.zeros(100, dtype=int)\n",
    "class_idxs[model1_classes] = np.arange(50)\n",
    "class_idxs[model2_classes] = np.arange(50)\n",
    "class_idxs = torch.from_numpy(class_idxs)\n",
    "class_idxs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "bc9e159c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['apple', 'aquarium_fish', 'baby', 'bear', 'beaver', 'bed', 'bee', 'beetle', 'bicycle', 'bottle', 'bowl', 'boy', 'bridge', 'bus', 'butterfly', 'camel', 'can', 'castle', 'caterpillar', 'cattle', 'chair', 'chimpanzee', 'clock', 'cloud', 'cockroach', 'couch', 'crab', 'crocodile', 'cup', 'dinosaur', 'dolphin', 'elephant', 'flatfish', 'forest', 'fox', 'girl', 'hamster', 'house', 'kangaroo', 'keyboard', 'lamp', 'lawn_mower', 'leopard', 'lion', 'lizard', 'lobster', 'man', 'maple_tree', 'motorcycle', 'mountain', 'mouse', 'mushroom', 'oak_tree', 'orange', 'orchid', 'otter', 'palm_tree', 'pear', 'pickup_truck', 'pine_tree', 'plain', 'plate', 'poppy', 'porcupine', 'possum', 'rabbit', 'raccoon', 'ray', 'road', 'rocket', 'rose', 'sea', 'seal', 'shark', 'shrew', 'skunk', 'skyscraper', 'snail', 'snake', 'spider', 'squirrel', 'streetcar', 'sunflower', 'sweet_pepper', 'table', 'tank', 'telephone', 'television', 'tiger', 'tractor', 'train', 'trout', 'tulip', 'turtle', 'wardrobe', 'whale', 'willow_tree', 'wolf', 'woman', 'worm']\n"
     ]
    }
   ],
   "source": [
    "print(test_dset.classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c95a16d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import clip\n",
    "\n",
    "text_inputs = torch.cat([clip.tokenize(f\"a photo of a {c}\") for c in test_dset.classes]).to(DEVICE)\n",
    "model, preprocess = clip.load('ViT-B/32', DEVICE)\n",
    "with torch.no_grad():\n",
    "    text_features = model.encode_text(text_inputs)\n",
    "\n",
    "\n",
    "text_features /= text_features.norm(dim=-1, keepdim=True)\n",
    "class_vecs1 = text_features[model1_classes]\n",
    "class_vecs2 = text_features[model2_classes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "38c3965a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluates accuracy\n",
    "def evaluate_texthead(model, loader, class_vectors, remap_class_idxs=None, return_confusion=False):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    confusion = np.zeros((100, 100))\n",
    "    \n",
    "    totals = [0] * class_vectors.shape[0]\n",
    "    corrects = [0] * class_vectors.shape[0]\n",
    "    \n",
    "    with torch.no_grad(), autocast():\n",
    "        for inputs, labels in loader:\n",
    "            encodings = model(inputs.to(DEVICE))\n",
    "            normed_encodings = encodings / encodings.norm(dim=-1, keepdim=True)\n",
    "            outputs = normed_encodings @ class_vectors.T\n",
    "            pred = outputs.argmax(dim=1)\n",
    "            if remap_class_idxs is not None:\n",
    "                correct += (remap_class_idxs[labels].to(DEVICE) == pred).sum().item()\n",
    "            else:\n",
    "                for gt, p in zip(labels, pred):\n",
    "                    totals[gt] += 1\n",
    "                    \n",
    "                    if gt == p:\n",
    "                        correct += 1\n",
    "                        corrects[gt] += 1\n",
    "                \n",
    "#                 correct += (labels.to(DEVICE) == pred).sum().item()\n",
    "                \n",
    "            confusion[labels.cpu().numpy(), pred.cpu().numpy()] += 1\n",
    "            total += inputs.shape[0]\n",
    "    if return_confusion:\n",
    "        return correct / sum(totals), list(map(lambda a: a[0] / a[1], zip(corrects, totals)))\n",
    "    else:\n",
    "        return correct / total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e3e7a9d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.nn.init as init\n",
    "\n",
    "def _weights_init(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if isinstance(m, nn.Linear) or isinstance(m, nn.Conv2d):\n",
    "        init.kaiming_normal_(m.weight)\n",
    "\n",
    "class LambdaLayer(nn.Module):\n",
    "    def __init__(self, lambd):\n",
    "        super(LambdaLayer, self).__init__()\n",
    "        self.lambd = lambd\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.lambd(x)\n",
    "\n",
    "\n",
    "class BasicBlock(nn.Module):\n",
    "    expansion = 1\n",
    "\n",
    "    def __init__(self, in_planes, planes, stride=1):\n",
    "        super(BasicBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_planes != planes:\n",
    "#             self.shortcut = LambdaLayer(lambda x:\n",
    "#                                         F.pad(x[:, :, ::2, ::2], (0, 0, 0, 0, planes//4, planes//4), \"constant\", 0))\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=False),\n",
    "                nn.BatchNorm2d(planes)\n",
    "            )\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.bn2(self.conv2(out))\n",
    "        out += self.shortcut(x)\n",
    "        out = F.relu(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "class ResNet(nn.Module):\n",
    "    def __init__(self, block, num_blocks, w=1, num_classes=10, text_head=False):\n",
    "        super(ResNet, self).__init__()\n",
    "        self.in_planes = int(w*16)\n",
    "\n",
    "        self.conv1 = nn.Conv2d(3, int(w*16), kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(int(w*16))\n",
    "        self.layer1 = self._make_layer(block, int(w*16), num_blocks[0], stride=1)\n",
    "        self.layer2 = self._make_layer(block, int(w*32), num_blocks[1], stride=2)\n",
    "        self.layer3 = self._make_layer(block, int(w*64), num_blocks[2], stride=2)\n",
    "        if text_head:\n",
    "            num_classes = 512\n",
    "        self.linear = nn.Linear(int(w*64), num_classes)\n",
    "\n",
    "        self.apply(_weights_init)\n",
    "\n",
    "    def _make_layer(self, block, planes, num_blocks, stride):\n",
    "        strides = [stride] + [1]*(num_blocks-1)\n",
    "        layers = []\n",
    "        for stride in strides:\n",
    "            layers.append(block(self.in_planes, planes, stride))\n",
    "            self.in_planes = planes * block.expansion\n",
    "\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.layer1(out)\n",
    "        out = self.layer2(out)\n",
    "        out = self.layer3(out)\n",
    "        out = F.avg_pool2d(out, out.size()[3])\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.linear(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "def resnet20(w=1, text_head=False):\n",
    "    return ResNet(BasicBlock, [3, 3, 3], w=w, text_head=text_head)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5fa7f25a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bipartite_soft_matching(\n",
    "    metric: torch.Tensor,\n",
    "    r: float,\n",
    "    class_token: bool = False,\n",
    "    distill_token: bool = False,\n",
    "):\n",
    "    \"\"\"\n",
    "    Applies ToMe with a balanced matching set (50%, 50%).\n",
    "    Input size is [batch, tokens, channels].\n",
    "    r indicates the ratio of tokens to remove (max 50% of tokens).\n",
    "    Extra args:\n",
    "     - class_token: Whether or not there's a class token.\n",
    "     - distill_token: Whether or not there's also a distillation token.\n",
    "    When enabled, the class token and distillation tokens won't get merged.\n",
    "    \"\"\"\n",
    "    protected = 0\n",
    "    if class_token:\n",
    "        protected += 1\n",
    "    if distill_token:\n",
    "        protected += 1\n",
    "\n",
    "    # We can only reduce by a maximum of 50% tokens\n",
    "    t = metric.shape[1]\n",
    "    r = int(r * t)\n",
    "    r = min(r, (t - protected) // 2)\n",
    "\n",
    "    if r <= 0:\n",
    "        return do_nothing, do_nothing\n",
    "\n",
    "    with torch.no_grad():\n",
    "        metric = metric / metric.norm(dim=-1, keepdim=True)\n",
    "        a, b = metric.chunk(2, dim=-2)\n",
    "        scores = a @ b.transpose(-1, -2)\n",
    "\n",
    "        if class_token:\n",
    "            scores[..., 0, :] = -math.inf\n",
    "        if distill_token:\n",
    "            scores[..., :, 0] = -math.inf\n",
    "\n",
    "        node_max, node_idx = scores.max(dim=-1)\n",
    "        edge_idx = node_max.argsort(dim=-1, descending=True)[..., None]\n",
    "\n",
    "        unm_idx = edge_idx[..., r:, :]  # Unmerged Tokens\n",
    "        src_idx = edge_idx[..., :r, :]  # Merged Tokens\n",
    "        dst_idx = node_idx[..., None].gather(dim=-2, index=src_idx)\n",
    "\n",
    "        if class_token:\n",
    "            # Sort to ensure the class token is at the start\n",
    "            unm_idx = unm_idx.sort(dim=1)[0]\n",
    "\n",
    "    def merge(x: torch.Tensor, mode=\"mean\") -> torch.Tensor:\n",
    "        src, dst = x.chunk(2, dim=-2)\n",
    "        n, t1, c = src.shape\n",
    "        unm = src.gather(dim=-2, index=unm_idx.expand(n, t1 - r, c))\n",
    "        src = src.gather(dim=-2, index=src_idx.expand(n, r, c))\n",
    "        dst = dst.scatter_reduce(-2, dst_idx.expand(n, r, c), src, reduce=mode)\n",
    "\n",
    "        if distill_token:\n",
    "            return torch.cat([unm[:, :1], dst[:, :1], unm[:, 1:], dst[:, 1:]], dim=1)\n",
    "        else:\n",
    "            return torch.cat([unm, dst], dim=1)\n",
    "\n",
    "    def unmerge(x: torch.Tensor) -> torch.Tensor:\n",
    "        unm_len = unm_idx.shape[1]\n",
    "        unm, dst = x[..., :unm_len, :], x[..., unm_len:, :]\n",
    "        n, _, c = unm.shape\n",
    "\n",
    "        src = dst.gather(dim=-2, index=dst_idx.expand(n, r, c))\n",
    "\n",
    "        out = torch.zeros(n, metric.shape[1], c, device=x.device, dtype=x.dtype)\n",
    "\n",
    "        out[..., dst.shape[-2]:, :] = dst\n",
    "        out.scatter_(dim=-2, index=(unm_idx).expand(n, unm_len, c), src=unm)\n",
    "        out.scatter_(dim=-2, index=(src_idx).expand(n, r, c), src=src)\n",
    "\n",
    "        return out\n",
    "    \n",
    "    return merge, unmerge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "317303a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def concat_mats(args, dim=0):\n",
    "    return torch.cat(args, dim=dim)\n",
    "\n",
    "def unconcat_mat(tensor, dim=0):\n",
    "    return torch.chunk(tensor, chunks=2, dim=dim)\n",
    "\n",
    "def match_tensors_permute(hull_tensor, eps=1e-7, interleave=False, random_perm=False):\n",
    "    \"\"\"\n",
    "    hull_tensor: [2O,I]\n",
    "    \"\"\"\n",
    "    O, I = hull_tensor.shape\n",
    "    O //= 2\n",
    "    \n",
    "    interleave_mat = torch.eye(2*O, device=hull_tensor.device)\n",
    "    if interleave:\n",
    "        A1, A2, B1, B2 = interleave_mat.chunk(4, dim=0)\n",
    "        interleave_mat = torch.cat([A1, B1, A2, B2], dim=0)\n",
    "        interleave_mat = interleave_mat.view(2, O, 2*O).transpose(0, 1).reshape(2*O, 2*O)\n",
    "#         interleave_mat = interleave_mat[torch.randperm(2*O, device=hull_tensor.device)]\n",
    "    \n",
    "    hull_tensor = interleave_mat @ hull_tensor\n",
    "    \n",
    "    hull_tensor = hull_tensor / (hull_tensor.norm(dim=-1, keepdim=True) + eps)\n",
    "    A, B = unconcat_mat(hull_tensor, dim=0)\n",
    "    scores = -(A @ B.T)\n",
    "    \n",
    "    O_eye = torch.eye(O, device=hull_tensor.device)\n",
    "    \n",
    "    try:\n",
    "        row_idx, col_idx = scipy.optimize.linear_sum_assignment(scores.cpu().numpy())\n",
    "    except ValueError:\n",
    "        pdb.set_trace()\n",
    "    \n",
    "    A_perm = O_eye[torch.from_numpy(row_idx)]#[perm]\n",
    "    B_perm = O_eye[torch.from_numpy(col_idx)]#[perm]\n",
    "    \n",
    "    if random_perm:\n",
    "        perm = torch.randperm(O, device=A.device)\n",
    "        A_perm = A_perm[perm]\n",
    "        B_perm = B_perm[perm]\n",
    "    \n",
    "    merge = (torch.cat((A_perm, B_perm), dim=1) / 2.) @ interleave_mat\n",
    "    unmerge = interleave_mat.T @ (torch.cat((A_perm.T, B_perm.T), dim=0))\n",
    "    return merge, unmerge\n",
    "\n",
    "\n",
    "def match_tensors_tome(hull_tensor, eps=1e-7, interleave=False, random_perm=False):\n",
    "    \"\"\"\n",
    "    hull_tensor: [2O,I]\n",
    "    \"\"\"\n",
    "    O, I = hull_tensor.shape\n",
    "    O //= 2\n",
    "    \n",
    "    big_eye = torch.eye(2*O, device=hull_tensor.device)\n",
    "    small_eye = torch.eye(O, device=hull_tensor.device)\n",
    "    \n",
    "    interleave_mat = big_eye\n",
    "    if interleave:\n",
    "        A1, A2, B1, B2 = interleave_mat.chunk(4, dim=0)\n",
    "        interleave_mat = torch.cat([A1, B1, A2, B2], dim=0)\n",
    "    \n",
    "    \n",
    "    hull_tensor = interleave_mat @ hull_tensor\n",
    "    \n",
    "    merge, unmerge = bipartite_soft_matching(hull_tensor[None], 0.5)\n",
    "    \n",
    "    merge_mat = merge(big_eye[None])[0] @ interleave_mat\n",
    "    unmerge_mat = interleave_mat.T @ unmerge(small_eye[None])[0]\n",
    "    return merge_mat, unmerge_mat\n",
    "\n",
    "##########################################################################################\n",
    "##########################################################################################\n",
    "##########################################################################################\n",
    "##########################################################################################\n",
    "##########################################################################################\n",
    "##########################################################################################\n",
    "##########################################################################################\n",
    "##########################################################################################\n",
    "##########################################################################################\n",
    "##########################################################################################\n",
    "\n",
    "def get_procrustes(corr_mtx):\n",
    "    U, _, Vh = torch.linalg.svd(corr_mtx)\n",
    "    S = torch.eye(U.shape[0], device=U.device)\n",
    "    S[-1, -1] = -1.\n",
    "    return (U @ S) @ Vh\n",
    "\n",
    "def match_tensors_procrustes(hull_tensor, eps=1e-7, interleave=False, random_perm=False):\n",
    "    \"\"\"\n",
    "    hull_tensor: [2O,I]\n",
    "    \"\"\"\n",
    "    O, I = hull_tensor.shape\n",
    "    O //= 2\n",
    "    \n",
    "    interleave_mat = torch.eye(2*O, device=hull_tensor.device)\n",
    "    if interleave:\n",
    "        A1, A2, B1, B2 = interleave_mat.chunk(4, dim=0)\n",
    "        interleave_mat = torch.cat([A1, B1, A2, B2], dim=0)\n",
    "        interleave_mat = interleave_mat.view(2, O, 2*O).transpose(0, 1).reshape(2*O, 2*O)\n",
    "#         interleave_mat = interleave_mat[torch.randperm(2*O, device=hull_tensor.device)]\n",
    "    \n",
    "    hull_tensor = interleave_mat @ hull_tensor\n",
    "    \n",
    "    hull_tensor = hull_tensor / (hull_tensor.norm(dim=-1, keepdim=True) + eps)\n",
    "    A, B = unconcat_mat(hull_tensor, dim=0)\n",
    "    scores = (A @ B.T)\n",
    "    \n",
    "    P = get_procrustes(scores).T\n",
    "    \n",
    "    O_eye = torch.eye(O, device=hull_tensor.device)\n",
    "    \n",
    "    try:\n",
    "        row_idx, col_idx = scipy.optimize.linear_sum_assignment(scores.cpu().numpy())\n",
    "    except ValueError:\n",
    "        pdb.set_trace()\n",
    "    \n",
    "    A_perm = O_eye[torch.from_numpy(row_idx)]#[perm]\n",
    "    B_perm = P #O_eye[torch.from_numpy(col_idx)]#[perm]\n",
    "    \n",
    "    if random_perm:\n",
    "        perm = torch.randperm(O, device=A.device)\n",
    "        A_perm = A_perm[perm]\n",
    "        B_perm = B_perm[perm]\n",
    "    \n",
    "    merge = (torch.cat((A_perm, B_perm), dim=1) / 2.) @ interleave_mat\n",
    "    unmerge = interleave_mat.T @ (torch.cat((A_perm.T, B_perm.T), dim=0))\n",
    "    return merge, unmerge\n",
    "\n",
    "\n",
    "\n",
    "# def match_tensors_procrustes(hull_tensor, use_S=True, interleave=False, random_perm=False):\n",
    "#     # We can only reduce by a maximum of 50% tokens\n",
    "#     t = hull_tensor.shape[0]\n",
    "#     r = int(.f5 * t)\n",
    "#     with torch.no_grad():\n",
    "#         A, B = unconcat_mat(hull_tensor, dim=0)\n",
    "#         scores = -(A @ B.T)\n",
    "#         U, S, V = torch.svd(scores)\n",
    "        \n",
    "# #         U[:, :rank] /= S[None, :]\n",
    "#         U_r = U[:, :r]\n",
    "#         U_r[:, rank:] = 0\n",
    "#         S_r = torch.diag(S[:r]) if use_S else torch.eye(r, device=DEVICE)\n",
    "# #         pdb.set_trace()\n",
    "# #         V_r = V[:, :r]\n",
    "#     merge_mat = U_r.T\n",
    "#     unmerge_mat = U_r\n",
    "#     return merge_mat, unmerge_mat\n",
    "\n",
    "# match_tensors = match_tensors\n",
    "\n",
    "def match_wrapper(fn, interleave=False, random_perm=False):\n",
    "    return lambda x: fn(x, interleave=interleave, random_perm=random_perm)\n",
    "##########################################################################################\n",
    "##########################################################################################\n",
    "##########################################################################################\n",
    "##########################################################################################\n",
    "##########################################################################################\n",
    "##########################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "58dfaa09",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerTransform(dict):\n",
    "    def __init__(self, normalize_tensors=False, tensor_merge_type='concat'):\n",
    "        super(LayerTransform, self).__init__()\n",
    "        self.output_align = None\n",
    "        self.next_input_align = None\n",
    "        self.normalize_tensors = normalize_tensors\n",
    "        self.tensor_merge_type = tensor_merge_type\n",
    "    \n",
    "    def compute_transform(self):\n",
    "        inputs = list(self.values())\n",
    "        if self.normalize_tensors:\n",
    "            for idx, inp in enumerate(inputs):\n",
    "                inputs[idx] = F.normalize(inp, dim=-1)\n",
    "        if self.tensor_merge_type == 'concat':\n",
    "            match_input = concat_mats(inputs, dim=-1)\n",
    "        elif self.tensor_merge_type == 'mean':\n",
    "            match_input = torch.stack(inputs, dim=0).mean(0)\n",
    "                \n",
    "        self.output_align, self.next_input_align = match_tensors(match_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "eb045e3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def unflatten(x, k=3):\n",
    "    O, IHW = x.shape\n",
    "    return x.view(O, -1, k, k)\n",
    "\n",
    "def merge_first_convs(state_dict, prefix, a_conv, b_conv, output_transform):\n",
    "    flatten_conv = lambda x: x.flatten(1)\n",
    "    a_w = flatten_conv(a_conv.weight)\n",
    "    b_w = flatten_conv(b_conv.weight)\n",
    "    ab_w = concat_mats((a_w, b_w), dim=0)\n",
    "    output_transform[prefix] = ab_w\n",
    "    output_transform.compute_transform()\n",
    "    # merge_mat, unmerge_mat = match_tensors(ab_w)\n",
    "    c_w = output_transform.output_align @ ab_w\n",
    "    state_dict[prefix + '.weight'] = unflatten(c_w, a_conv.weight.shape[-1])\n",
    "    return output_transform\n",
    "\n",
    "def merge_bn(state_dict, prefix, a_bn, b_bn, output_transform):\n",
    "    staterify = lambda bn: torch.stack((bn.weight, bn.bias, bn.running_mean), dim=1)\n",
    "    unstaterify = lambda stats: stats.unbind(-1)\n",
    "    \n",
    "    a_stats = staterify(a_bn)\n",
    "    b_stats = staterify(b_bn)\n",
    "    ab_stats = concat_mats((a_stats, b_stats), dim=0)\n",
    "    c_stats = output_transform.output_align @ ab_stats\n",
    "    c_weight, c_bias, c_mean = unstaterify(c_stats)\n",
    "    ab_var = concat_mats((a_bn.running_var[..., None], b_bn.running_var[...,None]))\n",
    "    var_out_transform = output_transform.output_align.square()\n",
    "    c_var = (var_out_transform @ ab_var).reshape(-1)\n",
    "    state_dict[prefix + '.weight'] = c_weight\n",
    "    state_dict[prefix + '.bias'] = c_bias\n",
    "    state_dict[prefix + '.running_mean'] = c_mean\n",
    "    state_dict[prefix + '.running_var'] = c_var\n",
    "    pass\n",
    "\n",
    "def block_diagonalize_tensors(tensor1, tensor2):\n",
    "    zerooos = torch.zeros_like(tensor1)\n",
    "    block_diagonal = concat_mats(\n",
    "        (\n",
    "            concat_mats((tensor1, zerooos), dim=1),\n",
    "            concat_mats((zerooos, tensor2), dim=1),\n",
    "        ),\n",
    "        dim=0\n",
    "    )\n",
    "    return block_diagonal\n",
    "\n",
    "def merge_hidden_conv(\n",
    "    state_dict, prefix, a_conv, b_conv, \n",
    "    input_transform, output_transform, \n",
    "    recompute_output=False\n",
    "):\n",
    "    O, I, H, W = a_conv.weight.shape\n",
    "    get_I_by_O_by_HW = lambda x: x.permute(1, 0, 2, 3).flatten(2)\n",
    "    \n",
    "    a_I_by_O_by_HW = get_I_by_O_by_HW(a_conv.weight)\n",
    "    b_I_by_O_by_HW = get_I_by_O_by_HW(b_conv.weight)\n",
    "    \n",
    "    dummy_zerooooo = torch.zeros_like(b_I_by_O_by_HW)\n",
    "    ab_block_diago = concat_mats(\n",
    "        (\n",
    "            concat_mats((a_I_by_O_by_HW, dummy_zerooooo), dim=1),\n",
    "            concat_mats((dummy_zerooooo, b_I_by_O_by_HW), dim=1)\n",
    "        ),\n",
    "        dim=0\n",
    "    )\n",
    "    \n",
    "    # [I,2I]x[2I,2OHW]->[I,2OHW]\n",
    "    ab_input_aligned = input_transform.next_input_align.T @ ab_block_diago.flatten(1)\n",
    "    ab_input_aligned = ab_input_aligned.\\\n",
    "    reshape(I, 2 * O, H*W).\\\n",
    "    transpose(1, 0).\\\n",
    "    flatten(1) # [I,2O,HW]->[2O,I,HW]->[2O,IHW]\n",
    "    output_transform[prefix] = ab_input_aligned\n",
    "    if recompute_output:\n",
    "        output_transform.compute_transform()\n",
    "    c_flat = output_transform.output_align @ ab_input_aligned\n",
    "    state_dict[prefix + '.weight'] = unflatten(c_flat, a_conv.weight.shape[-1])\n",
    "    \n",
    "    output_block_diagonal_ab = block_diagonalize_tensors(\n",
    "        a_conv.weight.flatten(2),\n",
    "        b_conv.weight.flatten(2)\n",
    "    )\n",
    "    ab_output_aligned = output_transform.output_align @ output_block_diagonal_ab.flatten(1)\n",
    "    ab_output_aligned = ab_output_aligned.reshape(O, 2 * I, H*W).transpose(1, 0).flatten(1)\n",
    "    input_transform[prefix] = ab_output_aligned\n",
    "    \n",
    "    return output_transform\n",
    "\n",
    "def merge_linear(\n",
    "    state_dict, prefix, a_linear, \n",
    "    b_linear, input_transform, \n",
    "    output_transform, \n",
    "    recompute_output=False\n",
    "):\n",
    "    class conv_wrapper:\n",
    "        def __init__(self, linear):\n",
    "            self.weight = linear.weight[:, :, None, None]\n",
    "    \n",
    "    output_transform = merge_hidden_conv(\n",
    "        state_dict, prefix, conv_wrapper(a_linear), \n",
    "        conv_wrapper(b_linear), input_transform, \n",
    "        output_transform, recompute_output=recompute_output\n",
    "    )\n",
    "    state_dict[prefix + '.weight'] = state_dict[prefix + '.weight'][..., 0, 0]\n",
    "    state_dict[prefix + '.bias'] = output_transform.output_align @ concat_mats((a_linear.bias, b_linear.bias))\n",
    "    return output_transform\n",
    "    \n",
    "def merge_block(\n",
    "    state_dict, prefix, a_block, b_block, \n",
    "    input_transform, intra_transform,\n",
    "    output_transform=None, shortcut=False\n",
    "):\n",
    "    conv1_transform = merge_hidden_conv(\n",
    "        state_dict, prefix + '.conv1', a_block.conv1, b_block.conv1, \n",
    "        input_transform, intra_transform, recompute_output=True\n",
    "    )\n",
    "    merge_bn(state_dict, prefix + '.bn1', a_block.bn1, b_block.bn1, conv1_transform)\n",
    "    \n",
    "    \n",
    "    conv2_transform = merge_hidden_conv(\n",
    "        state_dict, \n",
    "        prefix + '.conv2', \n",
    "        a_block.conv2, \n",
    "        b_block.conv2, \n",
    "        conv1_transform,\n",
    "        output_transform,\n",
    "        recompute_output=shortcut\n",
    "    )\n",
    "    merge_bn(state_dict, prefix + '.bn2', a_block.bn2, b_block.bn2, conv2_transform)\n",
    "    \n",
    "    if shortcut:\n",
    "        shortcut_transform = merge_hidden_conv(\n",
    "            state_dict, \n",
    "            prefix + '.shortcut.0', \n",
    "            a_block.shortcut[0], \n",
    "            b_block.shortcut[0], \n",
    "            input_transform,\n",
    "            output_transform=conv2_transform\n",
    "        )\n",
    "        merge_bn(\n",
    "            state_dict, \n",
    "            prefix + '.shortcut.1', \n",
    "            a_block.shortcut[1], \n",
    "            b_block.shortcut[1], \n",
    "            shortcut_transform\n",
    "        )\n",
    "    \n",
    "    return conv2_transform\n",
    "\n",
    "hard_pass = lambda : None\n",
    "\n",
    "def merge_resnet20(state_dict, a, b, transforms):\n",
    "    transforms['conv1'] = merge_first_convs(\n",
    "        state_dict, 'conv1', a.conv1, b.conv1, \n",
    "        output_transform=transforms['conv1']\n",
    "    )\n",
    "    merge_bn(state_dict, 'bn1', a.bn1, b.bn1, transforms['conv1'])\n",
    "    \n",
    "    for i in range(3):\n",
    "        merge_block(\n",
    "            state_dict, f'layer1.{i}', a.layer1[i], b.layer1[i], \n",
    "            input_transform=transforms['conv1'], \n",
    "            intra_transform=transforms[f'block1.{i}'],\n",
    "            output_transform=transforms['conv1'],\n",
    "            shortcut=False\n",
    "        )\n",
    "    \n",
    "    transforms['block2'] = merge_block(\n",
    "        state_dict, 'layer2.0', a.layer2[0], b.layer2[0], \n",
    "        input_transform=transforms['conv1'], \n",
    "        intra_transform=transforms[f'block2.0'],\n",
    "        output_transform=transforms['block2'],\n",
    "        shortcut=True\n",
    "    )\n",
    "    \n",
    "    for i in range(1, 3):\n",
    "        merge_block(\n",
    "            state_dict, f'layer2.{i}', a.layer2[i], b.layer2[i], \n",
    "            input_transform=transforms['block2'], \n",
    "            intra_transform=transforms[f'block2.{i}'],\n",
    "            output_transform=transforms['block2'],\n",
    "            shortcut=False\n",
    "        )\n",
    "        \n",
    "    transforms['block3'] = merge_block(\n",
    "        state_dict, 'layer3.0', a.layer3[0], b.layer3[0], \n",
    "        input_transform=transforms['block2'], \n",
    "        intra_transform=transforms[f'block3.0'],\n",
    "        output_transform=transforms['block3'],\n",
    "        shortcut=True\n",
    "    )\n",
    "    for i in range(1, 3):\n",
    "        merge_block(\n",
    "            state_dict, f'layer3.{i}', a.layer3[i], b.layer3[i], \n",
    "            input_transform=transforms['block3'], \n",
    "            intra_transform=transforms[f'block3.{i}'],\n",
    "            output_transform=transforms['block3'],\n",
    "            shortcut=False\n",
    "        )\n",
    "        \n",
    "    output_align_identity = torch.eye(a.linear.weight.shape[0], device=a.linear.weight.device)\n",
    "    output_align_mat = torch.cat((output_align_identity/2, output_align_identity/2), dim=1)\n",
    "    transforms['linear'].output_align = output_align_mat\n",
    "    transforms['linear'] = merge_linear(\n",
    "        state_dict, 'linear', a.linear, b.linear, \n",
    "        transforms['block3'], transforms['linear'],\n",
    "        recompute_output=False\n",
    "    )\n",
    "    \n",
    "    return transforms\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "432c3a2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use the train loader with data augmentation as this gives better results\n",
    "def reset_bn_stats(model, epochs=1, loader=train_aug_loader):\n",
    "    # resetting stats to baseline first as below is necessary for stability\n",
    "    for m in model.modules():\n",
    "        if type(m) == nn.BatchNorm2d:\n",
    "            m.momentum = None # use simple average\n",
    "            m.reset_running_stats()\n",
    "    # run a single train epoch with augmentations to recalc stats\n",
    "    model.train()\n",
    "    for _ in range(epochs):\n",
    "        with torch.no_grad(), autocast():\n",
    "            for images, _ in loader:\n",
    "                output = model(images.to(DEVICE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8fc5c564",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_convergence(old_transforms, current_transforms, eps=1e-5):\n",
    "    if len(old_transforms) == 0: \n",
    "        return False, {}\n",
    "    transform_norms = {}\n",
    "    is_converged = True\n",
    "    for key, old_transform in old_transforms.items():\n",
    "        current_transform = current_transforms[key]\n",
    "        is_close = torch.allclose(\n",
    "            current_transform.output_align, \n",
    "            old_transform.output_align, \n",
    "            atol=eps\n",
    "        )\n",
    "        norm = torch.norm(current_transform.output_align - old_transform.output_align)\n",
    "        if not is_close: \n",
    "            is_converged = False\n",
    "        transform_norms[key] = torch.round(norm, decimals=3).cpu().numpy().round(3)\n",
    "    return (is_converged, transform_norms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "73225ade",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.778\n",
      "0.7758\n"
     ]
    }
   ],
   "source": [
    "model1 = resnet20(w=4, text_head=True).to(DEVICE)\n",
    "model2 = resnet20(w=4, text_head=True).to(DEVICE)\n",
    "load_model(model1, f'resnet20x4_CIFAR50_clses{model1_classes.tolist()}')\n",
    "load_model(model2, f'resnet20x4_CIFAR50_clses{model2_classes.tolist()}')\n",
    "\n",
    "print(evaluate_texthead(model1, test_loader1, class_vecs1, remap_class_idxs=class_idxs))\n",
    "print(evaluate_texthead(model2, test_loader2, class_vecs2, remap_class_idxs=class_idxs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6a989d99",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "7bc67f5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_procrustes(corr_mtx):\n",
    "    U, _, Vh = torch.linalg.svd(corr_mtx)\n",
    "    S = torch.eye(U.shape[0], device=U.device)\n",
    "    S[-1, -1] = -1.\n",
    "    return (U @ S) @ Vh\n",
    "\n",
    "def match_tensors_procrustes(hull_tensor, eps=1e-7, interleave=False, random_perm=False):\n",
    "    \"\"\"\n",
    "    hull_tensor: [2O,I]\n",
    "    \"\"\"\n",
    "    O, I = hull_tensor.shape\n",
    "    O //= 2\n",
    "    \n",
    "    interleave_mat = torch.eye(2*O, device=hull_tensor.device)\n",
    "    if interleave:\n",
    "        A1, A2, B1, B2 = interleave_mat.chunk(4, dim=0)\n",
    "        interleave_mat = torch.cat([A1, B1, A2, B2], dim=0)\n",
    "        interleave_mat = interleave_mat.view(2, O, 2*O).transpose(0, 1).reshape(2*O, 2*O)\n",
    "#         interleave_mat = interleave_mat[torch.randperm(2*O, device=hull_tensor.device)]\n",
    "    \n",
    "    hull_tensor = interleave_mat @ hull_tensor\n",
    "    \n",
    "#     hull_tensor = hull_tensor / (hull_tensor.norm(dim=-1, keepdim=True) + eps)\n",
    "    A, B = unconcat_mat(hull_tensor, dim=0)\n",
    "    scores = (A @ B.T)\n",
    "    \n",
    "    P = get_procrustes(scores)\n",
    "    \n",
    "    O_eye = torch.eye(O, device=hull_tensor.device)\n",
    "    \n",
    "    try:\n",
    "        row_idx, col_idx = scipy.optimize.linear_sum_assignment(scores.cpu().numpy())\n",
    "    except ValueError:\n",
    "        pdb.set_trace()\n",
    "    \n",
    "    A_perm = O_eye[torch.from_numpy(row_idx)]#[perm]\n",
    "    B_perm = P #O_eye[torch.from_numpy(col_idx)]#[perm]\n",
    "    \n",
    "    if random_perm:\n",
    "        perm = torch.randperm(O, device=A.device)\n",
    "        A_perm = A_perm[perm]\n",
    "        B_perm = B_perm[perm]\n",
    "    \n",
    "    merge = (torch.cat((A_perm, B_perm), dim=1) / 2.) @ interleave_mat\n",
    "    unmerge = interleave_mat.T @ (torch.cat((A_perm.T, B_perm.T), dim=0))\n",
    "    return merge, unmerge\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "5ea06937",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------- Step 1 Dist: nan ---------------------\n"
     ]
    }
   ],
   "source": [
    "# Can choose between:\n",
    "# match_tensors_tome: ToMe with or without interleaving\n",
    "# match_tensors_permute: \n",
    "# match_tensors_svd\n",
    "match_tensors = match_wrapper(match_tensors_procrustes, interleave=False, random_perm=False)\n",
    "layer_transform = lambda : LayerTransform(normalize_tensors=False, tensor_merge_type='concat')\n",
    "old_state_dict = {}\n",
    "state_dict = {}\n",
    "old_transforms = defaultdict(lambda: layer_transform())\n",
    "new_transforms = defaultdict(lambda: layer_transform())\n",
    "modelc = resnet20(w=4, text_head=True).to(DEVICE)\n",
    "accuracies = []\n",
    "steps = []\n",
    "distances = []\n",
    "best_info = {'acc': 0., 'dist': np.inf}\n",
    "step = 1\n",
    "is_converged = False\n",
    "while not is_converged:\n",
    "# for step in range(10):\n",
    "    old_transforms = new_transforms\n",
    "    old_state_dict = deepcopy(state_dict)\n",
    "    new_transforms = merge_resnet20(state_dict, model1, model2, transforms=deepcopy(old_transforms))\n",
    "    if step == 0:\n",
    "        original_computation = deepcopy(new_transforms)\n",
    "#     modelc.load_state_dict(state_dict)\n",
    "#     reset_bn_stats(modelc)\n",
    "#     acc, perclass_acc = evaluate_texthead(\n",
    "#         modelc, test_loader, class_vectors=text_features, return_confusion=True\n",
    "#     )\n",
    "#     if acc > best_info['acc']:\n",
    "#         best_info['acc'] = acc\n",
    "#         best_info['perclass_acc'] = perclass_acc\n",
    "#     print(step, acc)\n",
    "    is_converged, transform2dist = check_convergence(old_transforms, new_transforms)\n",
    "    avg_distance = np.mean(list(transform2dist.values()))\n",
    "#     if step % 10 == 0:\n",
    "#         modelc.load_state_dict(state_dict)\n",
    "#         reset_bn_stats(modelc)\n",
    "#         acc, _ = evaluate_texthead(\n",
    "#             modelc, test_loader, class_vectors=text_features, return_confusion=True\n",
    "#         )\n",
    "#         accuracies += [acc]\n",
    "#         steps += [step]\n",
    "#         distances += [np.mean(list(transform2dist.values()))]\n",
    "        \n",
    "    print(f'--------------------- Step {step} Dist: {avg_distance:.3f} ---------------------')\n",
    "    step += 1\n",
    "    if step >= 1:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "a3fd3e11",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0196"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modelc.load_state_dict(state_dict)\n",
    "reset_bn_stats(modelc)\n",
    "acc, perclass_acc = evaluate_texthead(\n",
    "    modelc, test_loader, class_vectors=text_features, return_confusion=True\n",
    ")\n",
    "acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "2da6d12a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.337, 0.3398, 0.3373, 0.3365, 0.3368, 0.3338, 0.3364, 0.3376, 0.3513, 0.3393, 0.3376, 0.3407]\n",
      "[2.441 2.287 1.974 1.876 1.792 1.817 1.819 1.813 1.786 1.756 1.78  1.782]\n"
     ]
    }
   ],
   "source": [
    "print(accuracies)\n",
    "print(np.round(distances, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "8f61184b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[10, 20, 30, 40, 50, 60, 70, 80, 90, 100, 110, 120]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "7fc7ea60",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'acc': 0.3389, 'dist': 1.5622308}"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "2e115a45",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "permuted_sd = pickle.load(open(\n",
    "'/srv/share/jbjorner3/checkpoints/REPAIR/pytorch_cifar50_2_permuted_to_1_jakob.pkl', 'rb'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "8a9144a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_permute = resnet20(w=4, text_head=True)\n",
    "model_permute.load_state_dict(permuted_sd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "aab79bb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3884\n"
     ]
    }
   ],
   "source": [
    "model_permute = model_permute.to(DEVICE)\n",
    "print(evaluate_texthead(model_permute, test_loader, class_vectors=text_features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "fe8ad44b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mix_weights(model, alpha, sd0, sd1):\n",
    "    sd_alpha = {}\n",
    "    for k in sd0.keys():\n",
    "        param0 = sd0[k].to(DEVICE)\n",
    "        param1 = sd1[k].to(DEVICE)\n",
    "        sd_alpha[k] = (1 - alpha) * param0 + alpha * param1\n",
    "    model.load_state_dict(sd_alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "6921abc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0167\n",
      "0.2426\n"
     ]
    }
   ],
   "source": [
    "merged_model = resnet20(w=4, text_head=True).to(DEVICE)\n",
    "mix_weights(merged_model, .5, model1.state_dict(), model_permute.state_dict())\n",
    "\n",
    "print(evaluate_texthead(merged_model, test_loader, class_vectors=text_features))\n",
    "reset_bn_stats(merged_model)\n",
    "print(evaluate_texthead(merged_model, test_loader, class_vectors=text_features))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb25d933",
   "metadata": {},
   "source": [
    "# Activation Matching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "24f0b6a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "conv1a = torch.rand((100, 70))\n",
    "conv1b = torch.rand((100, 70))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "da347514",
   "metadata": {},
   "outputs": [],
   "source": [
    "conv1c = concat_mats((conv1a, conv1b), dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "7b88fa6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "Ua, Sa, Va = torch.svd(conv1a)\n",
    "Ub, Sb, Vb = torch.svd(conv1a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "bfa9ff39",
   "metadata": {},
   "outputs": [],
   "source": [
    "Uc, Sc, Vc = torch.svd(conv1c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "df5bd29a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([200, 70]), torch.Size([100, 70]))"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Uc.shape, Ua.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "1a828f1f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.1065,  0.0463,  0.0338,  ..., -0.1071,  0.0692,  0.0451],\n",
       "        [-0.0980,  0.0454,  0.0915,  ...,  0.1846, -0.2177, -0.0634],\n",
       "        [-0.0930,  0.0463,  0.0032,  ..., -0.0875,  0.0203,  0.0545],\n",
       "        ...,\n",
       "        [-0.0948,  0.0968,  0.0637,  ..., -0.0194, -0.1310,  0.1667],\n",
       "        [-0.0971, -0.0919,  0.2405,  ...,  0.0198,  0.0653,  0.1113],\n",
       "        [-0.1062, -0.2028, -0.1382,  ..., -0.0371,  0.0846, -0.0132]])"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Ua"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "929ce0fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0752, -0.0204, -0.0640,  ..., -0.0438, -0.0201, -0.1500],\n",
       "        [-0.0690, -0.0163,  0.0055,  ..., -0.0764, -0.0216,  0.0813],\n",
       "        [-0.0657, -0.1023, -0.0061,  ...,  0.2184, -0.0514,  0.0800],\n",
       "        ...,\n",
       "        [-0.0695, -0.1297,  0.1890,  ..., -0.0434, -0.1061, -0.0285],\n",
       "        [-0.0715, -0.0557,  0.0258,  ..., -0.0558,  0.0802,  0.0992],\n",
       "        [-0.0685,  0.0492,  0.0203,  ..., -0.0653, -0.0991,  0.0786]])"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Uc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe608bcc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
