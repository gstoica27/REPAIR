{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ac77b920",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/srv/share/gstoica3/miniconda3/envs/open-mmlab/lib/python3.7/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<torch.autograd.grad_mode.set_grad_enabled at 0x7f3c8b7e3290>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import pdb\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.optimize\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from torch.optim import SGD, Adam, lr_scheduler\n",
    "import torchvision\n",
    "import torchvision.transforms as T\n",
    "\n",
    "from sys import platform\n",
    "\n",
    "DEVICE = 'mps' if platform == 'darwin' else 'cuda'\n",
    "if DEVICE == 'mps':\n",
    "    DOWNLOAD_PATH = '/Users/georgestoica/Downloads' \n",
    "else:\n",
    "    DOWNLOAD_PATH = '/srv/share/gstoica3/checkpoints/REPAIR/'\n",
    "    \n",
    "torch.autograd.set_grad_enabled(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "eecce0b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "24850364",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(model, i):\n",
    "    sd = model.state_dict()\n",
    "    path = os.path.join(\n",
    "        DOWNLOAD_PATH,\n",
    "        '%s.pth.tar' % i\n",
    "    )\n",
    "    torch.save(model.state_dict(), path)\n",
    "\n",
    "def load_model(model, i):\n",
    "    path = os.path.join(\n",
    "        DOWNLOAD_PATH,\n",
    "        '%s.pth.tar' % i\n",
    "    )\n",
    "    sd = torch.load(path, map_location=torch.device(DEVICE))\n",
    "    model.load_state_dict(sd)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0cbd8ef6",
   "metadata": {},
   "outputs": [],
   "source": [
    "cifar100_info = {\n",
    "    'dir': '/nethome/gstoica3/research/pytorch-cifar100/data/cifar-100-python',\n",
    "    'classes1': np.arange(50),\n",
    "    'classes2': np.arange(50, 100),\n",
    "    'num_classes': 100,\n",
    "    'split_classes': 50\n",
    "}\n",
    "\n",
    "cifar10_info = {\n",
    "    'dir': '/tmp',\n",
    "    'classes1': np.array([3, 2, 0, 6, 4]),\n",
    "    'classes2': np.array([5, 7, 9, 8, 1]),\n",
    "    'num_classes': 10,\n",
    "    'split_classes': 5\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "eb5981fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_info = cifar10_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f77d8acf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "CIFAR_MEAN = [125.307, 122.961, 113.8575]\n",
    "CIFAR_STD = [51.5865, 50.847, 51.255]\n",
    "normalize = T.Normalize(np.array(CIFAR_MEAN)/255, np.array(CIFAR_STD)/255)\n",
    "denormalize = T.Normalize(-np.array(CIFAR_MEAN)/np.array(CIFAR_STD), 255/np.array(CIFAR_STD))\n",
    "\n",
    "train_transform = T.Compose([\n",
    "    T.RandomHorizontalFlip(),\n",
    "    T.RandomCrop(32, padding=4),\n",
    "    T.ToTensor(),\n",
    "    normalize,\n",
    "])\n",
    "test_transform = T.Compose([\n",
    "    T.ToTensor(),\n",
    "    normalize,\n",
    "])\n",
    "train_dset = torchvision.datasets.CIFAR10(\n",
    "    root=ds_info['dir'], \n",
    "    train=True,\n",
    "    download=True, transform=train_transform\n",
    ")\n",
    "test_dset = torchvision.datasets.CIFAR10(\n",
    "    root=ds_info['dir'],\n",
    "    train=False,\n",
    "    download=True, \n",
    "    transform=test_transform\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2714eefc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "50000it [00:13, 3792.19it/s]\n",
      "50000it [00:13, 3798.84it/s]\n",
      "10000it [00:01, 5619.34it/s]\n",
      "10000it [00:01, 5639.44it/s]\n"
     ]
    }
   ],
   "source": [
    "train_aug_loader = torch.utils.data.DataLoader(train_dset, batch_size=500, shuffle=True, num_workers=8)\n",
    "test_loader = torch.utils.data.DataLoader(test_dset, batch_size=500, shuffle=False, num_workers=8)\n",
    "\n",
    "model1_classes= ds_info['classes1']\n",
    "model2_classes = ds_info['classes2']\n",
    "\n",
    "valid_examples1 = [i for i, (_, label) in tqdm(enumerate(train_dset)) if label in model1_classes]\n",
    "valid_examples2 = [i for i, (_, label) in tqdm(enumerate(train_dset)) if label in model2_classes]\n",
    "\n",
    "assert len(set(valid_examples1).intersection(set(valid_examples2))) == 0, 'sets should be disjoint'\n",
    "\n",
    "train_aug_loader1 = torch.utils.data.DataLoader(\n",
    "    torch.utils.data.Subset(train_dset, valid_examples1), batch_size=500, shuffle=True, num_workers=8\n",
    ")\n",
    "train_aug_loader2 = torch.utils.data.DataLoader(\n",
    "    torch.utils.data.Subset(train_dset, valid_examples2), batch_size=500, shuffle=True, num_workers=8\n",
    ")\n",
    "\n",
    "test_valid_examples1 = [i for i, (_, label) in tqdm(enumerate(test_dset)) if label in model1_classes]\n",
    "test_valid_examples2 = [i for i, (_, label) in tqdm(enumerate(test_dset)) if label in model2_classes]\n",
    "\n",
    "test_loader1 = torch.utils.data.DataLoader(\n",
    "    torch.utils.data.Subset(test_dset, test_valid_examples1), batch_size=500, shuffle=False, num_workers=8\n",
    ")\n",
    "test_loader2 = torch.utils.data.DataLoader(\n",
    "    torch.utils.data.Subset(test_dset, test_valid_examples2), batch_size=500, shuffle=False, num_workers=8\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "02430130",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2, 4, 1, 0, 4, 0, 3, 1, 3, 2])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_idxs = np.zeros(ds_info['num_classes'], dtype=int)\n",
    "class_idxs[model1_classes] = np.arange(ds_info['classes1'].shape[0])\n",
    "class_idxs[model2_classes] = np.arange(ds_info['classes2'].shape[0])\n",
    "class_idxs = torch.from_numpy(class_idxs)\n",
    "class_idxs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bc9e159c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n"
     ]
    }
   ],
   "source": [
    "print(test_dset.classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c95a16d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import clip\n",
    "\n",
    "text_inputs = torch.cat([clip.tokenize(f\"a photo of a {c}\") for c in test_dset.classes]).to(DEVICE)\n",
    "model, preprocess = clip.load('ViT-B/32', DEVICE)\n",
    "with torch.no_grad():\n",
    "    text_features = model.encode_text(text_inputs)\n",
    "\n",
    "\n",
    "text_features /= text_features.norm(dim=-1, keepdim=True)\n",
    "class_vecs1 = text_features[model1_classes]\n",
    "class_vecs2 = text_features[model2_classes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "38c3965a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluates accuracy\n",
    "def evaluate_texthead(model, loader, class_vectors, remap_class_idxs=None, return_confusion=False):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    confusion = np.zeros((100, 100))\n",
    "    \n",
    "    totals = [0] * class_vectors.shape[0]\n",
    "    corrects = [0] * class_vectors.shape[0]\n",
    "    \n",
    "    with torch.no_grad(), autocast():\n",
    "        for inputs, labels in loader:\n",
    "            encodings = model(inputs.to(DEVICE))\n",
    "            normed_encodings = encodings / encodings.norm(dim=-1, keepdim=True)\n",
    "            outputs = normed_encodings @ class_vectors.T\n",
    "            pred = outputs.argmax(dim=1)\n",
    "            if remap_class_idxs is not None:\n",
    "                correct += (remap_class_idxs[labels].to(DEVICE) == pred).sum().item()\n",
    "            else:\n",
    "                for gt, p in zip(labels, pred):\n",
    "                    totals[gt] += 1\n",
    "                    \n",
    "                    if gt == p:\n",
    "                        correct += 1\n",
    "                        corrects[gt] += 1\n",
    "                \n",
    "#                 correct += (labels.to(DEVICE) == pred).sum().item()\n",
    "                \n",
    "            confusion[labels.cpu().numpy(), pred.cpu().numpy()] += 1\n",
    "            total += inputs.shape[0]\n",
    "    if return_confusion:\n",
    "        return correct / sum(totals), list(map(lambda a: a[0] / a[1], zip(corrects, totals)))\n",
    "    else:\n",
    "        return correct / total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e3e7a9d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.nn.init as init\n",
    "\n",
    "def _weights_init(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if isinstance(m, nn.Linear) or isinstance(m, nn.Conv2d):\n",
    "        init.kaiming_normal_(m.weight)\n",
    "\n",
    "class LambdaLayer(nn.Module):\n",
    "    def __init__(self, lambd):\n",
    "        super(LambdaLayer, self).__init__()\n",
    "        self.lambd = lambd\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.lambd(x)\n",
    "\n",
    "\n",
    "class BasicBlock(nn.Module):\n",
    "    expansion = 1\n",
    "\n",
    "    def __init__(self, in_planes, planes, stride=1):\n",
    "        super(BasicBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_planes != planes:\n",
    "#             self.shortcut = LambdaLayer(lambda x:\n",
    "#                                         F.pad(x[:, :, ::2, ::2], (0, 0, 0, 0, planes//4, planes//4), \"constant\", 0))\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=False),\n",
    "                nn.BatchNorm2d(planes)\n",
    "            )\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.bn2(self.conv2(out))\n",
    "        out += self.shortcut(x)\n",
    "        out = F.relu(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "class ResNet(nn.Module):\n",
    "    def __init__(self, block, num_blocks, w=1, num_classes=10, text_head=False):\n",
    "        super(ResNet, self).__init__()\n",
    "        self.in_planes = int(w*16)\n",
    "\n",
    "        self.conv1 = nn.Conv2d(3, int(w*16), kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(int(w*16))\n",
    "        self.layer1 = self._make_layer(block, int(w*16), num_blocks[0], stride=1)\n",
    "        self.layer2 = self._make_layer(block, int(w*32), num_blocks[1], stride=2)\n",
    "        self.layer3 = self._make_layer(block, int(w*64), num_blocks[2], stride=2)\n",
    "        if text_head:\n",
    "            num_classes = 512\n",
    "        self.linear = nn.Linear(int(w*64), num_classes)\n",
    "\n",
    "        self.apply(_weights_init)\n",
    "\n",
    "    def _make_layer(self, block, planes, num_blocks, stride):\n",
    "        strides = [stride] + [1]*(num_blocks-1)\n",
    "        layers = []\n",
    "        for stride in strides:\n",
    "            layers.append(block(self.in_planes, planes, stride))\n",
    "            self.in_planes = planes * block.expansion\n",
    "\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.layer1(out)\n",
    "        out = self.layer2(out)\n",
    "        out = self.layer3(out)\n",
    "        out = F.avg_pool2d(out, out.size()[3])\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.linear(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "def resnet20(w=1, text_head=False):\n",
    "    return ResNet(BasicBlock, [3, 3, 3], w=w, text_head=text_head)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5fa7f25a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bipartite_soft_matching(\n",
    "    metric: torch.Tensor,\n",
    "    r: float,\n",
    "    class_token: bool = False,\n",
    "    distill_token: bool = False,\n",
    "):\n",
    "    \"\"\"\n",
    "    Applies ToMe with a balanced matching set (50%, 50%).\n",
    "    Input size is [batch, tokens, channels].\n",
    "    r indicates the ratio of tokens to remove (max 50% of tokens).\n",
    "    Extra args:\n",
    "     - class_token: Whether or not there's a class token.\n",
    "     - distill_token: Whether or not there's also a distillation token.\n",
    "    When enabled, the class token and distillation tokens won't get merged.\n",
    "    \"\"\"\n",
    "    protected = 0\n",
    "    if class_token:\n",
    "        protected += 1\n",
    "    if distill_token:\n",
    "        protected += 1\n",
    "\n",
    "    # We can only reduce by a maximum of 50% tokens\n",
    "    t = metric.shape[1]\n",
    "    r = int(r * t)\n",
    "    r = min(r, (t - protected) // 2)\n",
    "\n",
    "    if r <= 0:\n",
    "        return do_nothing, do_nothing\n",
    "\n",
    "    with torch.no_grad():\n",
    "        metric = metric / metric.norm(dim=-1, keepdim=True)\n",
    "        a, b = metric.chunk(2, dim=-2)\n",
    "        scores = a @ b.transpose(-1, -2)\n",
    "\n",
    "        if class_token:\n",
    "            scores[..., 0, :] = -math.inf\n",
    "        if distill_token:\n",
    "            scores[..., :, 0] = -math.inf\n",
    "\n",
    "        node_max, node_idx = scores.max(dim=-1)\n",
    "        edge_idx = node_max.argsort(dim=-1, descending=True)[..., None]\n",
    "\n",
    "        unm_idx = edge_idx[..., r:, :]  # Unmerged Tokens\n",
    "        src_idx = edge_idx[..., :r, :]  # Merged Tokens\n",
    "        dst_idx = node_idx[..., None].gather(dim=-2, index=src_idx)\n",
    "\n",
    "        if class_token:\n",
    "            # Sort to ensure the class token is at the start\n",
    "            unm_idx = unm_idx.sort(dim=1)[0]\n",
    "\n",
    "    def merge(x: torch.Tensor, mode=\"mean\") -> torch.Tensor:\n",
    "        src, dst = x.chunk(2, dim=-2)\n",
    "        n, t1, c = src.shape\n",
    "        unm = src.gather(dim=-2, index=unm_idx.expand(n, t1 - r, c))\n",
    "        src = src.gather(dim=-2, index=src_idx.expand(n, r, c))\n",
    "        dst = dst.scatter_reduce(-2, dst_idx.expand(n, r, c), src, reduce=mode)\n",
    "\n",
    "        if distill_token:\n",
    "            return torch.cat([unm[:, :1], dst[:, :1], unm[:, 1:], dst[:, 1:]], dim=1)\n",
    "        else:\n",
    "            return torch.cat([unm, dst], dim=1)\n",
    "\n",
    "    def unmerge(x: torch.Tensor) -> torch.Tensor:\n",
    "        unm_len = unm_idx.shape[1]\n",
    "        unm, dst = x[..., :unm_len, :], x[..., unm_len:, :]\n",
    "        n, _, c = unm.shape\n",
    "\n",
    "        src = dst.gather(dim=-2, index=dst_idx.expand(n, r, c))\n",
    "\n",
    "        out = torch.zeros(n, metric.shape[1], c, device=x.device, dtype=x.dtype)\n",
    "\n",
    "        out[..., dst.shape[-2]:, :] = dst\n",
    "        out.scatter_(dim=-2, index=(unm_idx).expand(n, unm_len, c), src=unm)\n",
    "        out.scatter_(dim=-2, index=(src_idx).expand(n, r, c), src=src)\n",
    "\n",
    "        return out\n",
    "    \n",
    "    return merge, unmerge\n",
    "\n",
    "def general_soft_matching(\n",
    "    hull_tensor,\n",
    "    interleave=False,\n",
    "    random=False,\n",
    "    r=.5\n",
    "):  \n",
    "    hull_tensor = hull_tensor[0]\n",
    "    hull_normed = hull_tensor / hull_tensor.norm(dim=-1, keepdim=True)\n",
    "    \n",
    "    bound = int(hull_tensor.shape[0] * (1-r))\n",
    "    \n",
    "    sims = hull_normed @ hull_normed.transpose(-1, -2)\n",
    "    uppertri_indices = torch.triu_indices(sims.shape[-2], sims.shape[-1], offset=0)\n",
    "    sims[uppertri_indices[0], uppertri_indices[1]] = -torch.inf\n",
    "    candidate_scores, candidate_indices = sims.max(-1)\n",
    "    argsorted_scores = candidate_scores.argsort(descending=True)\n",
    "    merge_indices = argsorted_scores[:bound]\n",
    "    unmerge_indices = argsorted_scores[bound:]\n",
    "    \n",
    "    roots = torch.arange(sims.shape[0], device=sims.device)\n",
    "    for _ in range(bound-1):\n",
    "        roots[merge_indices] = roots[candidate_indices[merge_indices]]\n",
    "    \n",
    "    def merge(x, mode='mean'):\n",
    "        x = x[0]\n",
    "        merge_tensor = x.scatter_reduce(\n",
    "            0, \n",
    "            roots[merge_indices][:, None].expand(bound, x.shape[1]),\n",
    "            x[merge_indices], \n",
    "            reduce='mean'\n",
    "        )\n",
    "        unmerge_tensor = merge_tensor[unmerge_indices]\n",
    "        return unmerge_tensor[None]\n",
    "    \n",
    "    def unmerge(x):\n",
    "        x = x[0]\n",
    "        out = torch.zeros((hull_tensor.shape[0], x.shape[1]), device=x.device)\n",
    "        out.scatter_(\n",
    "            0,\n",
    "            index=unmerge_indices[:, None].expand(*x.shape),\n",
    "            src=x\n",
    "        )\n",
    "        out = out.scatter(\n",
    "            0,\n",
    "            index=merge_indices[:, None].expand(*x.shape),\n",
    "            src=out[roots[merge_indices]]\n",
    "        )\n",
    "        return out[None]\n",
    "    \n",
    "    return merge, unmerge\n",
    "\n",
    "def unravel_index(index, shape):\n",
    "    out = []\n",
    "    for dim in reversed(shape):\n",
    "        out.append(index % dim)\n",
    "        index = index // dim\n",
    "    return tuple(reversed(out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c20c11a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0b40456",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40112888",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "317303a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def concat_mats(args, dim=0):\n",
    "    return torch.cat(args, dim=dim)\n",
    "\n",
    "def unconcat_mat(tensor, dim=0):\n",
    "    return torch.chunk(tensor, chunks=2, dim=dim)\n",
    "\n",
    "def match_tensors_permute(hull_tensor, eps=1e-7, interleave=False, random_perm=False):\n",
    "    \"\"\"\n",
    "    hull_tensor: [2O,I]\n",
    "    \"\"\"\n",
    "    O, I = hull_tensor.shape\n",
    "    O //= 2\n",
    "    \n",
    "    interleave_mat = torch.eye(2*O, device=hull_tensor.device)\n",
    "    if interleave:\n",
    "        A1, A2, B1, B2 = interleave_mat.chunk(4, dim=0)\n",
    "        interleave_mat = torch.cat([A1, B1, A2, B2], dim=0)\n",
    "        interleave_mat = interleave_mat.view(2, O, 2*O).transpose(0, 1).reshape(2*O, 2*O)\n",
    "#         interleave_mat = interleave_mat[torch.randperm(2*O, device=hull_tensor.device)]\n",
    "    \n",
    "    hull_tensor = interleave_mat @ hull_tensor\n",
    "    \n",
    "    hull_tensor = hull_tensor / (hull_tensor.norm(dim=-1, keepdim=True) + eps)\n",
    "    A, B = unconcat_mat(hull_tensor, dim=0)\n",
    "    scores = -(A @ B.T)\n",
    "    \n",
    "    O_eye = torch.eye(O, device=hull_tensor.device)\n",
    "    \n",
    "    try:\n",
    "        row_idx, col_idx = scipy.optimize.linear_sum_assignment(scores.cpu().numpy())\n",
    "    except ValueError:\n",
    "        pdb.set_trace()\n",
    "    \n",
    "    A_perm = O_eye[torch.from_numpy(row_idx)]#[perm]\n",
    "    B_perm = O_eye[torch.from_numpy(col_idx)]#[perm]\n",
    "    \n",
    "    if random_perm:\n",
    "        perm = torch.randperm(O, device=A.device)\n",
    "        A_perm = A_perm[perm]\n",
    "        B_perm = B_perm[perm]\n",
    "    \n",
    "    merge = (torch.cat((A_perm, B_perm), dim=1) / 2.) @ interleave_mat\n",
    "    unmerge = interleave_mat.T @ (torch.cat((A_perm.T, B_perm.T), dim=0))\n",
    "    return merge, unmerge\n",
    "\n",
    "\n",
    "def match_tensors_tome(hull_tensor, eps=1e-7, interleave=False, random_perm=False):\n",
    "    \"\"\"\n",
    "    hull_tensor: [2O,I]\n",
    "    \"\"\"\n",
    "    O, I = hull_tensor.shape\n",
    "    O //= 2\n",
    "    \n",
    "    big_eye = torch.eye(2*O, device=hull_tensor.device)\n",
    "    small_eye = torch.eye(O, device=hull_tensor.device)\n",
    "    \n",
    "    interleave_mat = big_eye\n",
    "    if interleave:\n",
    "        A1, A2, B1, B2 = interleave_mat.chunk(4, dim=0)\n",
    "        interleave_mat = torch.cat([A1, B1, A2, B2], dim=0)\n",
    "    \n",
    "    \n",
    "    hull_tensor = interleave_mat @ hull_tensor\n",
    "    \n",
    "    merge, unmerge = bipartite_soft_matching(hull_tensor[None], 0.5)\n",
    "    \n",
    "    merge_mat = merge(big_eye[None])[0] @ interleave_mat\n",
    "    unmerge_mat = interleave_mat.T @ unmerge(small_eye[None])[0]\n",
    "    return merge_mat, unmerge_mat\n",
    "\n",
    "##########################################################################################\n",
    "##########################################################################################\n",
    "##########################################################################################\n",
    "##########################################################################################\n",
    "##########################################################################################\n",
    "##########################################################################################\n",
    "##########################################################################################\n",
    "##########################################################################################\n",
    "##########################################################################################\n",
    "##########################################################################################\n",
    "\n",
    "def get_procrustes(corr_mtx):\n",
    "    U, _, Vh = torch.linalg.svd(corr_mtx)\n",
    "    S = torch.eye(U.shape[0], device=U.device)\n",
    "    S[-1, -1] = -1.\n",
    "    return (U @ S) @ Vh\n",
    "\n",
    "def match_tensors_procrustes(hull_tensor, eps=1e-7, interleave=False, random_perm=False):\n",
    "    \"\"\"\n",
    "    hull_tensor: [2O,I]\n",
    "    \"\"\"\n",
    "    O, I = hull_tensor.shape\n",
    "    O //= 2\n",
    "    \n",
    "    interleave_mat = torch.eye(2*O, device=hull_tensor.device)\n",
    "    if interleave:\n",
    "        A1, A2, B1, B2 = interleave_mat.chunk(4, dim=0)\n",
    "        interleave_mat = torch.cat([A1, B1, A2, B2], dim=0)\n",
    "        interleave_mat = interleave_mat.view(2, O, 2*O).transpose(0, 1).reshape(2*O, 2*O)\n",
    "#         interleave_mat = interleave_mat[torch.randperm(2*O, device=hull_tensor.device)]\n",
    "    \n",
    "    hull_tensor = interleave_mat @ hull_tensor\n",
    "    \n",
    "    hull_tensor = hull_tensor / (hull_tensor.norm(dim=-1, keepdim=True) + eps)\n",
    "    A, B = unconcat_mat(hull_tensor, dim=0)\n",
    "    scores = (A @ B.T)\n",
    "    \n",
    "    P = get_procrustes(scores).T\n",
    "    \n",
    "    O_eye = torch.eye(O, device=hull_tensor.device)\n",
    "    \n",
    "    try:\n",
    "        row_idx, col_idx = scipy.optimize.linear_sum_assignment(scores.cpu().numpy())\n",
    "    except ValueError:\n",
    "        pdb.set_trace()\n",
    "    \n",
    "    A_perm = O_eye[torch.from_numpy(row_idx)]#[perm]\n",
    "    B_perm = P #O_eye[torch.from_numpy(col_idx)]#[perm]\n",
    "    \n",
    "    if random_perm:\n",
    "        perm = torch.randperm(O, device=A.device)\n",
    "        A_perm = A_perm[perm]\n",
    "        B_perm = B_perm[perm]\n",
    "    \n",
    "    merge = (torch.cat((A_perm, B_perm), dim=1) / 2.) @ interleave_mat\n",
    "    unmerge = interleave_mat.T @ (torch.cat((A_perm.T, B_perm.T), dim=0))\n",
    "    return merge, unmerge\n",
    "\n",
    "\n",
    "\n",
    "# def match_tensors_procrustes(hull_tensor, use_S=True, interleave=False, random_perm=False):\n",
    "#     # We can only reduce by a maximum of 50% tokens\n",
    "#     t = hull_tensor.shape[0]\n",
    "#     r = int(.f5 * t)\n",
    "#     with torch.no_grad():\n",
    "#         A, B = unconcat_mat(hull_tensor, dim=0)\n",
    "#         scores = -(A @ B.T)\n",
    "#         U, S, V = torch.svd(scores)\n",
    "        \n",
    "# #         U[:, :rank] /= S[None, :]\n",
    "#         U_r = U[:, :r]\n",
    "#         U_r[:, rank:] = 0\n",
    "#         S_r = torch.diag(S[:r]) if use_S else torch.eye(r, device=DEVICE)\n",
    "# #         pdb.set_trace()\n",
    "# #         V_r = V[:, :r]\n",
    "#     merge_mat = U_r.T\n",
    "#     unmerge_mat = U_r\n",
    "#     return merge_mat, unmerge_mat\n",
    "\n",
    "# match_tensors = match_tensors\n",
    "\n",
    "def match_wrapper(fn, interleave=False, random_perm=False):\n",
    "    return lambda x: fn(x, interleave=interleave, random_perm=random_perm)\n",
    "##########################################################################################\n",
    "##########################################################################################\n",
    "##########################################################################################\n",
    "##########################################################################################\n",
    "##########################################################################################\n",
    "##########################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "58dfaa09",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerTransform(dict):\n",
    "    def __init__(self, normalize_tensors=False, tensor_merge_type='concat'):\n",
    "        super(LayerTransform, self).__init__()\n",
    "        self.output_align = None\n",
    "        self.next_input_align = None\n",
    "        self.normalize_tensors = normalize_tensors\n",
    "        self.tensor_merge_type = tensor_merge_type\n",
    "    \n",
    "    def compute_transform(self):\n",
    "        inputs = list(self.values())\n",
    "        if self.normalize_tensors:\n",
    "            for idx, inp in enumerate(inputs):\n",
    "                inputs[idx] = F.normalize(inp, dim=-1)\n",
    "        if self.tensor_merge_type == 'concat':\n",
    "            match_input = concat_mats(inputs, dim=-1)\n",
    "        elif self.tensor_merge_type == 'mean':\n",
    "            match_input = torch.stack(inputs, dim=0).mean(0)\n",
    "                \n",
    "        self.output_align, self.next_input_align = match_tensors(match_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "eb045e3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def unflatten(x, k=3):\n",
    "    O, IHW = x.shape\n",
    "    return x.view(O, -1, k, k)\n",
    "\n",
    "def merge_first_convs(state_dict, prefix, a_conv, b_conv, output_transform):\n",
    "    flatten_conv = lambda x: x.flatten(1)\n",
    "    a_w = flatten_conv(a_conv.weight)\n",
    "    b_w = flatten_conv(b_conv.weight)\n",
    "    ab_w = concat_mats((a_w, b_w), dim=0)\n",
    "    output_transform[prefix] = ab_w\n",
    "    output_transform.compute_transform()\n",
    "    # merge_mat, unmerge_mat = match_tensors(ab_w)\n",
    "    c_w = output_transform.output_align @ ab_w\n",
    "    state_dict[prefix + '.weight'] = unflatten(c_w, a_conv.weight.shape[-1])\n",
    "    return output_transform\n",
    "\n",
    "def merge_bn(state_dict, prefix, a_bn, b_bn, output_transform):\n",
    "    staterify = lambda bn: torch.stack((bn.weight, bn.bias, bn.running_mean), dim=1)\n",
    "    unstaterify = lambda stats: stats.unbind(-1)\n",
    "    \n",
    "    a_stats = staterify(a_bn)\n",
    "    b_stats = staterify(b_bn)\n",
    "    ab_stats = concat_mats((a_stats, b_stats), dim=0)\n",
    "    c_stats = output_transform.output_align @ ab_stats\n",
    "    c_weight, c_bias, c_mean = unstaterify(c_stats)\n",
    "    ab_var = concat_mats((a_bn.running_var[..., None], b_bn.running_var[...,None]))\n",
    "    var_out_transform = output_transform.output_align.square()\n",
    "    c_var = (var_out_transform @ ab_var).reshape(-1)\n",
    "    state_dict[prefix + '.weight'] = c_weight\n",
    "    state_dict[prefix + '.bias'] = c_bias\n",
    "    state_dict[prefix + '.running_mean'] = c_mean\n",
    "    state_dict[prefix + '.running_var'] = c_var\n",
    "    pass\n",
    "\n",
    "def block_diagonalize_tensors(tensor1, tensor2):\n",
    "    zerooos = torch.zeros_like(tensor1)\n",
    "    block_diagonal = concat_mats(\n",
    "        (\n",
    "            concat_mats((tensor1, zerooos), dim=1),\n",
    "            concat_mats((zerooos, tensor2), dim=1),\n",
    "        ),\n",
    "        dim=0\n",
    "    )\n",
    "    return block_diagonal\n",
    "\n",
    "def merge_hidden_conv(\n",
    "    state_dict, prefix, a_conv, b_conv, \n",
    "    input_transform, output_transform, \n",
    "    recompute_output=False\n",
    "):\n",
    "    O, I, H, W = a_conv.weight.shape\n",
    "    get_I_by_O_by_HW = lambda x: x.permute(1, 0, 2, 3).flatten(2)\n",
    "    \n",
    "    a_I_by_O_by_HW = get_I_by_O_by_HW(a_conv.weight)\n",
    "    b_I_by_O_by_HW = get_I_by_O_by_HW(b_conv.weight)\n",
    "    \n",
    "    dummy_zerooooo = torch.zeros_like(b_I_by_O_by_HW)\n",
    "    ab_block_diago = concat_mats(\n",
    "        (\n",
    "            concat_mats((a_I_by_O_by_HW, dummy_zerooooo), dim=1),\n",
    "            concat_mats((dummy_zerooooo, b_I_by_O_by_HW), dim=1)\n",
    "        ),\n",
    "        dim=0\n",
    "    )\n",
    "    \n",
    "    # [I,2I]x[2I,2OHW]->[I,2OHW]\n",
    "    ab_input_aligned = input_transform.next_input_align.T @ ab_block_diago.flatten(1)\n",
    "    ab_input_aligned = ab_input_aligned.\\\n",
    "    reshape(I, 2 * O, H*W).\\\n",
    "    transpose(1, 0).\\\n",
    "    flatten(1) # [I,2O,HW]->[2O,I,HW]->[2O,IHW]\n",
    "    output_transform[prefix] = ab_input_aligned\n",
    "    if recompute_output:\n",
    "        output_transform.compute_transform()\n",
    "    c_flat = output_transform.output_align @ ab_input_aligned\n",
    "    state_dict[prefix + '.weight'] = unflatten(c_flat, a_conv.weight.shape[-1])\n",
    "    \n",
    "    output_block_diagonal_ab = block_diagonalize_tensors(\n",
    "        a_conv.weight.flatten(2),\n",
    "        b_conv.weight.flatten(2)\n",
    "    )\n",
    "    ab_output_aligned = output_transform.output_align @ output_block_diagonal_ab.flatten(1)\n",
    "    ab_output_aligned = ab_output_aligned.reshape(O, 2 * I, H*W).transpose(1, 0).flatten(1)\n",
    "    input_transform[prefix] = ab_output_aligned\n",
    "    \n",
    "    return output_transform\n",
    "\n",
    "def merge_linear(\n",
    "    state_dict, prefix, a_linear, \n",
    "    b_linear, input_transform, \n",
    "    output_transform, \n",
    "    recompute_output=False\n",
    "):\n",
    "    class conv_wrapper:\n",
    "        def __init__(self, linear):\n",
    "            self.weight = linear.weight[:, :, None, None]\n",
    "    \n",
    "    output_transform = merge_hidden_conv(\n",
    "        state_dict, prefix, conv_wrapper(a_linear), \n",
    "        conv_wrapper(b_linear), input_transform, \n",
    "        output_transform, recompute_output=recompute_output\n",
    "    )\n",
    "    state_dict[prefix + '.weight'] = state_dict[prefix + '.weight'][..., 0, 0]\n",
    "    state_dict[prefix + '.bias'] = output_transform.output_align @ concat_mats((a_linear.bias, b_linear.bias))\n",
    "    return output_transform\n",
    "    \n",
    "def merge_block(\n",
    "    state_dict, prefix, a_block, b_block, \n",
    "    input_transform, intra_transform,\n",
    "    output_transform=None, shortcut=False\n",
    "):\n",
    "    conv1_transform = merge_hidden_conv(\n",
    "        state_dict, prefix + '.conv1', a_block.conv1, b_block.conv1, \n",
    "        input_transform, intra_transform, recompute_output=True\n",
    "    )\n",
    "    merge_bn(state_dict, prefix + '.bn1', a_block.bn1, b_block.bn1, conv1_transform)\n",
    "    \n",
    "    \n",
    "    conv2_transform = merge_hidden_conv(\n",
    "        state_dict, \n",
    "        prefix + '.conv2', \n",
    "        a_block.conv2, \n",
    "        b_block.conv2, \n",
    "        conv1_transform,\n",
    "        output_transform,\n",
    "        recompute_output=shortcut\n",
    "    )\n",
    "    merge_bn(state_dict, prefix + '.bn2', a_block.bn2, b_block.bn2, conv2_transform)\n",
    "    \n",
    "    if shortcut:\n",
    "        shortcut_transform = merge_hidden_conv(\n",
    "            state_dict, \n",
    "            prefix + '.shortcut.0', \n",
    "            a_block.shortcut[0], \n",
    "            b_block.shortcut[0], \n",
    "            input_transform,\n",
    "            output_transform=conv2_transform\n",
    "        )\n",
    "        merge_bn(\n",
    "            state_dict, \n",
    "            prefix + '.shortcut.1', \n",
    "            a_block.shortcut[1], \n",
    "            b_block.shortcut[1], \n",
    "            shortcut_transform\n",
    "        )\n",
    "    \n",
    "    return conv2_transform\n",
    "\n",
    "hard_pass = lambda : None\n",
    "\n",
    "def merge_resnet20(state_dict, a, b, transforms):\n",
    "    transforms['conv1'] = merge_first_convs(\n",
    "        state_dict, 'conv1', a.conv1, b.conv1, \n",
    "        output_transform=transforms['conv1']\n",
    "    )\n",
    "    merge_bn(state_dict, 'bn1', a.bn1, b.bn1, transforms['conv1'])\n",
    "    \n",
    "    for i in range(3):\n",
    "        merge_block(\n",
    "            state_dict, f'layer1.{i}', a.layer1[i], b.layer1[i], \n",
    "            input_transform=transforms['conv1'], \n",
    "            intra_transform=transforms[f'block1.{i}'],\n",
    "            output_transform=transforms['conv1'],\n",
    "            shortcut=False\n",
    "        )\n",
    "    \n",
    "    transforms['block2'] = merge_block(\n",
    "        state_dict, 'layer2.0', a.layer2[0], b.layer2[0], \n",
    "        input_transform=transforms['conv1'], \n",
    "        intra_transform=transforms[f'block2.0'],\n",
    "        output_transform=transforms['block2'],\n",
    "        shortcut=True\n",
    "    )\n",
    "    \n",
    "    for i in range(1, 3):\n",
    "        merge_block(\n",
    "            state_dict, f'layer2.{i}', a.layer2[i], b.layer2[i], \n",
    "            input_transform=transforms['block2'], \n",
    "            intra_transform=transforms[f'block2.{i}'],\n",
    "            output_transform=transforms['block2'],\n",
    "            shortcut=False\n",
    "        )\n",
    "        \n",
    "    transforms['block3'] = merge_block(\n",
    "        state_dict, 'layer3.0', a.layer3[0], b.layer3[0], \n",
    "        input_transform=transforms['block2'], \n",
    "        intra_transform=transforms[f'block3.0'],\n",
    "        output_transform=transforms['block3'],\n",
    "        shortcut=True\n",
    "    )\n",
    "    for i in range(1, 3):\n",
    "        merge_block(\n",
    "            state_dict, f'layer3.{i}', a.layer3[i], b.layer3[i], \n",
    "            input_transform=transforms['block3'], \n",
    "            intra_transform=transforms[f'block3.{i}'],\n",
    "            output_transform=transforms['block3'],\n",
    "            shortcut=False\n",
    "        )\n",
    "        \n",
    "    output_align_identity = torch.eye(a.linear.weight.shape[0], device=a.linear.weight.device)\n",
    "    output_align_mat = torch.cat((output_align_identity/2, output_align_identity/2), dim=1)\n",
    "    transforms['linear'].output_align = output_align_mat\n",
    "    transforms['linear'] = merge_linear(\n",
    "        state_dict, 'linear', a.linear, b.linear, \n",
    "        transforms['block3'], transforms['linear'],\n",
    "        recompute_output=False\n",
    "    )\n",
    "    \n",
    "    return transforms\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "432c3a2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use the train loader with data augmentation as this gives better results\n",
    "def reset_bn_stats(model, epochs=1, loader=train_aug_loader):\n",
    "    # resetting stats to baseline first as below is necessary for stability\n",
    "    for m in model.modules():\n",
    "        if type(m) == nn.BatchNorm2d:\n",
    "            m.momentum = None # use simple average\n",
    "            m.reset_running_stats()\n",
    "    # run a single train epoch with augmentations to recalc stats\n",
    "    model.train()\n",
    "    for _ in range(epochs):\n",
    "        with torch.no_grad(), autocast():\n",
    "            for images, _ in loader:\n",
    "                output = model(images.to(DEVICE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "73225ade",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.778\n",
      "0.7758\n"
     ]
    }
   ],
   "source": [
    "model1 = resnet20(w=4, text_head=True).to(DEVICE)\n",
    "model2 = resnet20(w=4, text_head=True).to(DEVICE)\n",
    "load_model(model1, f'resnet20x4_CIFAR50_clses{model1_classes.tolist()}')\n",
    "load_model(model2, f'resnet20x4_CIFAR50_clses{model2_classes.tolist()}')\n",
    "\n",
    "print(evaluate_texthead(model1, test_loader1, class_vecs1, remap_class_idxs=class_idxs))\n",
    "print(evaluate_texthead(model2, test_loader2, class_vecs2, remap_class_idxs=class_idxs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6a989d99",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7bc67f5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_procrustes(corr_mtx):\n",
    "    U, _, Vh = torch.linalg.svd(corr_mtx)\n",
    "    S = torch.eye(U.shape[0], device=U.device)\n",
    "    S[-1, -1] = -1.\n",
    "    return (U @ S) @ Vh\n",
    "\n",
    "def match_tensors_procrustes(hull_tensor, eps=1e-7, interleave=False, random_perm=False):\n",
    "    \"\"\"\n",
    "    hull_tensor: [2O,I]\n",
    "    \"\"\"\n",
    "    O, I = hull_tensor.shape\n",
    "    O //= 2\n",
    "    \n",
    "    interleave_mat = torch.eye(2*O, device=hull_tensor.device)\n",
    "    if interleave:\n",
    "        A1, A2, B1, B2 = interleave_mat.chunk(4, dim=0)\n",
    "        interleave_mat = torch.cat([A1, B1, A2, B2], dim=0)\n",
    "        interleave_mat = interleave_mat.view(2, O, 2*O).transpose(0, 1).reshape(2*O, 2*O)\n",
    "#         interleave_mat = interleave_mat[torch.randperm(2*O, device=hull_tensor.device)]\n",
    "    \n",
    "    hull_tensor = interleave_mat @ hull_tensor\n",
    "    \n",
    "#     hull_tensor = hull_tensor / (hull_tensor.norm(dim=-1, keepdim=True) + eps)\n",
    "    A, B = unconcat_mat(hull_tensor, dim=0)\n",
    "    scores = (A @ B.T)\n",
    "    \n",
    "    P = get_procrustes(scores)\n",
    "    \n",
    "    O_eye = torch.eye(O, device=hull_tensor.device)\n",
    "    \n",
    "    try:\n",
    "        row_idx, col_idx = scipy.optimize.linear_sum_assignment(scores.cpu().numpy())\n",
    "    except ValueError:\n",
    "        pdb.set_trace()\n",
    "    \n",
    "    A_perm = O_eye[torch.from_numpy(row_idx)]#[perm]\n",
    "    B_perm = P #O_eye[torch.from_numpy(col_idx)]#[perm]\n",
    "    \n",
    "    if random_perm:\n",
    "        perm = torch.randperm(O, device=A.device)\n",
    "        A_perm = A_perm[perm]\n",
    "        B_perm = B_perm[perm]\n",
    "    \n",
    "    merge = (torch.cat((A_perm, B_perm), dim=1) / 2.) @ interleave_mat\n",
    "    unmerge = interleave_mat.T @ (torch.cat((A_perm.T, B_perm.T), dim=0))\n",
    "    return merge, unmerge\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "86b91ee3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def kmeans_matching(\n",
    "    hull_tensor,\n",
    "    interleave=False,\n",
    "    random_perm=False,\n",
    "    r=.5\n",
    "):\n",
    "    hull_normed = hull_tensor / hull_tensor.norm(dim=-1, keepdim=True)\n",
    "    O = hull_tensor.shape[0]\n",
    "    k = int(O * (1-r))\n",
    "    cluster_ids, cluster_centers = kmeans(\n",
    "        X=hull_normed, num_clusters=k, \n",
    "        distance='cosine', \n",
    "        device=hull_tensor.device,\n",
    "        tqdm_flag=False,\n",
    "        seed=123\n",
    "    )\n",
    "\n",
    "    eye = torch.eye(k, device=hull_tensor.device)\n",
    "    transform = eye[cluster_ids]\n",
    "\n",
    "    unmerge = transform\n",
    "    merge = (transform / transform.sum(dim=0, keepdim=True)).T\n",
    "    return merge, unmerge\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "92277494",
   "metadata": {},
   "outputs": [],
   "source": [
    "def match_tensors_exact_bipartite(\n",
    "    hull_tensor,\n",
    "    interleave=False,\n",
    "    random_perm=False,\n",
    "    r=.5\n",
    "):\n",
    "    hull_normed = hull_tensor / hull_tensor.norm(dim=-1, keepdim=True)\n",
    "    O = hull_tensor.shape[0]\n",
    "    remainder = int(hull_tensor.shape[0] * (1-r))\n",
    "    bound = O - remainder\n",
    "    sims = hull_normed @ hull_normed.transpose(-1, -2)\n",
    "    torch.diagonal(sims)[:] = -torch.inf\n",
    "    permutation_matrix = torch.zeros((O, O - bound), device=sims.device)\n",
    "    for i in range(bound):\n",
    "        best_idx = sims.view(-1).argmax()\n",
    "        row_idx = best_idx % sims.shape[1]\n",
    "        col_idx = best_idx // sims.shape[1]\n",
    "        permutation_matrix[row_idx, i] = 1\n",
    "        permutation_matrix[col_idx, i] = 1\n",
    "        sims[row_idx] = -torch.inf\n",
    "        sims[col_idx] = -torch.inf\n",
    "        sims[:, row_idx] = -torch.inf\n",
    "        sims[:, col_idx] = -torch.inf\n",
    "    \n",
    "    unused = (sims.max(-1)[0] > -torch.inf).to(torch.int).nonzero().view(-1)\n",
    "    for i in range(bound, O-bound):\n",
    "        permutation_matrix[unused[i-bound], i] = 1\n",
    "    merge = permutation_matrix / (permutation_matrix.sum(dim=0, keepdim=True) + 1e-5)\n",
    "    unmerge = permutation_matrix\n",
    "    return merge.T, unmerge\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8fc5c564",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def check_convergence(old_transforms, current_transforms, prev_distance=np.inf, eps=1e-5):\n",
    "    if len(old_transforms) == 0: \n",
    "        return False, {}\n",
    "    transform_norms = {}\n",
    "    is_converged = True\n",
    "    for key, old_transform in old_transforms.items():\n",
    "        current_transform = current_transforms[key]\n",
    "        is_close = torch.allclose(\n",
    "            current_transform.output_align, \n",
    "            old_transform.output_align, \n",
    "            atol=eps\n",
    "        )\n",
    "        norm = torch.norm(current_transform.output_align - old_transform.output_align)\n",
    "        if not is_close: \n",
    "            is_converged = False\n",
    "        transform_norms[key] = torch.round(norm, decimals=3).cpu().numpy().round(3)\n",
    "        \n",
    "    if np.abs(prev_distance - np.mean(list(transform_norms.values()))) <= eps:\n",
    "        is_converged = True\n",
    "    return (is_converged, transform_norms)\n",
    "\n",
    "def find_transform_differences(old_transforms, current_transforms):\n",
    "    if len(old_transforms) == 0:\n",
    "        return {}\n",
    "    transform2norm = {}\n",
    "    for key, old_transform in old_transforms.items():\n",
    "        current_transform = current_transforms[key]\n",
    "        old_align = old_transform.output_align\n",
    "        new_align = current_transform.output_align\n",
    "        cost = old_align.T @ new_align\n",
    "        row_ind, col_idx = scipy.optimize.linear_sum_assignment(cost.detach().cpu().numpy())\n",
    "        permutation = torch.eye(new_align.shape[1], device=old_align.device)[col_idx]\n",
    "        aligned_new = new_align @ permutation\n",
    "#         pdb.set_trace()\n",
    "        norm = torch.norm(old_align - aligned_new).cpu().numpy()\n",
    "        transform2norm[key] = norm\n",
    "    return transform2norm        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "e01131ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# old_transforms['conv1'].output_align.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "5ea06937",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------- Step 1 Dist: nan ---------------------\n",
      "--------------------- Step 2 Dist: 12.606 ---------------------\n",
      "--------------------- Step 3 Dist: 12.606 ---------------------\n",
      "--------------------- Step 4 Dist: 12.606 ---------------------\n",
      "--------------------- Step 5 Dist: 12.606 ---------------------\n",
      "--------------------- Step 6 Dist: 12.606 ---------------------\n",
      "--------------------- Step 7 Dist: 12.599 ---------------------\n",
      "--------------------- Step 8 Dist: 12.606 ---------------------\n",
      "--------------------- Step 9 Dist: 12.606 ---------------------\n",
      "--------------------- Step 10 Dist: 12.605 ---------------------\n",
      "--------------------- Step 11 Dist: 12.606 ---------------------\n",
      "--------------------- Step 12 Dist: 12.606 ---------------------\n",
      "--------------------- Step 13 Dist: 12.606 ---------------------\n",
      "--------------------- Step 14 Dist: 12.606 ---------------------\n",
      "--------------------- Step 15 Dist: 12.606 ---------------------\n",
      "--------------------- Step 16 Dist: 12.605 ---------------------\n",
      "--------------------- Step 17 Dist: 12.606 ---------------------\n",
      "--------------------- Step 18 Dist: 12.606 ---------------------\n",
      "--------------------- Step 19 Dist: 12.605 ---------------------\n",
      "--------------------- Step 20 Dist: 12.606 ---------------------\n",
      "--------------------- Step 21 Dist: 12.606 ---------------------\n",
      "--------------------- Step 22 Dist: 12.606 ---------------------\n",
      "--------------------- Step 23 Dist: 12.606 ---------------------\n",
      "--------------------- Step 24 Dist: 12.606 ---------------------\n",
      "--------------------- Step 25 Dist: 12.606 ---------------------\n",
      "--------------------- Step 26 Dist: 12.606 ---------------------\n",
      "--------------------- Step 27 Dist: 12.606 ---------------------\n",
      "--------------------- Step 28 Dist: 12.606 ---------------------\n",
      "--------------------- Step 29 Dist: 12.606 ---------------------\n",
      "--------------------- Step 30 Dist: 12.606 ---------------------\n",
      "--------------------- Step 31 Dist: 12.606 ---------------------\n",
      "--------------------- Step 32 Dist: 12.606 ---------------------\n",
      "--------------------- Step 33 Dist: 12.606 ---------------------\n",
      "--------------------- Step 34 Dist: 12.605 ---------------------\n",
      "--------------------- Step 35 Dist: 12.606 ---------------------\n",
      "--------------------- Step 36 Dist: 12.606 ---------------------\n",
      "--------------------- Step 37 Dist: 12.605 ---------------------\n",
      "--------------------- Step 38 Dist: 12.606 ---------------------\n",
      "--------------------- Step 39 Dist: 12.605 ---------------------\n",
      "--------------------- Step 40 Dist: 12.606 ---------------------\n",
      "--------------------- Step 41 Dist: 12.606 ---------------------\n",
      "--------------------- Step 42 Dist: 12.606 ---------------------\n",
      "--------------------- Step 43 Dist: 12.606 ---------------------\n",
      "--------------------- Step 44 Dist: 12.606 ---------------------\n",
      "--------------------- Step 45 Dist: 12.606 ---------------------\n",
      "--------------------- Step 46 Dist: 12.606 ---------------------\n",
      "--------------------- Step 47 Dist: 12.606 ---------------------\n",
      "--------------------- Step 48 Dist: 12.606 ---------------------\n",
      "--------------------- Step 49 Dist: 12.606 ---------------------\n",
      "--------------------- Step 50 Dist: 12.605 ---------------------\n",
      "--------------------- Step 51 Dist: 12.606 ---------------------\n",
      "--------------------- Step 52 Dist: 12.606 ---------------------\n",
      "--------------------- Step 53 Dist: 12.606 ---------------------\n",
      "--------------------- Step 54 Dist: 12.606 ---------------------\n",
      "--------------------- Step 55 Dist: 12.606 ---------------------\n",
      "--------------------- Step 56 Dist: 12.606 ---------------------\n",
      "--------------------- Step 57 Dist: 12.606 ---------------------\n",
      "--------------------- Step 58 Dist: 12.606 ---------------------\n",
      "--------------------- Step 59 Dist: 12.606 ---------------------\n",
      "--------------------- Step 60 Dist: 12.606 ---------------------\n",
      "--------------------- Step 61 Dist: 12.606 ---------------------\n",
      "--------------------- Step 62 Dist: 12.606 ---------------------\n",
      "--------------------- Step 63 Dist: 12.605 ---------------------\n",
      "--------------------- Step 64 Dist: 12.605 ---------------------\n",
      "--------------------- Step 65 Dist: 12.606 ---------------------\n",
      "--------------------- Step 66 Dist: 12.606 ---------------------\n",
      "--------------------- Step 67 Dist: 12.606 ---------------------\n",
      "--------------------- Step 68 Dist: 12.606 ---------------------\n",
      "--------------------- Step 69 Dist: 12.606 ---------------------\n",
      "--------------------- Step 70 Dist: 12.606 ---------------------\n",
      "--------------------- Step 71 Dist: 12.606 ---------------------\n",
      "--------------------- Step 72 Dist: 12.605 ---------------------\n",
      "--------------------- Step 73 Dist: 12.606 ---------------------\n",
      "--------------------- Step 74 Dist: 12.605 ---------------------\n",
      "--------------------- Step 75 Dist: 12.606 ---------------------\n",
      "--------------------- Step 76 Dist: 12.606 ---------------------\n",
      "--------------------- Step 77 Dist: 12.606 ---------------------\n",
      "--------------------- Step 78 Dist: 12.606 ---------------------\n",
      "--------------------- Step 79 Dist: 12.605 ---------------------\n",
      "--------------------- Step 80 Dist: 12.605 ---------------------\n",
      "--------------------- Step 81 Dist: 12.606 ---------------------\n",
      "--------------------- Step 82 Dist: 12.606 ---------------------\n",
      "--------------------- Step 83 Dist: 12.605 ---------------------\n",
      "--------------------- Step 84 Dist: 12.606 ---------------------\n",
      "--------------------- Step 85 Dist: 12.606 ---------------------\n",
      "--------------------- Step 86 Dist: 12.606 ---------------------\n",
      "--------------------- Step 87 Dist: 12.606 ---------------------\n",
      "--------------------- Step 88 Dist: 12.606 ---------------------\n",
      "--------------------- Step 89 Dist: 12.605 ---------------------\n",
      "--------------------- Step 90 Dist: 12.606 ---------------------\n",
      "--------------------- Step 91 Dist: 12.606 ---------------------\n",
      "--------------------- Step 92 Dist: 12.605 ---------------------\n",
      "--------------------- Step 93 Dist: 12.606 ---------------------\n",
      "--------------------- Step 94 Dist: 12.606 ---------------------\n",
      "--------------------- Step 95 Dist: 12.606 ---------------------\n",
      "--------------------- Step 96 Dist: 12.606 ---------------------\n",
      "--------------------- Step 97 Dist: 12.606 ---------------------\n",
      "--------------------- Step 98 Dist: 12.606 ---------------------\n",
      "--------------------- Step 99 Dist: 12.605 ---------------------\n",
      "--------------------- Step 100 Dist: 12.605 ---------------------\n",
      "--------------------- Step 101 Dist: 12.606 ---------------------\n",
      "--------------------- Step 102 Dist: 12.605 ---------------------\n",
      "--------------------- Step 103 Dist: 12.603 ---------------------\n",
      "--------------------- Step 104 Dist: 12.605 ---------------------\n",
      "--------------------- Step 105 Dist: 12.606 ---------------------\n",
      "--------------------- Step 106 Dist: 12.606 ---------------------\n",
      "--------------------- Step 107 Dist: 12.606 ---------------------\n",
      "--------------------- Step 108 Dist: 12.606 ---------------------\n",
      "--------------------- Step 109 Dist: 12.606 ---------------------\n",
      "--------------------- Step 110 Dist: 12.606 ---------------------\n",
      "--------------------- Step 111 Dist: 12.606 ---------------------\n",
      "--------------------- Step 112 Dist: 12.606 ---------------------\n",
      "--------------------- Step 113 Dist: 12.606 ---------------------\n",
      "--------------------- Step 114 Dist: 12.606 ---------------------\n",
      "--------------------- Step 115 Dist: 12.606 ---------------------\n",
      "--------------------- Step 116 Dist: 12.606 ---------------------\n",
      "--------------------- Step 117 Dist: 12.606 ---------------------\n",
      "--------------------- Step 118 Dist: 12.603 ---------------------\n",
      "--------------------- Step 119 Dist: 12.606 ---------------------\n",
      "--------------------- Step 120 Dist: 12.606 ---------------------\n",
      "--------------------- Step 121 Dist: 12.606 ---------------------\n",
      "--------------------- Step 122 Dist: 12.606 ---------------------\n",
      "--------------------- Step 123 Dist: 12.606 ---------------------\n",
      "--------------------- Step 124 Dist: 12.606 ---------------------\n",
      "--------------------- Step 125 Dist: 12.606 ---------------------\n",
      "--------------------- Step 126 Dist: 12.606 ---------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------- Step 127 Dist: 12.605 ---------------------\n",
      "--------------------- Step 128 Dist: 12.606 ---------------------\n",
      "--------------------- Step 129 Dist: 12.606 ---------------------\n",
      "--------------------- Step 130 Dist: 12.606 ---------------------\n",
      "--------------------- Step 131 Dist: 12.605 ---------------------\n",
      "--------------------- Step 132 Dist: 12.606 ---------------------\n",
      "--------------------- Step 133 Dist: 12.606 ---------------------\n",
      "--------------------- Step 134 Dist: 12.606 ---------------------\n",
      "--------------------- Step 135 Dist: 12.606 ---------------------\n",
      "--------------------- Step 136 Dist: 12.606 ---------------------\n",
      "--------------------- Step 137 Dist: 12.606 ---------------------\n",
      "--------------------- Step 138 Dist: 12.606 ---------------------\n",
      "--------------------- Step 139 Dist: 12.606 ---------------------\n",
      "--------------------- Step 140 Dist: 12.606 ---------------------\n",
      "--------------------- Step 141 Dist: 12.606 ---------------------\n",
      "--------------------- Step 142 Dist: 12.606 ---------------------\n",
      "--------------------- Step 143 Dist: 12.606 ---------------------\n",
      "--------------------- Step 144 Dist: 12.605 ---------------------\n",
      "--------------------- Step 145 Dist: 12.606 ---------------------\n",
      "--------------------- Step 146 Dist: 12.605 ---------------------\n",
      "--------------------- Step 147 Dist: 12.606 ---------------------\n",
      "--------------------- Step 148 Dist: 12.606 ---------------------\n",
      "--------------------- Step 149 Dist: 12.606 ---------------------\n",
      "--------------------- Step 150 Dist: 12.606 ---------------------\n",
      "--------------------- Step 151 Dist: 12.606 ---------------------\n",
      "--------------------- Step 152 Dist: 12.605 ---------------------\n",
      "--------------------- Step 153 Dist: 12.606 ---------------------\n",
      "--------------------- Step 154 Dist: 12.606 ---------------------\n",
      "--------------------- Step 155 Dist: 12.606 ---------------------\n",
      "--------------------- Step 156 Dist: 12.606 ---------------------\n",
      "--------------------- Step 157 Dist: 12.606 ---------------------\n",
      "--------------------- Step 158 Dist: 12.606 ---------------------\n",
      "--------------------- Step 159 Dist: 12.606 ---------------------\n",
      "--------------------- Step 160 Dist: 12.606 ---------------------\n",
      "--------------------- Step 161 Dist: 12.605 ---------------------\n",
      "--------------------- Step 162 Dist: 12.606 ---------------------\n",
      "--------------------- Step 163 Dist: 12.606 ---------------------\n",
      "--------------------- Step 164 Dist: 12.605 ---------------------\n",
      "--------------------- Step 165 Dist: 12.605 ---------------------\n",
      "--------------------- Step 166 Dist: 12.606 ---------------------\n",
      "--------------------- Step 167 Dist: 12.606 ---------------------\n",
      "--------------------- Step 168 Dist: 12.606 ---------------------\n",
      "--------------------- Step 169 Dist: 12.606 ---------------------\n",
      "--------------------- Step 170 Dist: 12.606 ---------------------\n",
      "--------------------- Step 171 Dist: 12.606 ---------------------\n",
      "--------------------- Step 172 Dist: 12.605 ---------------------\n",
      "--------------------- Step 173 Dist: 12.606 ---------------------\n",
      "--------------------- Step 174 Dist: 12.606 ---------------------\n",
      "--------------------- Step 175 Dist: 12.606 ---------------------\n",
      "--------------------- Step 176 Dist: 12.606 ---------------------\n",
      "--------------------- Step 177 Dist: 12.606 ---------------------\n",
      "--------------------- Step 178 Dist: 12.606 ---------------------\n",
      "--------------------- Step 179 Dist: 12.606 ---------------------\n",
      "--------------------- Step 180 Dist: 12.606 ---------------------\n",
      "--------------------- Step 181 Dist: 12.606 ---------------------\n",
      "--------------------- Step 182 Dist: 12.606 ---------------------\n",
      "--------------------- Step 183 Dist: 12.606 ---------------------\n",
      "--------------------- Step 184 Dist: 12.606 ---------------------\n",
      "--------------------- Step 185 Dist: 12.606 ---------------------\n",
      "--------------------- Step 186 Dist: 12.606 ---------------------\n",
      "--------------------- Step 187 Dist: 12.606 ---------------------\n",
      "--------------------- Step 188 Dist: 12.606 ---------------------\n"
     ]
    }
   ],
   "source": [
    "# np.random.seed(123)\n",
    "# torch.manual_seed(123)\n",
    "# Can choose between:\n",
    "# match_tensors_tome: ToMe with or without interleaving\n",
    "# match_tensors_permute: \n",
    "# match_tensors_svd\n",
    "match_tensors = match_wrapper(match_tensors_exact_bipartite, interleave=False, random_perm=False)\n",
    "layer_transform = lambda : LayerTransform(normalize_tensors=False, tensor_merge_type='concat')\n",
    "old_state_dict = {}\n",
    "state_dict = {}\n",
    "old_transforms = defaultdict(lambda: layer_transform())\n",
    "new_transforms = defaultdict(lambda: layer_transform())\n",
    "modelc = resnet20(w=4, text_head=True).to(DEVICE)\n",
    "accuracies = []\n",
    "steps = []\n",
    "distances = []\n",
    "best_info = {'acc': 0., 'dist': np.inf}\n",
    "step = 1\n",
    "is_converged = False\n",
    "prev_distance = np.inf\n",
    "same_window = 5\n",
    "same_span = 0\n",
    "while not is_converged:\n",
    "# for step in range(110):\n",
    "    old_transforms = new_transforms\n",
    "    old_state_dict = deepcopy(state_dict)\n",
    "    new_transforms = merge_resnet20(state_dict, model1, model2, transforms=deepcopy(old_transforms))\n",
    "    if step == 0:\n",
    "        original_computation = deepcopy(new_transforms)\n",
    "    \n",
    "    transform2dist = find_transform_differences(old_transforms, new_transforms)\n",
    "    avg_distance = np.mean(list(transform2dist.values())\n",
    "    \n",
    "    if abs(avg_distance - prev_distance) <= 1e-5:\n",
    "        same_span += 1\n",
    "    else:\n",
    "        same_span = 0\n",
    "    if same_span >= same_window:\n",
    "        is_converged = True\n",
    "        \n",
    "    prev_distance = avg_distance\n",
    "    \n",
    "#     modelc.load_state_dict(state_dict)\n",
    "#     acc, perclass_acc = evaluate_texthead(\n",
    "#         modelc, test_loader, class_vectors=text_features, return_confusion=True\n",
    "#     )\n",
    "#     print(f'--------------------- Step {step} Dist: {avg_distance:.3f} Acc: {acc:.3f} ---------------------')\n",
    "    print(f'--------------------- Step {step} Dist: {avg_distance:.3f} ---------------------')\n",
    "    step += 1\n",
    "#     if step >= 100 or is_converged:\n",
    "#         break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "a3fd3e11",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7ff337debcb0>\n",
      "Traceback (most recent call last):\n",
      "  File \"/srv/share/gstoica3/miniconda3/envs/open-mmlab/lib/python3.7/site-packages/torch/utils/data/dataloader.py\", line 1466, in __del__\n",
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7ff337debcb0>    \n",
      "Traceback (most recent call last):\n",
      "Exception ignored in:   File \"/srv/share/gstoica3/miniconda3/envs/open-mmlab/lib/python3.7/site-packages/torch/utils/data/dataloader.py\", line 1466, in __del__\n",
      "self._shutdown_workers()\n",
      "<function _MultiProcessingDataLoaderIter.__del__ at 0x7ff337debcb0>    \n",
      "  File \"/srv/share/gstoica3/miniconda3/envs/open-mmlab/lib/python3.7/site-packages/torch/utils/data/dataloader.py\", line 1449, in _shutdown_workers\n",
      "Traceback (most recent call last):\n",
      "self._shutdown_workers()  File \"/srv/share/gstoica3/miniconda3/envs/open-mmlab/lib/python3.7/site-packages/torch/utils/data/dataloader.py\", line 1466, in __del__\n",
      "\n",
      "    Exception ignored in:   File \"/srv/share/gstoica3/miniconda3/envs/open-mmlab/lib/python3.7/site-packages/torch/utils/data/dataloader.py\", line 1449, in _shutdown_workers\n",
      "<function _MultiProcessingDataLoaderIter.__del__ at 0x7ff337debcb0>    if w.is_alive():\n",
      "    Traceback (most recent call last):\n",
      "self._shutdown_workers()if w.is_alive():\n",
      "  File \"/srv/share/gstoica3/miniconda3/envs/open-mmlab/lib/python3.7/site-packages/torch/utils/data/dataloader.py\", line 1449, in _shutdown_workers\n",
      "\n",
      "  File \"/srv/share/gstoica3/miniconda3/envs/open-mmlab/lib/python3.7/site-packages/torch/utils/data/dataloader.py\", line 1466, in __del__\n",
      "        \n",
      "if w.is_alive():\n",
      "self._shutdown_workers()  File \"/srv/share/gstoica3/miniconda3/envs/open-mmlab/lib/python3.7/multiprocessing/process.py\", line 151, in is_alive\n",
      "  File \"/srv/share/gstoica3/miniconda3/envs/open-mmlab/lib/python3.7/multiprocessing/process.py\", line 151, in is_alive\n",
      "\n",
      "  File \"/srv/share/gstoica3/miniconda3/envs/open-mmlab/lib/python3.7/site-packages/torch/utils/data/dataloader.py\", line 1449, in _shutdown_workers\n",
      "  File \"/srv/share/gstoica3/miniconda3/envs/open-mmlab/lib/python3.7/multiprocessing/process.py\", line 151, in is_alive\n",
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7ff337debcb0>\n",
      "    Traceback (most recent call last):\n",
      "if w.is_alive():  File \"/srv/share/gstoica3/miniconda3/envs/open-mmlab/lib/python3.7/site-packages/torch/utils/data/dataloader.py\", line 1466, in __del__\n",
      "\n",
      "    self._shutdown_workers()\n",
      "  File \"/srv/share/gstoica3/miniconda3/envs/open-mmlab/lib/python3.7/site-packages/torch/utils/data/dataloader.py\", line 1449, in _shutdown_workers\n",
      "      File \"/srv/share/gstoica3/miniconda3/envs/open-mmlab/lib/python3.7/multiprocessing/process.py\", line 151, in is_alive\n",
      "if w.is_alive():Exception ignored in: \n",
      "<function _MultiProcessingDataLoaderIter.__del__ at 0x7ff337debcb0>  File \"/srv/share/gstoica3/miniconda3/envs/open-mmlab/lib/python3.7/multiprocessing/process.py\", line 151, in is_alive\n",
      "\n",
      "    Traceback (most recent call last):\n",
      "assert self._parent_pid == os.getpid(), 'can only test a child process'        assert self._parent_pid == os.getpid(), 'can only test a child process'        \n",
      "  File \"/srv/share/gstoica3/miniconda3/envs/open-mmlab/lib/python3.7/site-packages/torch/utils/data/dataloader.py\", line 1466, in __del__\n",
      "assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "assert self._parent_pid == os.getpid(), 'can only test a child process'assert self._parent_pid == os.getpid(), 'can only test a child process'AssertionError\n",
      ": Exception ignored in:     \n",
      "\n",
      "AssertionErrorAssertionErrorself._shutdown_workers()can only test a child process\n",
      "AssertionError  File \"/srv/share/gstoica3/miniconda3/envs/open-mmlab/lib/python3.7/site-packages/torch/utils/data/dataloader.py\", line 1449, in _shutdown_workers\n",
      "<function _MultiProcessingDataLoaderIter.__del__ at 0x7ff337debcb0>\n",
      ": : \n",
      "    : AssertionErrorcan only test a child processcan only test a child processTraceback (most recent call last):\n",
      "\n",
      "  File \"/srv/share/gstoica3/miniconda3/envs/open-mmlab/lib/python3.7/site-packages/torch/utils/data/dataloader.py\", line 1466, in __del__\n",
      ": \n",
      "can only test a child processif w.is_alive():    \n",
      "can only test a child processself._shutdown_workers()\n",
      "\n",
      "\n",
      "  File \"/srv/share/gstoica3/miniconda3/envs/open-mmlab/lib/python3.7/site-packages/torch/utils/data/dataloader.py\", line 1449, in _shutdown_workers\n",
      "  File \"/srv/share/gstoica3/miniconda3/envs/open-mmlab/lib/python3.7/multiprocessing/process.py\", line 151, in is_alive\n",
      "        if w.is_alive():assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "\n",
      "  File \"/srv/share/gstoica3/miniconda3/envs/open-mmlab/lib/python3.7/multiprocessing/process.py\", line 151, in is_alive\n",
      "AssertionError    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "AssertionErrorException ignored in: : : can only test a child process<function _MultiProcessingDataLoaderIter.__del__ at 0x7ff337debcb0>can only test a child process\n",
      "\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/srv/share/gstoica3/miniconda3/envs/open-mmlab/lib/python3.7/site-packages/torch/utils/data/dataloader.py\", line 1466, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/srv/share/gstoica3/miniconda3/envs/open-mmlab/lib/python3.7/site-packages/torch/utils/data/dataloader.py\", line 1449, in _shutdown_workers\n",
      "    if w.is_alive():\n",
      "  File \"/srv/share/gstoica3/miniconda3/envs/open-mmlab/lib/python3.7/multiprocessing/process.py\", line 151, in is_alive\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "AssertionError: can only test a child process\n",
      "Exception ignored in: <function _ConnectionBase.__del__ at 0x7ff33842bdd0>\n",
      "Traceback (most recent call last):\n",
      "  File \"/srv/share/gstoica3/miniconda3/envs/open-mmlab/lib/python3.7/multiprocessing/connection.py\", line 132, in __del__\n",
      "    self._close()\n",
      "  File \"/srv/share/gstoica3/miniconda3/envs/open-mmlab/lib/python3.7/multiprocessing/connection.py\", line 361, in _close\n",
      "    _close(self._handle)\n",
      "OSError: [Errno 9] Bad file descriptor\n",
      "Traceback (most recent call last):\n",
      "  File \"/srv/share/gstoica3/miniconda3/envs/open-mmlab/lib/python3.7/multiprocessing/queues.py\", line 232, in _feed\n",
      "    close()\n",
      "  File \"/srv/share/gstoica3/miniconda3/envs/open-mmlab/lib/python3.7/multiprocessing/connection.py\", line 177, in close\n",
      "    self._close()\n",
      "  File \"/srv/share/gstoica3/miniconda3/envs/open-mmlab/lib/python3.7/multiprocessing/connection.py\", line 361, in _close\n",
      "    _close(self._handle)\n",
      "OSError: [Errno 9] Bad file descriptor\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.3705"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modelc.load_state_dict(state_dict)\n",
    "reset_bn_stats(modelc)\n",
    "acc, perclass_acc = evaluate_texthead(\n",
    "    modelc, test_loader, class_vectors=text_features, return_confusion=True\n",
    ")\n",
    "acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "571a48be",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "55c0d505",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.22, 0.19, 0.16, 0.27, 0.35, 0.21, 0.2, 0.28, 0.45, 0.26, 0.21, 0.27, 0.47, 0.28, 0.42, 0.55, 0.35, 0.74, 0.28, 0.22, 0.78, 0.37, 0.42, 0.59, 0.58, 0.22, 0.16, 0.27, 0.48, 0.44, 0.15, 0.3, 0.22, 0.27, 0.31, 0.39, 0.7, 0.52, 0.18, 0.18, 0.45, 0.48, 0.42, 0.53, 0.21, 0.2, 0.1, 0.25, 0.71, 0.26, 0.3, 0.64, 0.37, 0.58, 0.53, 0.14, 0.8, 0.63, 0.7, 0.33, 0.36, 0.72, 0.5, 0.27, 0.29, 0.07, 0.32, 0.17, 0.62, 0.58, 0.38, 0.57, 0.23, 0.36, 0.34, 0.26, 0.58, 0.14, 0.52, 0.29, 0.1, 0.73, 0.72, 0.62, 0.1, 0.42, 0.57, 0.51, 0.51, 0.24, 0.45, 0.24, 0.39, 0.33, 0.44, 0.31, 0.18, 0.34, 0.56, 0.35]\n"
     ]
    }
   ],
   "source": [
    "print(perclass_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cd0921d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "954a355d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2da6d12a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(accuracies)\n",
    "print(np.round(distances, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f61184b",
   "metadata": {},
   "outputs": [],
   "source": [
    "steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fc7ea60",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e115a45",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "permuted_sd = pickle.load(open(\n",
    "'/srv/share/jbjorner3/checkpoints/REPAIR/pytorch_cifar50_2_permuted_to_1_jakob.pkl', 'rb'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a9144a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_permute = resnet20(w=4, text_head=True)\n",
    "model_permute.load_state_dict(permuted_sd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aab79bb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_permute = model_permute.to(DEVICE)\n",
    "print(evaluate_texthead(model_permute, test_loader, class_vectors=text_features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe8ad44b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mix_weights(model, alpha, sd0, sd1):\n",
    "    sd_alpha = {}\n",
    "    for k in sd0.keys():\n",
    "        param0 = sd0[k].to(DEVICE)\n",
    "        param1 = sd1[k].to(DEVICE)\n",
    "        sd_alpha[k] = (1 - alpha) * param0 + alpha * param1\n",
    "    model.load_state_dict(sd_alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6921abc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_model = resnet20(w=4, text_head=True).to(DEVICE)\n",
    "mix_weights(merged_model, .5, model1.state_dict(), model_permute.state_dict())\n",
    "\n",
    "print(evaluate_texthead(merged_model, test_loader, class_vectors=text_features))\n",
    "reset_bn_stats(merged_model)\n",
    "print(evaluate_texthead(merged_model, test_loader, class_vectors=text_features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe608bcc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
