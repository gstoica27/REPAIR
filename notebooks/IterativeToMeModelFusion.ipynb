{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "76e3fd77",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/srv/share/gstoica3/miniconda3/envs/open-mmlab/lib/python3.7/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<torch.autograd.grad_mode.set_grad_enabled at 0x7f911391f050>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import pdb\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.optimize\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from torch.optim import SGD, Adam, lr_scheduler\n",
    "import torchvision\n",
    "import torchvision.transforms as T\n",
    "\n",
    "from sys import platform\n",
    "\n",
    "DEVICE = 'mps' if platform == 'darwin' else 'cuda'\n",
    "if DEVICE == 'mps':\n",
    "    DOWNLOAD_PATH = '/Users/georgestoica/Downloads' \n",
    "else:\n",
    "    DOWNLOAD_PATH = '/srv/share/gstoica3/checkpoints/REPAIR/'\n",
    "    \n",
    "torch.autograd.set_grad_enabled(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3205761f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "33ed68a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(model, i):\n",
    "    sd = model.state_dict()\n",
    "    path = os.path.join(\n",
    "        DOWNLOAD_PATH,\n",
    "        '%s.pth.tar' % i\n",
    "    )\n",
    "    torch.save(model.state_dict(), path)\n",
    "\n",
    "def load_model(model, i):\n",
    "    path = os.path.join(\n",
    "        DOWNLOAD_PATH,\n",
    "        '%s.pth.tar' % i\n",
    "    )\n",
    "    sd = torch.load(path, map_location=torch.device(DEVICE))\n",
    "    model.load_state_dict(sd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "05f85fad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "CIFAR_MEAN = [125.307, 122.961, 113.8575]\n",
    "CIFAR_STD = [51.5865, 50.847, 51.255]\n",
    "normalize = T.Normalize(np.array(CIFAR_MEAN)/255, np.array(CIFAR_STD)/255)\n",
    "denormalize = T.Normalize(-np.array(CIFAR_MEAN)/np.array(CIFAR_STD), 255/np.array(CIFAR_STD))\n",
    "\n",
    "train_transform = T.Compose([\n",
    "    T.RandomHorizontalFlip(),\n",
    "    T.RandomCrop(32, padding=4),\n",
    "    T.ToTensor(),\n",
    "    normalize,\n",
    "])\n",
    "test_transform = T.Compose([\n",
    "    T.ToTensor(),\n",
    "    normalize,\n",
    "])\n",
    "train_dset = torchvision.datasets.CIFAR10(root='/tmp', train=True,\n",
    "                                        download=True, transform=train_transform)\n",
    "test_dset = torchvision.datasets.CIFAR10(root='/tmp', train=False,\n",
    "                                        download=True, transform=test_transform)\n",
    "\n",
    "train_aug_loader = torch.utils.data.DataLoader(train_dset, batch_size=500, shuffle=True, num_workers=8)\n",
    "test_loader = torch.utils.data.DataLoader(test_dset, batch_size=500, shuffle=False, num_workers=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0b77a3b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.nn.init as init\n",
    "\n",
    "def _weights_init(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if isinstance(m, nn.Linear) or isinstance(m, nn.Conv2d):\n",
    "        init.kaiming_normal_(m.weight)\n",
    "\n",
    "class LambdaLayer(nn.Module):\n",
    "    def __init__(self, lambd):\n",
    "        super(LambdaLayer, self).__init__()\n",
    "        self.lambd = lambd\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.lambd(x)\n",
    "\n",
    "\n",
    "class BasicBlock(nn.Module):\n",
    "    expansion = 1\n",
    "\n",
    "    def __init__(self, in_planes, planes, stride=1):\n",
    "        super(BasicBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_planes != planes:\n",
    "#             self.shortcut = LambdaLayer(lambda x:\n",
    "#                                         F.pad(x[:, :, ::2, ::2], (0, 0, 0, 0, planes//4, planes//4), \"constant\", 0))\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=False),\n",
    "                nn.BatchNorm2d(planes)\n",
    "            )\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.bn2(self.conv2(out))\n",
    "        out += self.shortcut(x)\n",
    "        out = F.relu(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "class ResNet(nn.Module):\n",
    "    def __init__(self, block, num_blocks, w=1, num_classes=10, text_head=False):\n",
    "        super(ResNet, self).__init__()\n",
    "        self.in_planes = int(w*16)\n",
    "\n",
    "        self.conv1 = nn.Conv2d(3, int(w*16), kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(int(w*16))\n",
    "        self.layer1 = self._make_layer(block, int(w*16), num_blocks[0], stride=1)\n",
    "        self.layer2 = self._make_layer(block, int(w*32), num_blocks[1], stride=2)\n",
    "        self.layer3 = self._make_layer(block, int(w*64), num_blocks[2], stride=2)\n",
    "        if text_head:\n",
    "            num_classes = 512\n",
    "        self.linear = nn.Linear(int(w*64), num_classes)\n",
    "\n",
    "        self.apply(_weights_init)\n",
    "\n",
    "    def _make_layer(self, block, planes, num_blocks, stride):\n",
    "        strides = [stride] + [1]*(num_blocks-1)\n",
    "        layers = []\n",
    "        for stride in strides:\n",
    "            layers.append(block(self.in_planes, planes, stride))\n",
    "            self.in_planes = planes * block.expansion\n",
    "\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.layer1(out)\n",
    "        out = self.layer2(out)\n",
    "        out = self.layer3(out)\n",
    "        out = F.avg_pool2d(out, out.size()[3])\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.linear(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "def resnet20(w=1, text_head=False):\n",
    "    return ResNet(BasicBlock, [3, 3, 3], w=w, text_head=text_head)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c93b97ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluates accuracy\n",
    "def evaluate(model, loader=test_loader, return_confusion=False):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    confusion_matrix = np.zeros((10, 10))\n",
    "    with torch.no_grad(), autocast():\n",
    "        for inputs, labels in loader:\n",
    "            outputs = model(inputs.to(DEVICE))\n",
    "            pred = outputs.argmax(dim=1)\n",
    "            correct += (labels.to(DEVICE) == pred).sum().item()\n",
    "            confusion_matrix[labels.cpu().numpy(), pred.cpu().numpy()] += 1\n",
    "    confusion_matrix /= confusion_matrix.sum(-1, keepdims=True)\n",
    "    if return_confusion:\n",
    "        return correct, confusion_matrix\n",
    "    else:\n",
    "        return correct\n",
    "\n",
    "# evaluates loss\n",
    "def evaluate1(model, loader=test_loader):\n",
    "    model.eval()\n",
    "    losses = []\n",
    "    with torch.no_grad(), autocast():\n",
    "        for inputs, labels in loader:\n",
    "            outputs = model(inputs.to(DEVICE))\n",
    "            loss = F.cross_entropy(outputs, labels.to(DEVICE))\n",
    "            losses.append(loss.item())\n",
    "    return np.array(losses).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7d7ff30c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use the train loader with data augmentation as this gives better results\n",
    "def reset_bn_stats(model, epochs=1, loader=train_aug_loader):\n",
    "    # resetting stats to baseline first as below is necessary for stability\n",
    "    for m in model.modules():\n",
    "        if type(m) == nn.BatchNorm2d:\n",
    "            m.momentum = None # use simple average\n",
    "            m.reset_running_stats()\n",
    "    # run a single train epoch with augmentations to recalc stats\n",
    "    model.train()\n",
    "    for _ in range(epochs):\n",
    "        with torch.no_grad(), autocast():\n",
    "            for images, _ in loader:\n",
    "                output = model(images.to(DEVICE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "965965fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bipartite_soft_matching(\n",
    "    metric: torch.Tensor,\n",
    "    r: float,\n",
    "    class_token: bool = False,\n",
    "    distill_token: bool = False,\n",
    "):\n",
    "    \"\"\"\n",
    "    Applies ToMe with a balanced matching set (50%, 50%).\n",
    "    Input size is [batch, tokens, channels].\n",
    "    r indicates the ratio of tokens to remove (max 50% of tokens).\n",
    "    Extra args:\n",
    "     - class_token: Whether or not there's a class token.\n",
    "     - distill_token: Whether or not there's also a distillation token.\n",
    "    When enabled, the class token and distillation tokens won't get merged.\n",
    "    \"\"\"\n",
    "    protected = 0\n",
    "    if class_token:\n",
    "        protected += 1\n",
    "    if distill_token:\n",
    "        protected += 1\n",
    "\n",
    "    # We can only reduce by a maximum of 50% tokens\n",
    "    t = metric.shape[1]\n",
    "    r = int(r * t)\n",
    "    r = min(r, (t - protected) // 2)\n",
    "\n",
    "    if r <= 0:\n",
    "        return do_nothing, do_nothing\n",
    "\n",
    "    with torch.no_grad():\n",
    "        metric = metric / metric.norm(dim=-1, keepdim=True)\n",
    "        a, b = metric.chunk(2, dim=-2)\n",
    "        scores = a @ b.transpose(-1, -2)\n",
    "\n",
    "        if class_token:\n",
    "            scores[..., 0, :] = -math.inf\n",
    "        if distill_token:\n",
    "            scores[..., :, 0] = -math.inf\n",
    "\n",
    "        node_max, node_idx = scores.max(dim=-1)\n",
    "        edge_idx = node_max.argsort(dim=-1, descending=True)[..., None]\n",
    "\n",
    "        unm_idx = edge_idx[..., r:, :]  # Unmerged Tokens\n",
    "        src_idx = edge_idx[..., :r, :]  # Merged Tokens\n",
    "        dst_idx = node_idx[..., None].gather(dim=-2, index=src_idx)\n",
    "\n",
    "        if class_token:\n",
    "            # Sort to ensure the class token is at the start\n",
    "            unm_idx = unm_idx.sort(dim=1)[0]\n",
    "\n",
    "    def merge(x: torch.Tensor, mode=\"mean\") -> torch.Tensor:\n",
    "        src, dst = x.chunk(2, dim=-2)\n",
    "        n, t1, c = src.shape\n",
    "        unm = src.gather(dim=-2, index=unm_idx.expand(n, t1 - r, c))\n",
    "        src = src.gather(dim=-2, index=src_idx.expand(n, r, c))\n",
    "        dst = dst.scatter_reduce(-2, dst_idx.expand(n, r, c), src, reduce=mode)\n",
    "\n",
    "        if distill_token:\n",
    "            return torch.cat([unm[:, :1], dst[:, :1], unm[:, 1:], dst[:, 1:]], dim=1)\n",
    "        else:\n",
    "            return torch.cat([unm, dst], dim=1)\n",
    "\n",
    "    def unmerge(x: torch.Tensor) -> torch.Tensor:\n",
    "        unm_len = unm_idx.shape[1]\n",
    "        unm, dst = x[..., :unm_len, :], x[..., unm_len:, :]\n",
    "        n, _, c = unm.shape\n",
    "\n",
    "        src = dst.gather(dim=-2, index=dst_idx.expand(n, r, c))\n",
    "\n",
    "        out = torch.zeros(n, metric.shape[1], c, device=x.device, dtype=x.dtype)\n",
    "\n",
    "        out[..., dst.shape[-2]:, :] = dst\n",
    "        out.scatter_(dim=-2, index=(unm_idx).expand(n, unm_len, c), src=unm)\n",
    "        out.scatter_(dim=-2, index=(src_idx).expand(n, r, c), src=src)\n",
    "\n",
    "        return out\n",
    "\n",
    "    return merge, unmerge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94a8be3f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8be7e088",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9536, 9531)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modela = resnet20(w=4).to(DEVICE)\n",
    "modelb = resnet20(w=4).to(DEVICE)\n",
    "load_model(modela, 'resnet20x4_v1')\n",
    "load_model(modelb, 'resnet20x4_v3')\n",
    "\n",
    "evaluate(modela), evaluate(modelb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a3eca0d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1., 1., 1., 1., 2., 2., 2., 2., 2., 3., 3., 3., 3., 3.],\n",
       "        [1., 1., 1., 1., 1., 2., 2., 2., 2., 2., 3., 3., 3., 3., 3.],\n",
       "        [1., 1., 1., 1., 1., 2., 2., 2., 2., 2., 3., 3., 3., 3., 3.],\n",
       "        [1., 1., 1., 1., 1., 2., 2., 2., 2., 2., 3., 3., 3., 3., 3.]])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def concat_mats(args, dim=0):\n",
    "    return torch.cat(args, dim=dim)\n",
    "\n",
    "ones = torch.ones(4, 5)\n",
    "twos = ones * 2\n",
    "threes = ones * 3\n",
    "concat_mats((ones, twos, threes), dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b278541d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def concat_mats(args, dim=0):\n",
    "    return torch.cat(args, dim=dim)\n",
    "\n",
    "def unconcat_mat(tensor, dim=0):\n",
    "    return torch.chunk(tensor, chunks=2, dim=dim)\n",
    "\n",
    "def match_tensors_permute(hull_tensor, eps=1e-7, interleave=False, random_perm=False):\n",
    "    \"\"\"\n",
    "    hull_tensor: [2O,I]\n",
    "    \"\"\"\n",
    "    O, I = hull_tensor.shape\n",
    "    O //= 2\n",
    "    \n",
    "    interleave_mat = torch.eye(2*O, device=hull_tensor.device)\n",
    "    if interleave:\n",
    "        A1, A2, B1, B2 = interleave_mat.chunk(4, dim=0)\n",
    "        interleave_mat = torch.cat([A1, B1, A2, B2], dim=0)\n",
    "        interleave_mat = interleave_mat.view(2, O, 2*O).transpose(0, 1).reshape(2*O, 2*O)\n",
    "#         interleave_mat = interleave_mat[torch.randperm(2*O, device=hull_tensor.device)]\n",
    "    \n",
    "    hull_tensor = interleave_mat @ hull_tensor\n",
    "    \n",
    "    hull_tensor = hull_tensor / (hull_tensor.norm(dim=-1, keepdim=True) + eps)\n",
    "    A, B = unconcat_mat(hull_tensor, dim=0)\n",
    "    scores = -(A @ B.T)\n",
    "    \n",
    "    O_eye = torch.eye(O, device=hull_tensor.device)\n",
    "    \n",
    "    try:\n",
    "        row_idx, col_idx = scipy.optimize.linear_sum_assignment(scores.cpu().numpy())\n",
    "    except ValueError:\n",
    "        pdb.set_trace()\n",
    "    \n",
    "    A_perm = O_eye[torch.from_numpy(row_idx)]#[perm]\n",
    "    B_perm = O_eye[torch.from_numpy(col_idx)]#[perm]\n",
    "    \n",
    "    if random_perm:\n",
    "        perm = torch.randperm(O, device=A.device)\n",
    "        A_perm = A_perm[perm]\n",
    "        B_perm = B_perm[perm]\n",
    "    \n",
    "    merge = (torch.cat((A_perm, B_perm), dim=1) / 2.) @ interleave_mat\n",
    "    unmerge = interleave_mat.T @ (torch.cat((A_perm.T, B_perm.T), dim=0))\n",
    "    return merge, unmerge\n",
    "\n",
    "\n",
    "def match_tensors_tome(hull_tensor, eps=1e-7, interleave=False, random_perm=False):\n",
    "    \"\"\"\n",
    "    hull_tensor: [2O,I]\n",
    "    \"\"\"\n",
    "    O, I = hull_tensor.shape\n",
    "    O //= 2\n",
    "    \n",
    "    big_eye = torch.eye(2*O, device=hull_tensor.device)\n",
    "    small_eye = torch.eye(O, device=hull_tensor.device)\n",
    "    \n",
    "    interleave_mat = big_eye\n",
    "    if interleave:\n",
    "        A1, A2, B1, B2 = interleave_mat.chunk(4, dim=0)\n",
    "        interleave_mat = torch.cat([A1, B1, A2, B2], dim=0)\n",
    "    \n",
    "    \n",
    "    hull_tensor = interleave_mat @ hull_tensor\n",
    "    \n",
    "    merge, unmerge = bipartite_soft_matching(hull_tensor[None], 0.5)\n",
    "    \n",
    "    merge_mat = merge(big_eye[None])[0] @ interleave_mat\n",
    "    unmerge_mat = interleave_mat.T @ unmerge(small_eye[None])[0]\n",
    "    return merge_mat, unmerge_mat\n",
    "\n",
    "def match_tensors_svd(hull_tensor, use_S=True):\n",
    "    # We can only reduce by a maximum of 50% tokens\n",
    "    t = hull_tensor.shape[0]\n",
    "    r = int(.5 * t)\n",
    "    r = min(r, (t) // 2)\n",
    "    with torch.no_grad():\n",
    "        hull_tensor = hull_tensor / hull_tensor.norm(dim=-1, keepdim=True)\n",
    "        scores = hull_tensor @ hull_tensor.transpose(-1, -2)\n",
    "        U, S, V = torch.svd(scores)\n",
    "        \n",
    "        U_r = U[:, :r]\n",
    "        S_r = torch.diag(S[:r]) if use_S else torch.eye(r, device=DEVICE)\n",
    "        V_r = V[:, :r]\n",
    "    merge_mat = U_r\n",
    "    unmerge_mat = S_r @ V_r.mT\n",
    "    return merge_mat.T, unmerge_mat.T\n",
    "\n",
    "# match_tensors = match_tensors\n",
    "\n",
    "def match_wrapper(fn, interleave=False, random_perm=False):\n",
    "    return lambda x: fn(x, interleave=interleave, random_perm=random_perm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "88b8009f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerTransform(dict):\n",
    "    def __init__(self, normalize_tensors=False):\n",
    "        super(LayerTransform, self).__init__()\n",
    "        self.output_align = None\n",
    "        self.next_input_align = None\n",
    "        self.normalize_tensors = normalize_tensors\n",
    "    \n",
    "    def compute_transform(self):\n",
    "        inputs = list(self.values())\n",
    "        if self.normalize_tensors:\n",
    "            for idx, inp in enumerate(inputs):\n",
    "                inputs[idx] = F.normalize(inp, dim=-1)\n",
    "        self.output_align, self.next_input_align = match_tensors(concat_mats(inputs, dim=-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "ddf39944",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lt = LayerTransform()\n",
    "lt[1] = torch.tensor(3)\n",
    "lt[2] = torch.tensor(4)\n",
    "lt.output_align = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "0afbc86e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def unflatten(x, k=3):\n",
    "    O, IHW = x.shape\n",
    "    return x.view(O, -1, k, k)\n",
    "\n",
    "def merge_first_convs(state_dict, prefix, a_conv, b_conv, output_transform):\n",
    "    flatten_conv = lambda x: x.flatten(1)\n",
    "    a_w = flatten_conv(a_conv.weight)\n",
    "    b_w = flatten_conv(b_conv.weight)\n",
    "    ab_w = concat_mats((a_w, b_w), dim=0)\n",
    "    output_transform[prefix] = ab_w\n",
    "    output_transform.compute_transform()\n",
    "    # merge_mat, unmerge_mat = match_tensors(ab_w)\n",
    "    c_w = output_transform.output_align @ ab_w\n",
    "    state_dict[prefix + '.weight'] = unflatten(c_w, a_conv.weight.shape[-1])\n",
    "    return output_transform\n",
    "\n",
    "def merge_bn(state_dict, prefix, a_bn, b_bn, output_transform):\n",
    "    staterify = lambda bn: torch.stack((bn.weight, bn.bias, bn.running_mean), dim=1)\n",
    "    unstaterify = lambda stats: stats.unbind(-1)\n",
    "    \n",
    "    a_stats = staterify(a_bn)\n",
    "    b_stats = staterify(b_bn)\n",
    "    ab_stats = concat_mats((a_stats, b_stats), dim=0)\n",
    "    c_stats = output_transform.output_align @ ab_stats\n",
    "    c_weight, c_bias, c_mean = unstaterify(c_stats)\n",
    "    ab_var = concat_mats((a_bn.running_var[..., None], b_bn.running_var[...,None]))\n",
    "    var_out_transform = output_transform.output_align.square()\n",
    "    c_var = (var_out_transform @ ab_var).reshape(-1)\n",
    "    state_dict[prefix + '.weight'] = c_weight\n",
    "    state_dict[prefix + '.bias'] = c_bias\n",
    "    state_dict[prefix + '.running_mean'] = c_mean\n",
    "    state_dict[prefix + '.running_var'] = c_var\n",
    "    pass\n",
    "\n",
    "def block_diagonalize_tensors(tensor1, tensor2):\n",
    "    zerooos = torch.zeros_like(tensor1)\n",
    "    block_diagonal = concat_mats(\n",
    "        (\n",
    "            concat_mats((tensor1, zerooos), dim=1),\n",
    "            concat_mats((zerooos, tensor2), dim=1),\n",
    "        ),\n",
    "        dim=0\n",
    "    )\n",
    "    return block_diagonal\n",
    "\n",
    "def merge_hidden_conv(\n",
    "    state_dict, prefix, a_conv, b_conv, \n",
    "    input_transform, output_transform, \n",
    "    recompute_output=False\n",
    "):\n",
    "    O, I, H, W = a_conv.weight.shape\n",
    "    # Align output spaces for global iterations\n",
    "    if output_transform.output_align is not None:\n",
    "        output_block_diagonal_ab = block_diagonalize_tensors(\n",
    "            a_conv.weight.flatten(2),\n",
    "            b_conv.weight.flatten(2)\n",
    "        )\n",
    "        ab_output_aligned = output_transform.output_align @ output_block_diagonal_ab.flatten(1)\n",
    "        ab_output_aligned = ab_output_aligned.reshape(O, 2 * I, H*W).transpose(1, 0).flatten(1)\n",
    "        input_transform[prefix] = ab_output_aligned\n",
    "    \n",
    "    get_I_by_O_by_HW = lambda x: x.permute(1, 0, 2, 3).flatten(2)\n",
    "    \n",
    "    a_I_by_O_by_HW = get_I_by_O_by_HW(a_conv.weight)\n",
    "    b_I_by_O_by_HW = get_I_by_O_by_HW(b_conv.weight)\n",
    "    \n",
    "    dummy_zerooooo = torch.zeros_like(b_I_by_O_by_HW)\n",
    "    ab_block_diago = concat_mats(\n",
    "        (\n",
    "            concat_mats((a_I_by_O_by_HW, dummy_zerooooo), dim=1),\n",
    "            concat_mats((dummy_zerooooo, b_I_by_O_by_HW), dim=1)\n",
    "        ),\n",
    "        dim=0\n",
    "    )\n",
    "    \n",
    "    # [I,2I]x[2I,2OHW]->[I,2OHW]\n",
    "    ab_input_aligned = input_transform.next_input_align.T @ ab_block_diago.flatten(1)\n",
    "    ab_input_aligned = ab_input_aligned.\\\n",
    "    reshape(I, 2 * O, H*W).\\\n",
    "    transpose(1, 0).\\\n",
    "    flatten(1) # [I,2O,HW]->[2O,I,HW]->[2O,IHW]\n",
    "    output_transform[prefix] = ab_input_aligned\n",
    "    if recompute_output:\n",
    "        output_transform.compute_transform()\n",
    "    c_flat = output_transform.output_align @ ab_input_aligned\n",
    "    state_dict[prefix + '.weight'] = unflatten(c_flat, a_conv.weight.shape[-1])\n",
    "    \n",
    "    return output_transform\n",
    "\n",
    "def merge_linear(\n",
    "    state_dict, prefix, a_linear, \n",
    "    b_linear, input_transform, \n",
    "    output_transform, \n",
    "    recompute_output=False\n",
    "):\n",
    "    class conv_wrapper:\n",
    "        def __init__(self, linear):\n",
    "            self.weight = linear.weight[:, :, None, None]\n",
    "    \n",
    "    output_transform = merge_hidden_conv(\n",
    "        state_dict, prefix, conv_wrapper(a_linear), \n",
    "        conv_wrapper(b_linear), input_transform, \n",
    "        output_transform, recompute_output=recompute_output\n",
    "    )\n",
    "    state_dict[prefix + '.weight'] = state_dict[prefix + '.weight'][..., 0, 0]\n",
    "    state_dict[prefix + '.bias'] = output_transform.output_align @ concat_mats((a_linear.bias, b_linear.bias))\n",
    "    return output_transform\n",
    "    \n",
    "def merge_block(\n",
    "    state_dict, prefix, a_block, b_block, \n",
    "    input_transform, intra_transform,\n",
    "    output_transform=None, shortcut=False\n",
    "):\n",
    "    conv1_transform = merge_hidden_conv(\n",
    "        state_dict, prefix + '.conv1', a_block.conv1, b_block.conv1, \n",
    "        input_transform, intra_transform, recompute_output=True\n",
    "    )\n",
    "    merge_bn(state_dict, prefix + '.bn1', a_block.bn1, b_block.bn1, conv1_transform)\n",
    "    \n",
    "    \n",
    "    conv2_transform = merge_hidden_conv(\n",
    "        state_dict, \n",
    "        prefix + '.conv2', \n",
    "        a_block.conv2, \n",
    "        b_block.conv2, \n",
    "        conv1_transform,\n",
    "        output_transform,\n",
    "        recompute_output=shortcut\n",
    "    )\n",
    "    merge_bn(state_dict, prefix + '.bn2', a_block.bn2, b_block.bn2, conv2_transform)\n",
    "    \n",
    "    if shortcut:\n",
    "        shortcut_transform = merge_hidden_conv(\n",
    "            state_dict, \n",
    "            prefix + '.shortcut.0', \n",
    "            a_block.shortcut[0], \n",
    "            b_block.shortcut[0], \n",
    "            input_transform,\n",
    "            output_transform=conv2_transform\n",
    "        )\n",
    "        merge_bn(\n",
    "            state_dict, \n",
    "            prefix + '.shortcut.1', \n",
    "            a_block.shortcut[1], \n",
    "            b_block.shortcut[1], \n",
    "            shortcut_transform\n",
    "        )\n",
    "    \n",
    "    return conv2_transform\n",
    "\n",
    "hard_pass = lambda : None\n",
    "\n",
    "def merge_resnet20(state_dict, a, b, transforms):\n",
    "    transforms['conv1'] = merge_first_convs(\n",
    "        state_dict, 'conv1', a.conv1, b.conv1, \n",
    "        output_transform=transforms['conv1']\n",
    "    )\n",
    "    merge_bn(state_dict, 'bn1', a.bn1, b.bn1, transforms['conv1'])\n",
    "    \n",
    "    for i in range(3):\n",
    "        merge_block(\n",
    "            state_dict, f'layer1.{i}', a.layer1[i], b.layer1[i], \n",
    "            input_transform=transforms['conv1'], \n",
    "            intra_transform=transforms[f'block1.{i}'],\n",
    "            output_transform=transforms['conv1'],\n",
    "            shortcut=False\n",
    "        )\n",
    "    \n",
    "    transforms['block2'] = merge_block(\n",
    "        state_dict, 'layer2.0', a.layer2[0], b.layer2[0], \n",
    "        input_transform=transforms['conv1'], \n",
    "        intra_transform=transforms[f'block2.0'],\n",
    "        output_transform=transforms['block2'],\n",
    "        shortcut=True\n",
    "    )\n",
    "    \n",
    "    for i in range(1, 3):\n",
    "        merge_block(\n",
    "            state_dict, f'layer2.{i}', a.layer2[i], b.layer2[i], \n",
    "            input_transform=transforms['block2'], \n",
    "            intra_transform=transforms[f'block2.{i}'],\n",
    "            output_transform=transforms['block2'],\n",
    "            shortcut=False\n",
    "        )\n",
    "        \n",
    "    transforms['block3'] = merge_block(\n",
    "        state_dict, 'layer3.0', a.layer3[0], b.layer3[0], \n",
    "        input_transform=transforms['block2'], \n",
    "        intra_transform=transforms[f'block3.0'],\n",
    "        output_transform=transforms['block3'],\n",
    "        shortcut=True\n",
    "    )\n",
    "    for i in range(1, 3):\n",
    "        merge_block(\n",
    "            state_dict, f'layer3.{i}', a.layer3[i], b.layer3[i], \n",
    "            input_transform=transforms['block3'], \n",
    "            intra_transform=transforms[f'block3.{i}'],\n",
    "            output_transform=transforms['block3'],\n",
    "            shortcut=False\n",
    "        )\n",
    "        \n",
    "    output_align_identity = torch.eye(a.linear.weight.shape[0], device=a.linear.weight.device)\n",
    "    output_align_mat = torch.cat((output_align_identity/2, output_align_identity/2), dim=1)\n",
    "    transforms['linear'].output_align = output_align_mat\n",
    "    transforms['linear'] = merge_linear(\n",
    "        state_dict, 'linear', a.linear, b.linear, \n",
    "        transforms['block3'], transforms['linear'],\n",
    "        recompute_output=False\n",
    "    )\n",
    "    \n",
    "    return transforms\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f64d61ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9536, 9510)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modela = resnet20(w=4).to(DEVICE)\n",
    "modelb = resnet20(w=4).to(DEVICE)\n",
    "load_model(modela, 'resnet20x4_v1')\n",
    "load_model(modelb, 'resnet20x4_v2')\n",
    "\n",
    "evaluate(modela), evaluate(modelb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d85b68f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_converged(old_transforms, current_transforms, eps=1e-5):\n",
    "    if len(old_transforms) == 0: \n",
    "        return False\n",
    "    transform_norms = {}\n",
    "    for key, old_transform in old_transforms.items():\n",
    "        current_transform = current_transforms[key]\n",
    "        is_close = torch.allclose(\n",
    "            current_transform.output_align, \n",
    "            old_transform.output_align, \n",
    "            atol=eps\n",
    "        )\n",
    "        norm = torch.norm(current_transform.output_align - old_transform.output_align)\n",
    "        transform_norms[key] = (is_close, torch.round(norm, decimals=3))\n",
    "    print(transform_norms)\n",
    "    converges = [i[0] for i in transform_norms.values()]\n",
    "    return sum(converges) == len(transform_norms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "7d9603ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "ed29b4b6",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 2608 8824\n",
      "1 3721 9099\n",
      "2 2617 9163\n",
      "3 2890 9148\n",
      "4 3531 9155\n",
      "5 3145 9162\n",
      "6 3353 9172\n",
      "7 3018 9186\n",
      "8 4073 9154\n",
      "9 3258 9164\n"
     ]
    }
   ],
   "source": [
    "# Can choose between:\n",
    "# match_tensors_tome: ToMe with or without interleaving\n",
    "# match_tensors_permute: \n",
    "# match_tensors_svd\n",
    "match_tensors = match_wrapper(match_tensors_tome, interleave=False, random_perm=False)\n",
    "layer_transform = lambda : LayerTransform(normalize_tensors=False)\n",
    "state_dict = {}\n",
    "old_transforms = defaultdict(lambda: layer_transform())\n",
    "new_transforms = defaultdict(lambda: layer_transform())\n",
    "\n",
    "modelc = resnet20(w=4).to(DEVICE)\n",
    "step = 1\n",
    "# while not is_converged(old_transforms, new_transforms):\n",
    "for step in range(10):\n",
    "    old_transforms = new_transforms\n",
    "    new_transforms = merge_resnet20(state_dict, modela, modelb, transforms=deepcopy(old_transforms))\n",
    "    modelc.load_state_dict(state_dict)\n",
    "    pre_reset_acc = evaluate(modelc)\n",
    "    reset_bn_stats(modelc)\n",
    "    print(step, pre_reset_acc, evaluate(modelc))\n",
    "    step += 1\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "13add56f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['conv1', 'block2', 'block3', 'linear'])"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_transforms.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f380153",
   "metadata": {},
   "source": [
    "#### Weight Averaging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "id": "489a1e7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stoopid_dumb(state_dict, a, b):\n",
    "    for (k, v1), (_, v2) in zip(a.named_parameters(), b.named_parameters()):\n",
    "        state_dict[k] = (v1 + v2) / 2\n",
    "    for (k, v1), (_, v2) in zip(a.named_buffers(), b.named_buffers()):\n",
    "        state_dict[k] = (v1 + v2) / 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "id": "c4a574ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3094"
      ]
     },
     "execution_count": 245,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state_dict = {}\n",
    "stoopid_dumb(state_dict, modela, modelb)\n",
    "modelc = resnet20(w=4).to(DEVICE)\n",
    "modelc.load_state_dict(state_dict)\n",
    "reset_bn_stats(modelc)\n",
    "evaluate(modelc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd85008f",
   "metadata": {},
   "source": [
    "#### Model Ensembling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "id": "e9e6c1ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_logit_ensemble(model1, model2, loader=test_loader, return_confusion=False):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    confusion_matrix = np.zeros((10, 10))\n",
    "    with torch.no_grad(), autocast():\n",
    "        for inputs, labels in loader:\n",
    "            outputs1 = model1(inputs.to(DEVICE))\n",
    "            outputs2 = model2(inputs.to(DEVICE))\n",
    "            outputs = (outputs1 + outputs2) / 2.\n",
    "            pred = outputs.argmax(dim=1)\n",
    "            correct += (labels.to(DEVICE) == pred).sum().item()\n",
    "            confusion_matrix[labels.cpu().numpy(), pred.cpu().numpy()] += 1\n",
    "    confusion_matrix /= confusion_matrix.sum(-1, keepdims=True)\n",
    "    if return_confusion:\n",
    "        return correct, confusion_matrix\n",
    "    else:\n",
    "        return correct\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "id": "7c723873",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9586"
      ]
     },
     "execution_count": 253,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate_logit_ensemble(modela, modelb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ac2cde7",
   "metadata": {},
   "source": [
    "# Test on Disjoint Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6936d101",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "50000it [00:13, 3629.73it/s]\n",
      "50000it [00:13, 3638.01it/s]\n",
      "10000it [00:01, 5439.14it/s]\n",
      "10000it [00:01, 5496.54it/s]\n"
     ]
    }
   ],
   "source": [
    "model1_classes= np.array([3, 2, 0, 6, 4])\n",
    "model2_classes = np.array([5, 7, 9, 8, 1])\n",
    "\n",
    "valid_examples1 = [i for i, (_, label) in tqdm(enumerate(train_dset)) if label in model1_classes]\n",
    "valid_examples2 = [i for i, (_, label) in tqdm(enumerate(train_dset)) if label in model2_classes]\n",
    "\n",
    "assert len(set(valid_examples1).intersection(set(valid_examples2))) == 0, 'sets should be disjoint'\n",
    "\n",
    "train_aug_loader1 = torch.utils.data.DataLoader(\n",
    "    torch.utils.data.Subset(train_dset, valid_examples1), batch_size=500, shuffle=True, num_workers=8\n",
    ")\n",
    "train_aug_loader2 = torch.utils.data.DataLoader(\n",
    "    torch.utils.data.Subset(train_dset, valid_examples2), batch_size=500, shuffle=True, num_workers=8\n",
    ")\n",
    "\n",
    "test_valid_examples1 = [i for i, (_, label) in tqdm(enumerate(test_dset)) if label in model1_classes]\n",
    "test_valid_examples2 = [i for i, (_, label) in tqdm(enumerate(test_dset)) if label in model2_classes]\n",
    "\n",
    "test_loader1 = torch.utils.data.DataLoader(\n",
    "    torch.utils.data.Subset(test_dset, test_valid_examples1), batch_size=500, shuffle=False, num_workers=8\n",
    ")\n",
    "test_loader2 = torch.utils.data.DataLoader(\n",
    "    torch.utils.data.Subset(test_dset, test_valid_examples2), batch_size=500, shuffle=False, num_workers=8\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "05b34e27",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2, 4, 1, 0, 4, 0, 3, 1, 3, 2])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_idxs = np.zeros(10, dtype=int)\n",
    "class_idxs[model1_classes] = np.arange(5)\n",
    "class_idxs[model2_classes] = np.arange(5)\n",
    "class_idxs = torch.from_numpy(class_idxs)\n",
    "class_idxs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cbb75dfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import clip\n",
    "\n",
    "text_inputs = torch.cat([clip.tokenize(f\"a photo of a {c}\") for c in test_dset.classes]).to(DEVICE)\n",
    "model, preprocess = clip.load('ViT-B/32', DEVICE)\n",
    "with torch.no_grad():\n",
    "    text_features = model.encode_text(text_inputs)\n",
    "\n",
    "\n",
    "text_features /= text_features.norm(dim=-1, keepdim=True)\n",
    "class_vecs1 = text_features[model1_classes]\n",
    "class_vecs2 = text_features[model2_classes]\n",
    "\n",
    "if not os.path.exists(\n",
    "    os.path.join(\n",
    "        '/srv/share/gstoica3/checkpoints/REPAIR/',\n",
    "        f'resnet20x4_CIFAR5_clses{model1_classes.tolist()}.pth.tar'\n",
    "    )\n",
    "):\n",
    "    print('training model...')\n",
    "    model1 = resnet20(w=4).to(DEVICE)\n",
    "    train(\n",
    "        f'resnet20x4_CIFAR5_clses{model1_classes.tolist()}', \n",
    "        model=model1, \n",
    "        class_vectors=class_vecs1,\n",
    "        train_loader=train_aug_loader1,\n",
    "        test_loader=test_loader1,\n",
    "        remap_class_idxs=class_idxs\n",
    "    )\n",
    "if not os.path.exists(\n",
    "    os.path.join(\n",
    "        '/srv/share/gstoica3/checkpoints/REPAIR/',\n",
    "        f'resnet20x4_CIFAR5_clses{model1_classes.tolist()}.pth.tar'\n",
    "    )\n",
    "):\n",
    "    print('training model...')\n",
    "    model2 = resnet20(w=4).to(DEVICE)\n",
    "    train(\n",
    "        f'resnet20x4_CIFAR5_clses{model2_classes.tolist()}', \n",
    "        model=model2, \n",
    "        class_vectors=class_vecs2,\n",
    "        train_loader=train_aug_loader2,\n",
    "        test_loader=test_loader2,\n",
    "        remap_class_idxs=class_idxs\n",
    "    )\n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ec9f51c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluates accuracy\n",
    "def evaluate_texthead(model, loader, class_vectors, remap_class_idxs=None, return_confusion=False):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    confusion = np.zeros((10, 10))\n",
    "    with torch.no_grad(), autocast():\n",
    "        for inputs, labels in loader:\n",
    "            encodings = model(inputs.to(DEVICE))\n",
    "            normed_encodings = encodings / encodings.norm(dim=-1, keepdim=True)\n",
    "            outputs = normed_encodings @ class_vectors.T\n",
    "            pred = outputs.argmax(dim=1)\n",
    "            if remap_class_idxs is not None:\n",
    "                correct += (remap_class_idxs[labels].to(DEVICE) == pred).sum().item()\n",
    "            else:\n",
    "                correct += (labels.to(DEVICE) == pred).sum().item()\n",
    "            confusion[labels.cpu().numpy(), pred.cpu().numpy()] += 1\n",
    "            total += inputs.shape[0]\n",
    "    if return_confusion:\n",
    "        return correct / total, confusion / confusion.sum(-1, keepdims=True)\n",
    "    else:\n",
    "        return correct / total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e39e8a72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluates accuracy\n",
    "def evaluate_texthead(model, loader, class_vectors, remap_class_idxs=None, return_confusion=False):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    confusion = np.zeros((10, 10))\n",
    "    \n",
    "    totals = [0] * class_vectors.shape[0]\n",
    "    corrects = [0] * class_vectors.shape[0]\n",
    "    \n",
    "    with torch.no_grad(), autocast():\n",
    "        for inputs, labels in loader:\n",
    "            encodings = model(inputs.to(DEVICE))\n",
    "            normed_encodings = encodings / encodings.norm(dim=-1, keepdim=True)\n",
    "            outputs = normed_encodings @ class_vectors.T\n",
    "            pred = outputs.argmax(dim=1)\n",
    "            if remap_class_idxs is not None:\n",
    "                correct += (remap_class_idxs[labels].to(DEVICE) == pred).sum().item()\n",
    "            else:\n",
    "                for gt, p in zip(labels, pred):\n",
    "                    totals[gt] += 1\n",
    "                    \n",
    "                    if gt == p:\n",
    "                        correct += 1\n",
    "                        corrects[gt] += 1\n",
    "                \n",
    "#                 correct += (labels.to(DEVICE) == pred).sum().item()\n",
    "                \n",
    "            confusion[labels.cpu().numpy(), pred.cpu().numpy()] += 1\n",
    "            total += inputs.shape[0]\n",
    "    if return_confusion:\n",
    "        return correct / sum(totals), list(map(lambda a: a[0] / a[1], zip(corrects, totals)))\n",
    "    else:\n",
    "        return correct / total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6077f533",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9558\n",
      "0.9726\n"
     ]
    }
   ],
   "source": [
    "model1 = resnet20(w=4, text_head=True).to(DEVICE)\n",
    "model2 = resnet20(w=4, text_head=True).to(DEVICE)\n",
    "load_model(model1, f'resnet20x4_CIFAR5_clses{model1_classes.tolist()}')\n",
    "load_model(model2, f'resnet20x4_CIFAR5_clses{model2_classes.tolist()}')\n",
    "\n",
    "print(evaluate_texthead(model1, test_loader1, class_vecs1, remap_class_idxs=class_idxs))\n",
    "print(evaluate_texthead(model2, test_loader2, class_vecs2, remap_class_idxs=class_idxs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "ca2eedea",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.5417\n",
      "1 0.5226\n",
      "2 0.5382\n",
      "3 0.5344\n",
      "4 0.5379\n",
      "5 0.5402\n",
      "6 0.5517\n",
      "7 0.5372\n",
      "8 0.5435\n",
      "9 0.5231\n"
     ]
    }
   ],
   "source": [
    "# Can choose between:\n",
    "# match_tensors_tome: ToMe with or without interleaving\n",
    "# match_tensors_permute: \n",
    "# match_tensors_svd\n",
    "match_tensors = match_wrapper(match_tensors_tome, interleave=False, random_perm=False)\n",
    "layer_transform = lambda : LayerTransform(normalize_tensors=True)\n",
    "state_dict = {}\n",
    "old_transforms = defaultdict(lambda: layer_transform())\n",
    "new_transforms = defaultdict(lambda: layer_transform())\n",
    "modelc = resnet20(w=4, text_head=True).to(DEVICE)\n",
    "best_info = {'acc': 0.}\n",
    "step = 1\n",
    "# while not is_converged(old_transforms, new_transforms):\n",
    "for step in range(10):\n",
    "    old_transforms = new_transforms\n",
    "    new_transforms = merge_resnet20(state_dict, model1, model2, transforms=deepcopy(old_transforms))\n",
    "    modelc.load_state_dict(state_dict)\n",
    "    reset_bn_stats(modelc)\n",
    "    acc, perclass_acc = evaluate_texthead(\n",
    "        modelc, test_loader, class_vectors=text_features, return_confusion=True\n",
    "    )\n",
    "    if acc > best_info['acc']:\n",
    "        best_info['acc'] = acc\n",
    "        best_info['perclass_acc'] = perclass_acc\n",
    "    print(step, acc)\n",
    "    step += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "56c49d22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5517\n",
      "[0.524, 0.862, 0.237, 0.752, 0.549, 0.064, 0.259, 0.598, 0.754, 0.918]\n"
     ]
    }
   ],
   "source": [
    "print(best_info['acc'])\n",
    "print(best_info['perclass_acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "id": "56e10441",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4794 [0.98, 0.0, 0.918, 0.928, 0.974, 0.0, 0.973, 0.0, 0.021, 0.0]\n",
      "0.4884 [0.016, 0.977, 0.0, 0.004, 0.003, 0.974, 0.0, 0.968, 0.98, 0.962]\n"
     ]
    }
   ],
   "source": [
    "acc1, confusion_disj_1 = evaluate_texthead(model1, test_loader, class_vectors=text_features, return_confusion=True)\n",
    "acc2, confusion_disj_2 = evaluate_texthead(model2, test_loader, class_vectors=text_features, return_confusion=True)\n",
    "print(acc1, confusion_disj_1)\n",
    "print(acc2, confusion_disj_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b221586b",
   "metadata": {},
   "source": [
    "#### Weight Averaging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "id": "fd68dedf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4191 [0.797, 0.018, 0.791, 0.752, 0.86, 0.012, 0.843, 0.001, 0.012, 0.105]\n"
     ]
    }
   ],
   "source": [
    "state_dict = {}\n",
    "stoopid_dumb(state_dict, model1, model2)\n",
    "modelc = resnet20(w=4, text_head=True).to(DEVICE)\n",
    "modelc.load_state_dict(state_dict)\n",
    "reset_bn_stats(modelc)\n",
    "acc, confusion_stopid = evaluate_texthead(modelc, test_loader, class_vectors=text_features, return_confusion=True)\n",
    "print(acc, confusion_stopid)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e095a60",
   "metadata": {},
   "source": [
    "#### Model Ensembling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "id": "5df17f51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluates accuracy\n",
    "def evaluate_ensemble_texthead(model1, model2, loader, class_vectors, remap_class_idxs=None, return_confusion=False):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    confusion = np.zeros((10, 10))\n",
    "    \n",
    "    totals = [0] * class_vectors.shape[0]\n",
    "    corrects = [0] * class_vectors.shape[0]\n",
    "    \n",
    "    with torch.no_grad(), autocast():\n",
    "        for inputs, labels in loader:\n",
    "            encodings1 = model1(inputs.to(DEVICE))\n",
    "            encodings2 = model2(inputs.to(DEVICE))\n",
    "            normed_encodings1 = encodings1 / encodings1.norm(dim=-1, keepdim=True)\n",
    "            normed_encodings2 = encodings2 / encodings2.norm(dim=-1, keepdim=True)\n",
    "            outputs1 = normed_encodings1 @ class_vectors.T\n",
    "            outputs2 = normed_encodings2 @ class_vectors.T\n",
    "            pred, pred_prob1 = outputs1.argmax(dim=1), outputs1.max(dim=1)[0]\n",
    "            pred2, pred_prob2 = outputs2.argmax(dim=1), outputs2.max(dim=1)[0]\n",
    "            pred[pred_prob1 < pred_prob2] = pred2[pred_prob1 < pred_prob2]\n",
    "#             pred = outputs.argmax(dim=1)\n",
    "            if remap_class_idxs is not None:\n",
    "                correct += (remap_class_idxs[labels].to(DEVICE) == pred).sum().item()\n",
    "            else:\n",
    "                for gt, p in zip(labels, pred):\n",
    "                    totals[gt] += 1\n",
    "                    \n",
    "                    if gt == p:\n",
    "                        correct += 1\n",
    "                        corrects[gt] += 1\n",
    "                \n",
    "#                 correct += (labels.to(DEVICE) == pred).sum().item()\n",
    "                \n",
    "            confusion[labels.cpu().numpy(), pred.cpu().numpy()] += 1\n",
    "            total += inputs.shape[0]\n",
    "    if return_confusion:\n",
    "        return correct / sum(totals), list(map(lambda a: a[0] / a[1], zip(corrects, totals)))\n",
    "    else:\n",
    "        return correct / total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "id": "9ebea6da",
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_ensemble, confusion_ensemble = evaluate_ensemble_texthead(\n",
    "    model1, model2, test_loader, class_vectors=text_features, return_confusion=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "id": "a45c282e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.7362, [0.553, 0.97, 0.621, 0.309, 0.466, 0.965, 0.595, 0.96, 0.967, 0.956])"
      ]
     },
     "execution_count": 262,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc_ensemble, confusion_ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48a833c3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
