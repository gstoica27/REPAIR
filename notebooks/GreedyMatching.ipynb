{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2a5991d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/srv/share/gstoica3/miniconda3/envs/open-mmlab/lib/python3.7/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import scipy.optimize\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from torch.optim import SGD, lr_scheduler\n",
    "import torchvision\n",
    "import torchvision.transforms as T\n",
    "\n",
    "import pdb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "6a3c3c5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def save_model(model, i):\n",
    "    sd = model.state_dict()\n",
    "    torch.save(model.state_dict(), os.path.join('/srv/share4/gstoica3/checkpoints/REPAIR/', '%s.pt' % i))\n",
    "\n",
    "def load_model(model, i):\n",
    "    sd = torch.load(os.path.join('/srv/share4/gstoica3/checkpoints/REPAIR', '%s.pt' % i))\n",
    "    model.load_state_dict(sd)\n",
    "    \n",
    "def evaluate(model):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    with torch.no_grad(), autocast():\n",
    "        for inputs, labels in test_loader:\n",
    "            outputs = model(inputs.cuda())\n",
    "            pred = outputs.argmax(dim=1)\n",
    "            correct += (labels.cuda() == pred).sum().item()\n",
    "    return correct / len(test_loader.dataset)\n",
    "\n",
    "def train_model(w=1):\n",
    "    model = vgg11(w)\n",
    "    optimizer = SGD(model.parameters(), lr=0.08, momentum=0.9, weight_decay=5e-4)\n",
    "\n",
    "    EPOCHS = 100\n",
    "    ne_iters = len(train_aug_loader)\n",
    "    lr_schedule = np.interp(np.arange(1+EPOCHS*ne_iters), [0, 5*ne_iters, EPOCHS*ne_iters], [0, 1, 0])\n",
    "    scheduler = lr_scheduler.LambdaLR(optimizer, lr_schedule.__getitem__)\n",
    "\n",
    "    scaler = GradScaler()\n",
    "    loss_fn = CrossEntropyLoss()\n",
    "\n",
    "    losses = []\n",
    "    for epoch in tqdm(range(EPOCHS)):\n",
    "        model.train()\n",
    "        for i, (inputs, labels) in enumerate(train_aug_loader):\n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "            with autocast():\n",
    "                outputs = model(inputs.cuda())\n",
    "                loss = loss_fn(outputs, labels.cuda())\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            scheduler.step()\n",
    "            losses.append(loss.item())\n",
    "    return model\n",
    "\n",
    "# Given two networks net0, net1 which each output a feature map of shape NxCxWxH,\n",
    "# this will reshape both outputs to (N*W*H)xC\n",
    "# and then compute a CxC correlation matrix between the two\n",
    "def run_corr_matrix(net0, net1):\n",
    "    n = len(train_aug_loader)\n",
    "    with torch.no_grad():\n",
    "        net0.eval()\n",
    "        net1.eval()\n",
    "        for i, (images, _) in enumerate(tqdm(train_aug_loader)):\n",
    "            \n",
    "            img_t = images.float().cuda()\n",
    "            out0 = net0(img_t).double()\n",
    "            out0 = out0.permute(0, 2, 3, 1).reshape(-1, out0.shape[1])\n",
    "            out1 = net1(img_t).double()\n",
    "            out1 = out1.permute(0, 2, 3, 1).reshape(-1, out1.shape[1])\n",
    "\n",
    "            # save batchwise first+second moments and outer product\n",
    "            mean0_b = out0.mean(dim=0)\n",
    "            mean1_b = out1.mean(dim=0)\n",
    "            sqmean0_b = out0.square().mean(dim=0)\n",
    "            sqmean1_b = out1.square().mean(dim=0)\n",
    "            outer_b = (out0.T @ out1) / out0.shape[0]\n",
    "            if i == 0:\n",
    "                mean0 = torch.zeros_like(mean0_b)\n",
    "                mean1 = torch.zeros_like(mean1_b)\n",
    "                sqmean0 = torch.zeros_like(sqmean0_b)\n",
    "                sqmean1 = torch.zeros_like(sqmean1_b)\n",
    "                outer = torch.zeros_like(outer_b)\n",
    "            mean0 += mean0_b / n\n",
    "            mean1 += mean1_b / n\n",
    "            sqmean0 += sqmean0_b / n\n",
    "            sqmean1 += sqmean1_b / n\n",
    "            outer += outer_b / n\n",
    "\n",
    "    cov = outer - torch.outer(mean0, mean1)\n",
    "    std0 = (sqmean0 - mean0**2).sqrt()\n",
    "    std1 = (sqmean1 - mean1**2).sqrt()\n",
    "    corr = cov / (torch.outer(std0, std1) + 1e-4)\n",
    "    return corr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d977e17f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "CIFAR_MEAN = [125.307, 122.961, 113.8575]\n",
    "CIFAR_STD = [51.5865, 50.847, 51.255]\n",
    "normalize = T.Normalize(np.array(CIFAR_MEAN)/255, np.array(CIFAR_STD)/255)\n",
    "denormalize = T.Normalize(-np.array(CIFAR_MEAN)/np.array(CIFAR_STD), 255/np.array(CIFAR_STD))\n",
    "\n",
    "train_transform = T.Compose([\n",
    "    T.RandomHorizontalFlip(),\n",
    "    T.RandomCrop(32, padding=4),\n",
    "    T.ToTensor(),\n",
    "    normalize,\n",
    "])\n",
    "test_transform = T.Compose([\n",
    "    T.ToTensor(),\n",
    "    normalize,\n",
    "])\n",
    "train_dset = torchvision.datasets.CIFAR10(root='/tmp', train=True,\n",
    "                                        download=True, transform=train_transform)\n",
    "test_dset = torchvision.datasets.CIFAR10(root='/tmp', train=False,\n",
    "                                        download=True, transform=test_transform)\n",
    "\n",
    "train_aug_loader = torch.utils.data.DataLoader(train_dset, batch_size=500, shuffle=True, num_workers=8)\n",
    "test_loader = torch.utils.data.DataLoader(test_dset, batch_size=500, shuffle=False, num_workers=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0afc8ee4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/kuangliu/pytorch-cifar/blob/master/models/vgg.py\n",
    "cfg = {\n",
    "    'VGG11': [64, 'M', 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512, 'M'],\n",
    "    'VGG13': [64, 64, 'M', 128, 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512, 'M'],\n",
    "    'VGG16': [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 'M', 512, 512, 512, 'M', 512, 512, 512, 'M'],\n",
    "    'VGG19': [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 256, 'M', 512, 512, 512, 512, 'M', 512, 512, 512, 512, 'M'],\n",
    "}\n",
    "\n",
    "class VGG(nn.Module):\n",
    "    def __init__(self, vgg_name, w=1):\n",
    "        super(VGG, self).__init__()\n",
    "        self.w = w\n",
    "        self.features = self._make_layers(cfg[vgg_name])\n",
    "        self.classifier = nn.Linear(self.w*512, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.features(x)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.classifier(out)\n",
    "        return out\n",
    "\n",
    "    def _make_layers(self, cfg):\n",
    "        layers = []\n",
    "        in_channels = 3\n",
    "        for x in cfg:\n",
    "            if x == 'M':\n",
    "                layers += [nn.MaxPool2d(kernel_size=2, stride=2)]\n",
    "            else:\n",
    "                layers.append(nn.Conv2d(in_channels if in_channels == 3 else self.w*in_channels,\n",
    "                                     self.w*x, kernel_size=3, padding=1))\n",
    "                layers.append(nn.ReLU(inplace=True))\n",
    "                in_channels = x\n",
    "        layers += [nn.AvgPool2d(kernel_size=1, stride=1)]\n",
    "        return nn.Sequential(*layers)\n",
    "    \n",
    "def vgg11(w=1):\n",
    "    return VGG('VGG11', w).cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d2e2dd5",
   "metadata": {},
   "source": [
    "# Bipartite Matching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b6f89820",
   "metadata": {},
   "outputs": [],
   "source": [
    "sd0 = torch.load(os.path.join('/srv/share4/gstoica3/checkpoints/REPAIR', '%s.pt' % 'vgg11_v1'))\n",
    "sd1 = torch.load(os.path.join('/srv/share4/gstoica3/checkpoints/REPAIR', '%s.pt' % 'vgg11_v2_perm1'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "901edf16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# modifies the weight matrices of a convolution and batchnorm\n",
    "# layer given a permutation of the output channels\n",
    "def permute_output(perm_map, layer):\n",
    "    pre_weights = [layer.weight,\n",
    "                   layer.bias]\n",
    "    for w in pre_weights:\n",
    "#         pdb.set_trace()\n",
    "        if len(w.shape) == 4:\n",
    "            w.data = torch.einsum('ab,bcde->acde', perm_map, w)\n",
    "        else:\n",
    "            w.data = perm_map @ w\n",
    "#         w.data = w[perm_map]\n",
    "\n",
    "# modifies the weight matrix of a layer for a given permutation of the input channels\n",
    "# works for both conv2d and linear\n",
    "def permute_input(perm_map, layer):\n",
    "    w = layer.weight\n",
    "#     w.data = w[:, perm_map]\n",
    "    if len(w.shape) == 4:\n",
    "        w.data = torch.einsum('abcd,be->aecd', w, perm_map.T)\n",
    "    else:\n",
    "        w.data = w @ perm_map.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "e593ea23",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_layer_perm1(corr_mtx):\n",
    "    corr_mtx_a = corr_mtx.cpu().numpy()\n",
    "    row_ind, col_ind = scipy.optimize.linear_sum_assignment(corr_mtx_a, maximize=True)\n",
    "    assert (row_ind == np.arange(len(corr_mtx_a))).all()\n",
    "    perm_map = torch.tensor(col_ind).long()\n",
    "    perm_map = torch.eye(corr_mtx.shape[0], device=corr_mtx.device)[perm_map]\n",
    "    return perm_map\n",
    "\n",
    "def bipartite_matching(corr):\n",
    "    idx = corr.argmax(0)\n",
    "    mapping = torch.eye(corr.shape[0], device=corr.device)[idx]\n",
    "    mapping = mapping / torch.maximum(\n",
    "        mapping.sum(0, keepdim=True), \n",
    "        torch.ones_like(mapping[0], device=mapping.device)\n",
    "    )\n",
    "    pdb.set_trace()\n",
    "    return mapping\n",
    "\n",
    "# returns the channel-permutation to make layer1's activations most closely\n",
    "# match layer0's.\n",
    "def get_layer_perm(net0, net1):\n",
    "    corr_mtx = run_corr_matrix(net0, net1)\n",
    "    return get_layer_perm1(corr_mtx)\n",
    "#     return bipartite_matching(corr_mtx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "0a1bf96f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def greedy_matching(alignments, threshold=0.5):\n",
    "    matrix_copy = alignments.detach().cpu().numpy()\n",
    "    matches = np.zeros((matrix_copy.shape[1], 2), dtype=np.float32)\n",
    "    total_matches = 0\n",
    "    while total_matches < matches.shape[0]:\n",
    "        best_alignments, best_idxs = matrix_copy.max(0), matrix_copy.argmax(0)\n",
    "        best_source = best_alignments.argmax()\n",
    "        matches[best_source, 0] = best_idxs[best_source]\n",
    "        matches[best_source, 1] = best_alignments[best_source]\n",
    "        matrix_copy[best_idxs[best_source]] = -np.inf\n",
    "        matrix_copy[:, best_source] = -np.inf\n",
    "        total_matches += 1\n",
    "    matches = torch.from_numpy(matches).to(alignments.device)\n",
    "    permutations = torch.eye(\n",
    "        alignments.shape[0], \n",
    "        device=alignments.device\n",
    "    )[matches[:, 0].type(torch.long)]\n",
    "#     pdb.set_trace()\n",
    "    permutations[matches[:, 1] < threshold] = torch.zeros(\n",
    "        permutations.shape[1], device=permutations.device\n",
    "    )\n",
    "#     print(permutations.sum(), permutations.shape[0])\n",
    "#     pdb.set_trace()\n",
    "    return permutations\n",
    "\n",
    "# returns the channel-permutation to make layer1's activations most closely\n",
    "# match layer0's.\n",
    "def get_layer_perm(net0, net1):\n",
    "    corr_mtx = run_corr_matrix(net0, net1)\n",
    "#     return get_layer_perm1(corr_mtx)\n",
    "    return greedy_matching(corr_mtx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "a2e5664f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.8982, 0.8984)"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model0 = vgg11()\n",
    "model1 = vgg11()\n",
    "load_model(model0, 'vgg11_v1')\n",
    "load_model(model1, 'vgg11_v2')\n",
    "\n",
    "evaluate(model0), evaluate(model1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "855cae02",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [00:04<00:00, 23.99it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [00:04<00:00, 22.61it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [00:04<00:00, 24.43it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [00:04<00:00, 23.22it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [00:04<00:00, 24.33it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [00:04<00:00, 23.57it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [00:03<00:00, 25.65it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [00:03<00:00, 25.22it/s]\n"
     ]
    }
   ],
   "source": [
    "def subnet(model, n_layers):\n",
    "    return model.features[:n_layers]\n",
    "\n",
    "feats1 = model1.features\n",
    "\n",
    "n = len(feats1)\n",
    "for i in range(n):\n",
    "    if not isinstance(feats1[i], nn.Conv2d):\n",
    "        continue\n",
    "    \n",
    "    # permute the outputs of the current conv layer\n",
    "    assert isinstance(feats1[i+1], nn.ReLU)\n",
    "    perm_map = get_layer_perm(subnet(model0, i+2), subnet(model1, i+2))\n",
    "    permute_output(perm_map, feats1[i])\n",
    "    \n",
    "    # look for the next conv layer, whose inputs should be permuted the same way\n",
    "    next_layer = None\n",
    "    for j in range(i+1, n):\n",
    "        if isinstance(feats1[j], nn.Conv2d):\n",
    "            next_layer = feats1[j]\n",
    "            break\n",
    "    if next_layer is None:\n",
    "        next_layer = model1.classifier\n",
    "    permute_input(perm_map, next_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "15571464",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1\n"
     ]
    }
   ],
   "source": [
    "# ensure accuracy didn't change\n",
    "# (it may be slightly different due to non-associativity of floating point arithmetic)\n",
    "print(evaluate(model1))\n",
    "save_model(model1, 'vgg11_v2_greedyprune.5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "de4a0602",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mix_weights(net, alpha, key0, key1):\n",
    "    sd0 = torch.load(os.path.join('/srv/share4/gstoica3/checkpoints/REPAIR', '%s.pt' % key0))\n",
    "    sd1 = torch.load(os.path.join('/srv/share4/gstoica3/checkpoints/REPAIR', '%s.pt' % key1))\n",
    "    sd_alpha = {}\n",
    "    for k in sd0.keys():\n",
    "        v0 = sd0[k].cuda()\n",
    "        v1 = sd1[k].cuda()\n",
    "        if '0' not in k: \n",
    "            sd_alpha[k] = v0\n",
    "            continue\n",
    "        pdb.set_trace()\n",
    "        sd_alpha[k] = (1 - alpha) * v0 + alpha * v1\n",
    "#     sd_alpha = {k: (1 - alpha) * sd0[k].cuda() + alpha * sd1[k].cuda()\n",
    "#                 for k in sd0.keys()}\n",
    "    net.load_state_dict(sd_alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "6e91fc2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> \u001b[0;32m/tmp/ipykernel_9942/1373196417.py\u001b[0m(12)\u001b[0;36mmix_weights\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m     10 \u001b[0;31m            \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     11 \u001b[0;31m        \u001b[0mpdb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_trace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m---> 12 \u001b[0;31m        \u001b[0msd_alpha\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mv0\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0malpha\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mv1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     13 \u001b[0;31m\u001b[0;31m#     sd_alpha = {k: (1 - alpha) * sd0[k].cuda() + alpha * sd1[k].cuda()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     14 \u001b[0;31m\u001b[0;31m#                 for k in sd0.keys()}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n",
      "ipdb> v1.shape\n",
      "torch.Size([64, 3, 3, 3])\n",
      "ipdb> v1[:, 0, 0, 0]\n",
      "tensor([ 0.0000,  0.0000,  0.0000,  0.0000,  0.0814,  0.0000, -0.1377, -0.0065,\n",
      "         0.1004, -0.1905, -0.0904, -0.2233,  0.0978, -0.2477, -0.3859, -0.1544,\n",
      "         0.1011,  0.0000,  0.0000, -0.0134,  0.0000, -0.0424, -0.0957,  0.0000,\n",
      "         0.1267,  0.0801,  0.0000,  0.2017,  0.1152,  0.0000,  0.0117,  0.0000,\n",
      "         0.1143,  0.4771,  0.1286, -0.0261, -0.1871, -0.0419,  0.0000,  0.0000,\n",
      "         0.0000,  0.0251,  0.2194,  0.0000,  0.0000, -0.1056,  0.0000, -0.1234,\n",
      "         0.0674, -0.0922, -0.1908,  0.0978,  0.0000,  0.1945,  0.1618,  0.0000,\n",
      "        -0.1397,  0.0457,  0.0000,  0.0000,  0.0700,  0.2055,  0.0000, -0.0234],\n",
      "       device='cuda:0')\n",
      "ipdb> v1.flatten(1)\n",
      "tensor([[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        ...,\n",
      "        [ 0.2055, -0.0709, -0.0132,  ..., -0.2311,  0.1621,  0.1893],\n",
      "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.0234, -0.0100, -0.0944,  ..., -0.0739, -0.0442, -0.1846]],\n",
      "       device='cuda:0')\n",
      "ipdb> dots = v0.flatten(1) @ v1.flatten(1).T\n",
      "ipdb> v0_norm = torch.norm(v0.flatten(1), dim=-1, p=2)\n",
      "ipdb> v1_norm = torch.norm(v1.flatten(1), dim=-1, p=2)\n",
      "ipdb> norm_muls = v0_norm * v1_norm\n",
      "ipdb> dots / norm_muls\n",
      "tensor([[    nan,     nan,     nan,  ..., -0.2116,     nan,  0.2866],\n",
      "        [    nan,     nan,     nan,  ..., -0.1211,     nan, -0.4463],\n",
      "        [    nan,     nan,     nan,  ..., -0.1606,     nan, -0.8921],\n",
      "        ...,\n",
      "        [    nan,     nan,     nan,  ...,  0.3539,     nan,  0.2609],\n",
      "        [    nan,     nan,     nan,  ..., -0.0833,     nan, -1.0276],\n",
      "        [    nan,     nan,     nan,  ...,  0.0970,     nan, -0.4371]],\n",
      "       device='cuda:0')\n",
      "ipdb> torch.diag(dots / norm_muls)\n",
      "tensor([    nan,     nan,     nan,     nan,  0.0246,     nan,  0.0120,  0.2324,\n",
      "         0.0444, -0.1641, -0.0799,  0.3742, -0.2639, -0.0283, -0.2627,  0.1193,\n",
      "        -0.0452,     nan,     nan, -0.3643,     nan, -0.3622,  0.0801,     nan,\n",
      "        -0.1149, -0.1507,     nan,  0.3832,  0.0798,     nan, -0.5492,     nan,\n",
      "         0.3522,  0.0359, -0.0935,  0.1723,  0.8307, -0.1586,     nan,     nan,\n",
      "            nan, -0.0797,  0.2176,     nan,     nan,  0.0401,     nan,  0.2996,\n",
      "         0.6784,  0.0164, -0.1071, -0.4300,     nan,  0.3663,  0.1033,     nan,\n",
      "         0.4336, -0.4222,     nan,     nan, -0.1027,  0.3539,     nan, -0.4371],\n",
      "       device='cuda:0')\n",
      "ipdb> q\n"
     ]
    }
   ],
   "source": [
    "k0 = 'vgg11_v1'\n",
    "k1 = 'vgg11_v2_greedyprune.5'\n",
    "model0 = vgg11()\n",
    "model1 = vgg11()\n",
    "model_a = vgg11()\n",
    "mix_weights(model0, 0.0, k0, k1)\n",
    "mix_weights(model1, 1.0, k0, k1)\n",
    "\n",
    "alpha = 0.5\n",
    "mix_weights(model_a, alpha, k0, k1)\n",
    "print('(α=0): %.1f%% \\t\\t<-- Model A' % (100*evaluate(model0)))\n",
    "print('(α=1): %.1f%% \\t\\t<-- Model B' % (100*evaluate(model1)))\n",
    "print('(α=0.5): %.1f%% \\t\\t<-- Merged model' % (100*evaluate(model_a)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f32b3c49",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
