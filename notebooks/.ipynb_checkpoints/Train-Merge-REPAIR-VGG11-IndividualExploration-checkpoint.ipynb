{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9021242c",
   "metadata": {},
   "source": [
    "# Train-Merge-REPAIR-VGG11\n",
    "\n",
    "This is a minimal notebook which demonstrates REPAIR applied to a VGG11 network. It does the following:\n",
    "1. Separately trains two VGG11 networks on CIFAR-10, \"A\" and \"B\".\n",
    "2. Permutes the channels of each convolutional layer in \"B\" in order to align them with \"A\".\n",
    "3. Merges the two models in weight-space. The merged model performs poorly.\n",
    "4. Uses REPAIR to correct the neuronal statistics of the merged model.\n",
    "\n",
    "Notes:\n",
    "* The merged VGG network should initially attain 67.3% (+/-5%) accuracy. After REPAIR, it should reach 84.9% (+/-1%).\n",
    "* The trained networks should obtain 89-91% accuracy.\n",
    "* We use the original VGG architecture which does not contain normalization layers.\n",
    "* To align channels, we maximize correlations between the activations of matched neurons; this method is due to Li et al. (2015) https://arxiv.org/abs/1511.07543\n",
    "* REPAIR is a generalization of the method of resetting BatchNorms, which goes back to SWA https://arxiv.org/abs/1803.05407. We introduce it in https://arxiv.org/abs/2211.08403."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c0db779d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/srv/share/gstoica3/miniconda3/envs/open-mmlab/lib/python3.7/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import scipy.optimize\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from torch.optim import SGD, lr_scheduler\n",
    "import torchvision\n",
    "import torchvision.transforms as T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdf1a55e",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c477d1e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def save_model(model, i):\n",
    "    sd = model.state_dict()\n",
    "    torch.save(model.state_dict(), os.path.join('/srv/share4/gstoica3/checkpoints/REPAIR/', '%s.pt' % i))\n",
    "\n",
    "def load_model(model, i):\n",
    "    sd = torch.load(os.path.join('/srv/share4/gstoica3/checkpoints/REPAIR', '%s.pt' % i))\n",
    "    model.load_state_dict(sd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fabbc157",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "CIFAR_MEAN = [125.307, 122.961, 113.8575]\n",
    "CIFAR_STD = [51.5865, 50.847, 51.255]\n",
    "normalize = T.Normalize(np.array(CIFAR_MEAN)/255, np.array(CIFAR_STD)/255)\n",
    "denormalize = T.Normalize(-np.array(CIFAR_MEAN)/np.array(CIFAR_STD), 255/np.array(CIFAR_STD))\n",
    "\n",
    "train_transform = T.Compose([\n",
    "    T.RandomHorizontalFlip(),\n",
    "    T.RandomCrop(32, padding=4),\n",
    "    T.ToTensor(),\n",
    "    normalize,\n",
    "])\n",
    "test_transform = T.Compose([\n",
    "    T.ToTensor(),\n",
    "    normalize,\n",
    "])\n",
    "train_dset = torchvision.datasets.CIFAR10(root='/tmp', train=True,\n",
    "                                        download=True, transform=train_transform)\n",
    "test_dset = torchvision.datasets.CIFAR10(root='/tmp', train=False,\n",
    "                                        download=True, transform=test_transform)\n",
    "\n",
    "train_aug_loader = torch.utils.data.DataLoader(train_dset, batch_size=500, shuffle=True, num_workers=8)\n",
    "test_loader = torch.utils.data.DataLoader(test_dset, batch_size=500, shuffle=False, num_workers=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "31e22780",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    with torch.no_grad(), autocast():\n",
    "        for inputs, labels in test_loader:\n",
    "            outputs = model(inputs.cuda())\n",
    "            pred = outputs.argmax(dim=1)\n",
    "            correct += (labels.cuda() == pred).sum().item()\n",
    "    return correct / len(test_loader.dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e42cdbf3",
   "metadata": {},
   "source": [
    "# Train two VGG11 networks on CIFAR-10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "51b6c738",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/kuangliu/pytorch-cifar/blob/master/models/vgg.py\n",
    "cfg = {\n",
    "    'VGG11': [64, 'M', 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512, 'M'],\n",
    "    'VGG13': [64, 64, 'M', 128, 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512, 'M'],\n",
    "    'VGG16': [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 'M', 512, 512, 512, 'M', 512, 512, 512, 'M'],\n",
    "    'VGG19': [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 256, 'M', 512, 512, 512, 512, 'M', 512, 512, 512, 512, 'M'],\n",
    "}\n",
    "\n",
    "class VGG(nn.Module):\n",
    "    def __init__(self, vgg_name, w=1):\n",
    "        super(VGG, self).__init__()\n",
    "        self.w = w\n",
    "        self.features = self._make_layers(cfg[vgg_name])\n",
    "        self.classifier = nn.Linear(self.w*512, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.features(x)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.classifier(out)\n",
    "        return out\n",
    "\n",
    "    def _make_layers(self, cfg):\n",
    "        layers = []\n",
    "        in_channels = 3\n",
    "        for x in cfg:\n",
    "            if x == 'M':\n",
    "                layers += [nn.MaxPool2d(kernel_size=2, stride=2)]\n",
    "            else:\n",
    "                layers.append(nn.Conv2d(in_channels if in_channels == 3 else self.w*in_channels,\n",
    "                                     self.w*x, kernel_size=3, padding=1))\n",
    "                layers.append(nn.ReLU(inplace=True))\n",
    "                in_channels = x\n",
    "        layers += [nn.AvgPool2d(kernel_size=1, stride=1)]\n",
    "        return nn.Sequential(*layers)\n",
    "    \n",
    "def vgg11(w=1):\n",
    "    return VGG('VGG11', w).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d120e6b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(w=1):\n",
    "    model = vgg11(w)\n",
    "    optimizer = SGD(model.parameters(), lr=0.08, momentum=0.9, weight_decay=5e-4)\n",
    "\n",
    "    EPOCHS = 100\n",
    "    ne_iters = len(train_aug_loader)\n",
    "    lr_schedule = np.interp(np.arange(1+EPOCHS*ne_iters), [0, 5*ne_iters, EPOCHS*ne_iters], [0, 1, 0])\n",
    "    scheduler = lr_scheduler.LambdaLR(optimizer, lr_schedule.__getitem__)\n",
    "\n",
    "    scaler = GradScaler()\n",
    "    loss_fn = CrossEntropyLoss()\n",
    "\n",
    "    losses = []\n",
    "    for epoch in tqdm(range(EPOCHS)):\n",
    "        model.train()\n",
    "        for i, (inputs, labels) in enumerate(train_aug_loader):\n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "            with autocast():\n",
    "                outputs = model(inputs.cuda())\n",
    "                loss = loss_fn(outputs, labels.cuda())\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            scheduler.step()\n",
    "            losses.append(loss.item())\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "08a4ae3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████| 100/100 [08:02<00:00,  4.82s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.895\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████| 100/100 [07:51<00:00,  4.72s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8979\n"
     ]
    }
   ],
   "source": [
    "model = train_model()\n",
    "print(evaluate(model))\n",
    "save_model(model, 'vgg11_v1')\n",
    "\n",
    "model = train_model()\n",
    "print(evaluate(model))\n",
    "save_model(model, 'vgg11_v2')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7114582",
   "metadata": {},
   "source": [
    "# Permute the channels of model B to align with model A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "363e68ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Given two networks net0, net1 which each output a feature map of shape NxCxWxH,\n",
    "# this will reshape both outputs to (N*W*H)xC\n",
    "# and then compute a CxC correlation matrix between the two\n",
    "def run_corr_matrix(net0, net1):\n",
    "    n = len(train_aug_loader)\n",
    "    with torch.no_grad():\n",
    "        net0.eval()\n",
    "        net1.eval()\n",
    "        for i, (images, _) in enumerate(tqdm(train_aug_loader)):\n",
    "            \n",
    "            img_t = images.float().cuda()\n",
    "            out0 = net0(img_t).double()\n",
    "            out0 = out0.permute(0, 2, 3, 1).reshape(-1, out0.shape[1])\n",
    "            out1 = net1(img_t).double()\n",
    "            out1 = out1.permute(0, 2, 3, 1).reshape(-1, out1.shape[1])\n",
    "\n",
    "            # save batchwise first+second moments and outer product\n",
    "            mean0_b = out0.mean(dim=0)\n",
    "            mean1_b = out1.mean(dim=0)\n",
    "            sqmean0_b = out0.square().mean(dim=0)\n",
    "            sqmean1_b = out1.square().mean(dim=0)\n",
    "            outer_b = (out0.T @ out1) / out0.shape[0]\n",
    "            if i == 0:\n",
    "                mean0 = torch.zeros_like(mean0_b)\n",
    "                mean1 = torch.zeros_like(mean1_b)\n",
    "                sqmean0 = torch.zeros_like(sqmean0_b)\n",
    "                sqmean1 = torch.zeros_like(sqmean1_b)\n",
    "                outer = torch.zeros_like(outer_b)\n",
    "            mean0 += mean0_b / n\n",
    "            mean1 += mean1_b / n\n",
    "            sqmean0 += sqmean0_b / n\n",
    "            sqmean1 += sqmean1_b / n\n",
    "            outer += outer_b / n\n",
    "\n",
    "    cov = outer - torch.outer(mean0, mean1)\n",
    "    std0 = (sqmean0 - mean0**2).sqrt()\n",
    "    std1 = (sqmean1 - mean1**2).sqrt()\n",
    "    corr = cov / (torch.outer(std0, std1) + 1e-4)\n",
    "    return corr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1f9ef263",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_layer_perm1(corr_mtx):\n",
    "    corr_mtx_a = corr_mtx.cpu().numpy()\n",
    "    row_ind, col_ind = scipy.optimize.linear_sum_assignment(corr_mtx_a, maximize=True)\n",
    "    assert (row_ind == np.arange(len(corr_mtx_a))).all()\n",
    "    perm_map = torch.tensor(col_ind).long()\n",
    "    return perm_map\n",
    "\n",
    "# returns the channel-permutation to make layer1's activations most closely\n",
    "# match layer0's.\n",
    "def get_layer_perm(net0, net1):\n",
    "    corr_mtx = run_corr_matrix(net0, net1)\n",
    "    return get_layer_perm1(corr_mtx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f1aae9be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# modifies the weight matrices of a convolution and batchnorm\n",
    "# layer given a permutation of the output channels\n",
    "def permute_output(perm_map, layer):\n",
    "    pre_weights = [layer.weight,\n",
    "                   layer.bias]\n",
    "    for w in pre_weights:\n",
    "        w.data = w[perm_map]\n",
    "\n",
    "# modifies the weight matrix of a layer for a given permutation of the input channels\n",
    "# works for both conv2d and linear\n",
    "def permute_input(perm_map, layer):\n",
    "    w = layer.weight\n",
    "    w.data = w[:, perm_map]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c0db278e",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.895, 0.8979)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model0 = vgg11()\n",
    "model1 = vgg11()\n",
    "load_model(model0, 'vgg11_v1')\n",
    "load_model(model1, 'vgg11_v2')\n",
    "\n",
    "evaluate(model0), evaluate(model1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8157aa0c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b3f2f3bb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████| 100/100 [00:04<00:00, 21.92it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████| 100/100 [00:04<00:00, 21.35it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████| 100/100 [00:04<00:00, 20.64it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████| 100/100 [00:04<00:00, 21.13it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████| 100/100 [00:04<00:00, 20.89it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████| 100/100 [00:04<00:00, 20.51it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████| 100/100 [00:04<00:00, 21.67it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████| 100/100 [00:04<00:00, 22.23it/s]\n"
     ]
    }
   ],
   "source": [
    "def subnet(model, n_layers):\n",
    "    return model.features[:n_layers]\n",
    "\n",
    "feats1 = model1.features\n",
    "\n",
    "n = len(feats1)\n",
    "for i in range(n):\n",
    "    if not isinstance(feats1[i], nn.Conv2d):\n",
    "        continue\n",
    "    \n",
    "    # permute the outputs of the current conv layer\n",
    "    assert isinstance(feats1[i+1], nn.ReLU)\n",
    "    perm_map = get_layer_perm(subnet(model0, i+2), subnet(model1, i+2))\n",
    "    permute_output(perm_map, feats1[i])\n",
    "    \n",
    "    # look for the next conv layer, whose inputs should be permuted the same way\n",
    "    next_layer = None\n",
    "    for j in range(i+1, n):\n",
    "        if isinstance(feats1[j], nn.Conv2d):\n",
    "            next_layer = feats1[j]\n",
    "            break\n",
    "    if next_layer is None:\n",
    "        next_layer = model1.classifier\n",
    "    permute_input(perm_map, next_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3112ee07",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8979\n"
     ]
    }
   ],
   "source": [
    "# ensure accuracy didn't change\n",
    "# (it may be slightly different due to non-associativity of floating point arithmetic)\n",
    "print(evaluate(model1))\n",
    "save_model(model1, 'vgg11_v2_perm1')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc494077",
   "metadata": {},
   "source": [
    "# Merge the two networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1a7ae6a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mix_weights(net, alpha, key0, key1):\n",
    "    sd0 = torch.load(os.path.join('/srv/share4/gstoica3/checkpoints/REPAIR', '%s.pt' % key0))\n",
    "    sd1 = torch.load(os.path.join('/srv/share4/gstoica3/checkpoints/REPAIR', '%s.pt' % key1))\n",
    "    sd_alpha = {k: (1 - alpha) * sd0[k].cuda() + alpha * sd1[k].cuda()\n",
    "                for k in sd0.keys()}\n",
    "    net.load_state_dict(sd_alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "00b6b3e5",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(α=0): 89.5% \t\t<-- Model A\n",
      "(α=1): 89.8% \t\t<-- Model B\n",
      "(α=0.5): 72.9% \t\t<-- Merged model\n"
     ]
    }
   ],
   "source": [
    "k0 = 'vgg11_v1'\n",
    "k1 = 'vgg11_v2_perm1'\n",
    "model0 = vgg11()\n",
    "model1 = vgg11()\n",
    "model_a = vgg11()\n",
    "mix_weights(model0, 0.0, k0, k1)\n",
    "mix_weights(model1, 1.0, k0, k1)\n",
    "\n",
    "alpha = 0.5\n",
    "mix_weights(model_a, alpha, k0, k1)\n",
    "print('(α=0): %.1f%% \\t\\t<-- Model A' % (100*evaluate(model0)))\n",
    "print('(α=1): %.1f%% \\t\\t<-- Model B' % (100*evaluate(model1)))\n",
    "print('(α=0.5): %.1f%% \\t\\t<-- Merged model' % (100*evaluate(model_a)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d94668f0",
   "metadata": {},
   "source": [
    "# Merge the two networks via Procrustes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b064faf4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.895, 0.8979)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model0 = vgg11()\n",
    "model1 = vgg11()\n",
    "load_model(model0, 'vgg11_v1')\n",
    "load_model(model1, 'vgg11_v2')\n",
    "\n",
    "evaluate(model0), evaluate(model1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "aba2ccb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_procrustes(corr_mtx):\n",
    "    U, _, Vh = torch.linalg.svd(corr_mtx)\n",
    "    return U @ Vh\n",
    "\n",
    "# returns the channel-permutation to make layer1's activations most closely\n",
    "# match layer0's.\n",
    "def get_layer_procrustes(net0, net1):\n",
    "    corr_mtx = run_corr_matrix(net0, net1)\n",
    "    return get_procrustes(corr_mtx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "4b4d0087",
   "metadata": {},
   "outputs": [],
   "source": [
    "# modifies the weight matrices of a convolution and batchnorm\n",
    "# layer given a permutation of the output channels\n",
    "def procrustes_output(perm_map, layer):\n",
    "    pre_weights = [layer.weight,\n",
    "                   layer.bias]\n",
    "    for w in pre_weights:\n",
    "        if len(w.shape) == 4:\n",
    "            w.data = torch.einsum('ab,bcde->acde', perm_map.to(torch.float32), w)\n",
    "        else:\n",
    "            w.data = perm_map.to(torch.float32) @ w\n",
    "\n",
    "# modifies the weight matrix of a layer for a given permutation of the input channels\n",
    "# works for both conv2d and linear\n",
    "def procrustes_input(perm_map, layer):\n",
    "    w = layer.weight\n",
    "    if len(w.shape) == 4:\n",
    "        w.data = torch.einsum('abcd,be->aecd', w, perm_map.t().to(torch.float32)) # w @ perm_map.t()\n",
    "    else:\n",
    "        w.data = w @ perm_map.t().to(torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "897f5b6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████| 100/100 [00:04<00:00, 22.74it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████| 100/100 [00:04<00:00, 21.53it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████| 100/100 [00:04<00:00, 21.86it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████| 100/100 [00:04<00:00, 21.60it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████| 100/100 [00:04<00:00, 20.63it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████| 100/100 [00:04<00:00, 22.24it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████| 100/100 [00:04<00:00, 21.68it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████| 100/100 [00:04<00:00, 22.33it/s]\n"
     ]
    }
   ],
   "source": [
    "feats1 = model1.features\n",
    "\n",
    "n = len(feats1)\n",
    "for i in range(n):\n",
    "    if not isinstance(feats1[i], nn.Conv2d):\n",
    "        continue\n",
    "    \n",
    "    # permute the outputs of the current conv layer\n",
    "    assert isinstance(feats1[i+1], nn.ReLU)\n",
    "    procrustes_map = get_layer_procrustes(subnet(model0, i+2), subnet(model1, i+2))\n",
    "    procrustes_output(procrustes_map, feats1[i])\n",
    "    \n",
    "    # look for the next conv layer, whose inputs should be permuted the same way\n",
    "    next_layer = None\n",
    "    for j in range(i+1, n):\n",
    "        if isinstance(feats1[j], nn.Conv2d):\n",
    "            next_layer = feats1[j]\n",
    "            break\n",
    "    if next_layer is None:\n",
    "        next_layer = model1.classifier\n",
    "    procrustes_input(procrustes_map, next_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "c48ed894",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1\n"
     ]
    }
   ],
   "source": [
    "# ensure accuracy didn't change\n",
    "# (it may be slightly different due to non-associativity of floating point arithmetic)\n",
    "print(evaluate(model1))\n",
    "save_model(model1, 'vgg11_v2_procrustes1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "5f8dc917",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(α=0): 89.5% \t\t<-- Model A\n",
      "(α=1): 10.0% \t\t<-- Model B\n",
      "(α=0.5): 22.5% \t\t<-- Merged model\n"
     ]
    }
   ],
   "source": [
    "k0 = 'vgg11_v1'\n",
    "k1 = 'vgg11_v2_procrustes1'\n",
    "model0 = vgg11()\n",
    "model1 = vgg11()\n",
    "model_a = vgg11()\n",
    "mix_weights(model0, 0.0, k0, k1)\n",
    "mix_weights(model1, 1.0, k0, k1)\n",
    "\n",
    "alpha = 0.5\n",
    "mix_weights(model_a, alpha, k0, k1)\n",
    "print('(α=0): %.1f%% \\t\\t<-- Model A' % (100*evaluate(model0)))\n",
    "print('(α=1): %.1f%% \\t\\t<-- Model B' % (100*evaluate(model1)))\n",
    "print('(α=0.5): %.1f%% \\t\\t<-- Merged model' % (100*evaluate(model_a)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "491f13ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "feats1 = model1.features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9676a6f7",
   "metadata": {},
   "source": [
    "# The Fails. Why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "263fc3c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.895, 0.8979)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model0 = vgg11()\n",
    "model1 = vgg11()\n",
    "load_model(model0, 'vgg11_v1')\n",
    "load_model(model1, 'vgg11_v2')\n",
    "\n",
    "evaluate(model0), evaluate(model1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "0cb4262a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████| 100/100 [00:04<00:00, 22.33it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████| 100/100 [00:04<00:00, 21.55it/s]\n"
     ]
    }
   ],
   "source": [
    "feats1 = model1.features\n",
    "\n",
    "n = len(feats1)\n",
    "for i in range(n):\n",
    "    if not isinstance(feats1[i], nn.Conv2d):\n",
    "        continue\n",
    "    \n",
    "    # permute the outputs of the current conv layer\n",
    "    assert isinstance(feats1[i+1], nn.ReLU)\n",
    "    perm_map = get_layer_perm(subnet(model0, i+2), subnet(model1, i+2))\n",
    "    procrustes_map = get_layer_procrustes(subnet(model0, i+2), subnet(model1, i+2))\n",
    "    break\n",
    "#     permute_output(perm_map, feats1[i])\n",
    "    \n",
    "#     # look for the next conv layer, whose inputs should be permuted the same way\n",
    "#     next_layer = None\n",
    "#     for j in range(i+1, n):\n",
    "#         if isinstance(feats1[j], nn.Conv2d):\n",
    "#             next_layer = feats1[j]\n",
    "#             break\n",
    "#     if next_layer is None:\n",
    "#         next_layer = model1.classifier\n",
    "#     permute_input(perm_map, next_layer)\n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "64b4b223",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 8, 24,  6, 34, 22, 55, 14,  0,  1,  9, 26, 40, 23, 58, 11, 30, 63, 41,\n",
       "        45, 48, 43,  7,  5, 50, 13, 31, 42, 60,  2, 62, 39, 20, 18, 16, 10, 27,\n",
       "        54, 28, 21, 33, 15, 19, 29, 61, 59, 17, 47, 57, 35, 52, 46,  3, 49, 51,\n",
       "        36, 12, 37, 32, 53, 56, 44,  4, 25, 38])"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "perm_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "f9579a38",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 8, 24,  6, 34, 22, 60, 14,  0,  4,  9, 26, 40,  7, 58, 11, 30, 63, 41,\n",
       "        45, 48, 43, 13, 62, 50,  1, 31, 27, 43, 41, 62,  2, 20, 22, 16, 10, 27,\n",
       "        54, 28, 48, 33, 15, 19, 29, 61, 59, 17, 47, 37, 26, 18, 46,  2, 34, 51,\n",
       "        27, 12,  0, 32, 53,  9, 44, 20, 25, 38], device='cuda:0')"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "procrustes_map.argmax(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "621115d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "w = feats1[i].weight\n",
    "b = feats1[i].bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "447cebea",
   "metadata": {},
   "outputs": [],
   "source": [
    "w_out_permuted = w[perm_map]\n",
    "b_out_permuted = b[perm_map]\n",
    "\n",
    "w_out_procrustes = torch.einsum('ab,bcde->acde', procrustes_map.to(torch.float32), w)\n",
    "b_out_procrustes = procrustes_map.to(torch.float32) @ b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "5b385178",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(5.8678, device='cuda:0', grad_fn=<LinalgVectorNormBackward0>)"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.linalg.norm(w_out_permuted - w_out_procrustes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "d6bf9e8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(3.8173, device='cuda:0', grad_fn=<LinalgVectorNormBackward0>)\n",
      "tensor(6.2049, device='cuda:0', grad_fn=<LinalgVectorNormBackward0>)\n"
     ]
    }
   ],
   "source": [
    "feats0 = model0.features\n",
    "print(torch.linalg.norm(feats0[i].weight - w_out_procrustes))\n",
    "print(torch.linalg.norm(feats0[i].weight - w_out_permuted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "329fee38",
   "metadata": {},
   "outputs": [],
   "source": [
    "0-\\\\"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32512df0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1e7a5d9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af60efb0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeacff94",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cc9f0f1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57f51e76",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "843a0a41",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3b837bf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "689199fd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59ffbc59",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a3a1407",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bff6de2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca28306d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b66c65d0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ccab462",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36c86b85",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b52bb23a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "06e6d0cc",
   "metadata": {},
   "source": [
    "### Merge the two networks by pruning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb144bb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "sd0 = torch.load(os.path.join('/srv/share4/gstoica3/checkpoints/REPAIR', '%s.pt' % 'vgg11_v1'))\n",
    "sd1 = torch.load(os.path.join('/srv/share4/gstoica3/checkpoints/REPAIR', '%s.pt' % 'vgg11_v2_perm1'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d3424df",
   "metadata": {},
   "outputs": [],
   "source": [
    "features.6.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0148d423",
   "metadata": {},
   "outputs": [],
   "source": [
    "param_shape = sd1[key].shape\n",
    "param0 = sd1[key].flatten(1)\n",
    "param1 = sd1[key].flatten(1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "88ead617",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mix_weights_pruning(net, alpha, key0, key1):\n",
    "    d = {}\n",
    "    sd0 = torch.load(os.path.join('/srv/share4/gstoica3/checkpoints/REPAIR', '%s.pt' % key0))\n",
    "    sd1 = torch.load(os.path.join('/srv/share4/gstoica3/checkpoints/REPAIR', '%s.pt' % key1))\n",
    "    sd_alpha = {}\n",
    "    for key in sd0.keys():\n",
    "        if 'weight' not in key:\n",
    "            sd_alpha[key] = (1 - alpha) * sd0[key].cuda() + alpha * sd1[key].cuda()\n",
    "            continue\n",
    "        param_shape = sd1[key].shape\n",
    "        param0 = sd0[key].flatten(1)\n",
    "        param1 = sd1[key].flatten(1)\n",
    "        \n",
    "        cosine_num = torch.diagonal(param0 @ param1.T)\n",
    "        cosine_denom = (\n",
    "            torch.norm(param0, p=2,dim=-1) * torch.norm(param1, p=2,dim=-1)\n",
    "        )\n",
    "        cosine_alignments = (cosine_num / cosine_denom).reshape(-1)\n",
    "#         import pdb; pdb.set_trace()\n",
    "        sorted_alignments, sorted_args = cosine_alignments.sort()\n",
    "        outlier_bound = 0.0# sorted_alignments.mean() - 1.97 * sorted_alignments.std()\n",
    "        mixed_param = (1 - alpha) * param0 + alpha * param1\n",
    "        conditional = cosine_alignments < outlier_bound\n",
    "        if conditional.sum() > 0:\n",
    "            print(f'pruning {conditional.sum()} components ouf of {param0.shape[1]} for {key}')\n",
    "            d[key.replace('.weight', '')] = conditional\n",
    "#             print(conditional)\n",
    "#             print(outlier_bound)\n",
    "#             print(cosine_alignments.sort())\n",
    "#             return\n",
    "        mixed_param[conditional] = param0[conditional]\n",
    "#         new_param = torch.where(cosine_alignments <= outlier_bound, param0.t(), mixed_param.t()).t()\n",
    "        sd_alpha[key] = mixed_param.cuda().reshape(*param_shape)\n",
    "        \n",
    "#     sd_alpha = {k: (1 - alpha) * sd0[k].cuda() + alpha * sd1[k].cuda()\n",
    "#                 for k in sd0.keys()}\n",
    "    net.load_state_dict(sd_alpha)\n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0de29d84",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "160ad0e0",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pruning 1 components ouf of 27 for features.0.weight\n",
      "pruning 6 components ouf of 576 for features.3.weight\n",
      "pruning 4 components ouf of 1152 for features.6.weight\n",
      "pruning 1 components ouf of 2304 for features.8.weight\n",
      "pruning 15 components ouf of 2304 for features.11.weight\n",
      "pruning 20 components ouf of 4608 for features.13.weight\n",
      "pruning 73 components ouf of 4608 for features.16.weight\n",
      "pruning 200 components ouf of 4608 for features.18.weight\n",
      "(α=0): 89.6% \t\t<-- Model A\n",
      "(α=1): 90.2% \t\t<-- Model B\n",
      "(α=0.5): 60.8% \t\t<-- Merged model\n"
     ]
    }
   ],
   "source": [
    "k0 = 'vgg11_v1'\n",
    "k1 = 'vgg11_v2_perm1'\n",
    "model0 = vgg11()\n",
    "model1 = vgg11()\n",
    "model_a = vgg11()\n",
    "mix_weights(model0, 0.0, k0, k1)\n",
    "mix_weights(model1, 1.0, k0, k1)\n",
    "\n",
    "alpha = 0.5\n",
    "key2cond = mix_weights_pruning(model_a, alpha, k0, k1)\n",
    "print('(α=0): %.1f%% \\t\\t<-- Model A' % (100*evaluate(model0)))\n",
    "print('(α=1): %.1f%% \\t\\t<-- Model B' % (100*evaluate(model1)))\n",
    "print('(α=0.5): %.1f%% \\t\\t<-- Merged model' % (100*evaluate(model_a)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "c0078974",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mix_weights_pruning_ranges(net, alpha, key0, key1):\n",
    "    d = {}\n",
    "    sd0 = torch.load(os.path.join('/srv/share4/gstoica3/checkpoints/REPAIR', '%s.pt' % key0))\n",
    "    sd1 = torch.load(os.path.join('/srv/share4/gstoica3/checkpoints/REPAIR', '%s.pt' % key1))\n",
    "    k2sd_alpha = {}\n",
    "    for k in tqdm(np.arange(-1., 1.05, .05)):\n",
    "        k2sd_alpha[k] = {}\n",
    "        \n",
    "        for key in sd0.keys():\n",
    "            if 'weight' not in key:\n",
    "                k2sd_alpha[k][key] = (1 - alpha) * sd0[key].cuda() + alpha * sd1[key].cuda()\n",
    "                continue\n",
    "            \n",
    "            param_shape = sd1[key].shape\n",
    "            param0 = sd0[key].flatten(1)\n",
    "            param1 = sd1[key].flatten(1)\n",
    "            \n",
    "            mixed_param = (1 - alpha) * param0 + alpha * param1\n",
    "            \n",
    "            cosine_num = torch.diagonal(param0 @ param1.T)\n",
    "            cosine_denom = (\n",
    "                torch.norm(param0, p=2,dim=-1) * torch.norm(param1, p=2,dim=-1)\n",
    "            )\n",
    "            cosine_alignments = (cosine_num / cosine_denom).reshape(-1)\n",
    "            sorted_alignments, sorted_args = cosine_alignments.sort()\n",
    "            conditional = cosine_alignments < k        \n",
    "            if conditional.sum() > 0:\n",
    "#                 print(f'pruning {conditional.sum()} components ouf of {param0.shape[1]} for {key}')\n",
    "                d[key.replace('.weight', '')] = conditional\n",
    "    \n",
    "            mixed_param[conditional] = param0[conditional]\n",
    "            k2sd_alpha[k][key] = mixed_param.cuda().reshape(*param_shape)\n",
    "#     net.load_state_dict(sd_alpha)\n",
    "    return k2sd_alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "f721caf1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████| 41/41 [00:00<00:00, 232.54it/s]\n"
     ]
    }
   ],
   "source": [
    "model_a = vgg11()\n",
    "k2sd_alpha = mix_weights_pruning_ranges(model_a, alpha, k0, k1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0650dd88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(α=-1.000): 58.160 \t\t<-- Merged model\n",
      "(α=-0.950): 58.160 \t\t<-- Merged model\n",
      "(α=-0.900): 58.160 \t\t<-- Merged model\n",
      "(α=-0.850): 58.160 \t\t<-- Merged model\n",
      "(α=-0.800): 58.160 \t\t<-- Merged model\n",
      "(α=-0.750): 58.160 \t\t<-- Merged model\n",
      "(α=-0.700): 58.160 \t\t<-- Merged model\n",
      "(α=-0.650): 58.160 \t\t<-- Merged model\n",
      "(α=-0.600): 58.160 \t\t<-- Merged model\n",
      "(α=-0.550): 58.160 \t\t<-- Merged model\n",
      "(α=-0.500): 58.160 \t\t<-- Merged model\n"
     ]
    }
   ],
   "source": [
    "for k, sd_alpha in k2sd_alpha.items():\n",
    "    k0 = 'vgg11_v1'\n",
    "    k1 = 'vgg11_v2_perm1'\n",
    "    model_a = vgg11()\n",
    "    mix_weights(model0, 0.0, k0, k1)\n",
    "    mix_weights(model1, 1.0, k0, k1)\n",
    "    model_a.load_state_dict(sd_alpha)\n",
    "    print(f'(α={k:.3f}): {100*evaluate(model_a):.3f} \\t\\t<-- Merged model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "921c192a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "215ed79b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pruning 5 components ouf of 1152 for features.6.weight\n",
      "tensor([False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False,  True, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False,  True, False, False, False, False,\n",
      "         True, False, False, False, False, False, False, False, False,  True,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False,  True, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False], device='cuda:0')\n",
      "tensor(1.0000, device='cuda:0')\n",
      "tensor([1.0000, 1.0000, 1.0000, 1.0000, 1.0000], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "k0 = 'vgg11_v1'\n",
    "k1 = 'vgg11_v2_perm1'\n",
    "model0 = vgg11()\n",
    "model1 = vgg11()\n",
    "model_a = vgg11()\n",
    "\n",
    "alpha = 0.5\n",
    "key2cond = mix_weights_pruning(model_a, alpha, k0, k1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "964307f3",
   "metadata": {},
   "source": [
    "### Procrustes Aligning "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "f4a302b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_procrustes(A, B):\n",
    "    R = B.T @ A\n",
    "    U, s, Vh = torch.linalg.svd(R)\n",
    "    return U @ Vh\n",
    "\n",
    "def mix_weights_procrustes(net, alpha, key0, key1):\n",
    "    d = {}\n",
    "    sd0 = torch.load(os.path.join('/srv/share4/gstoica3/checkpoints/REPAIR', '%s.pt' % key0))\n",
    "    sd1 = torch.load(os.path.join('/srv/share4/gstoica3/checkpoints/REPAIR', '%s.pt' % key1))\n",
    "    sd_alpha = {}\n",
    "    for key in sd0.keys():\n",
    "        if 'weight' not in key:\n",
    "            sd_alpha[key] = (1 - alpha) * sd0[key].cuda() + alpha * sd1[key].cuda()\n",
    "            continue\n",
    "        param_shape = sd1[key].shape\n",
    "        param0 = sd0[key].flatten(1)\n",
    "        param1 = sd1[key].flatten(1)\n",
    "        \n",
    "        cosine_num = torch.diagonal(param0 @ param1.T)\n",
    "        cosine_denom = (\n",
    "            torch.norm(param0, p=2,dim=-1) * torch.norm(param1, p=2,dim=-1)\n",
    "        )\n",
    "        cosine_alignments = (cosine_num / cosine_denom).reshape(-1)\n",
    "#         import pdb; pdb.set_trace()\n",
    "        sorted_alignments, sorted_args = cosine_alignments.sort()\n",
    "        outlier_bound = sorted_alignments.mean() - 1.97 * sorted_alignments.std()\n",
    "        mixed_param = (1 - alpha) * param0 + alpha * param1\n",
    "        conditional = cosine_alignments < outlier_bound\n",
    "        if conditional.sum() > 0:\n",
    "            print(f'pruning {conditional.sum()} components ouf of {param0.shape[1]} for {key}')\n",
    "            d[key.replace('.weight', '')] = conditional\n",
    "        \n",
    "        procrustes = compute_procrustes(param1[conditional == False], param0[conditional == False])\n",
    "        param1[conditional] = param1[conditional == False] @ procrustes\n",
    "        mixed_param[conditional] = ((1 - alpha) * param0 + alpha * param1)[conditional == False]\n",
    "        mixed_param[conditional] = param0[conditional == False]\n",
    "        \n",
    "#         new_param = torch.where(cosine_alignments <= outlier_bound, param0.t(), mixed_param.t()).t()\n",
    "        sd_alpha[key] = mixed_param.cuda().reshape(*param_shape)\n",
    "        \n",
    "#     sd_alpha = {k: (1 - alpha) * sd0[k].cuda() + alpha * sd1[k].cuda()\n",
    "#                 for k in sd0.keys()}\n",
    "    net.load_state_dict(sd_alpha)\n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "0f72e9c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pruning 2 components ouf of 27 for features.0.weight\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "shape mismatch: value tensor of shape [62, 27] cannot be broadcast to indexing result of shape [2, 27]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_87077/745359854.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0malpha\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.5\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mkey2cond\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmix_weights_procrustes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_a\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'(α=0): %.1f%% \\t\\t<-- Model A'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'(α=1): %.1f%% \\t\\t<-- Model B'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_87077/390782333.py\u001b[0m in \u001b[0;36mmix_weights_procrustes\u001b[0;34m(net, alpha, key0, key1)\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0mprocrustes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_procrustes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam1\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mconditional\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam0\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mconditional\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m         \u001b[0mparam1\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mconditional\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparam1\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mconditional\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m@\u001b[0m \u001b[0mprocrustes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m         \u001b[0mmixed_param\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mconditional\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mparam0\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0malpha\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mparam1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mconditional\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m         \u001b[0mmixed_param\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mconditional\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparam0\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mconditional\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: shape mismatch: value tensor of shape [62, 27] cannot be broadcast to indexing result of shape [2, 27]"
     ]
    }
   ],
   "source": [
    "k0 = 'vgg11_v1'\n",
    "k1 = 'vgg11_v2_perm1'\n",
    "model0 = vgg11()\n",
    "model1 = vgg11()\n",
    "model_a = vgg11()\n",
    "mix_weights(model0, 0.0, k0, k1)\n",
    "mix_weights(model1, 1.0, k0, k1)\n",
    "\n",
    "alpha = 0.5\n",
    "key2cond = mix_weights_procrustes(model_a, alpha, k0, k1)\n",
    "print('(α=0): %.1f%% \\t\\t<-- Model A' % (100*evaluate(model0)))\n",
    "print('(α=1): %.1f%% \\t\\t<-- Model B' % (100*evaluate(model1)))\n",
    "print('(α=0.5): %.1f%% \\t\\t<-- Merged model' % (100*evaluate(model_a)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e0c9d6c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "89396da3",
   "metadata": {},
   "source": [
    "# Correct neuronal statistics with REPAIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "4ff057c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrackLayer(nn.Module):\n",
    "    def __init__(self, layer):\n",
    "        super().__init__()\n",
    "        self.layer = layer\n",
    "        self.bn = nn.BatchNorm2d(layer.out_channels)\n",
    "        \n",
    "    def get_stats(self):\n",
    "        return (self.bn.running_mean, self.bn.running_var.sqrt())\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x1 = self.layer(x)\n",
    "        self.bn(x1)\n",
    "        return x1\n",
    "\n",
    "class ResetLayer(nn.Module):\n",
    "    def __init__(self, layer):\n",
    "        super().__init__()\n",
    "        self.layer = layer\n",
    "        self.bn = nn.BatchNorm2d(layer.out_channels)\n",
    "        \n",
    "    def set_stats(self, goal_mean, goal_std):\n",
    "        self.bn.bias.data = goal_mean\n",
    "        self.bn.weight.data = goal_std\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x1 = self.layer(x)\n",
    "        return self.bn(x1)\n",
    "\n",
    "# adds TrackLayers around every conv layer\n",
    "def make_tracked_net(net):\n",
    "    net1 = vgg11()\n",
    "    net1.load_state_dict(net.state_dict())\n",
    "    for i, layer in enumerate(net1.features):\n",
    "        if isinstance(layer, nn.Conv2d):\n",
    "            net1.features[i] = TrackLayer(layer)\n",
    "    return net1.cuda().eval()\n",
    "\n",
    "# adds ResetLayers around every conv layer\n",
    "def make_repaired_net(net):\n",
    "    net1 = vgg11()\n",
    "    net1.load_state_dict(net.state_dict())\n",
    "    for i, layer in enumerate(net1.features):\n",
    "        if isinstance(layer, nn.Conv2d):\n",
    "            net1.features[i] = ResetLayer(layer)\n",
    "    return net1.cuda().eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "c5dce4e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reset all tracked BN stats against training data\n",
    "def reset_bn_stats(model):\n",
    "    # resetting stats to baseline first as below is necessary for stability\n",
    "    for m in model.modules():\n",
    "        if type(m) == nn.BatchNorm2d:\n",
    "            m.momentum = None # use simple average\n",
    "            m.reset_running_stats()\n",
    "    model.train()\n",
    "    with torch.no_grad(), autocast():\n",
    "        for images, _ in train_aug_loader:\n",
    "            output = model(images.cuda())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "f4fce3fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Calculate all neuronal statistics in the endpoint networks\n",
    "wrap0 = make_tracked_net(model0)\n",
    "wrap1 = make_tracked_net(model1)\n",
    "reset_bn_stats(wrap0)\n",
    "reset_bn_stats(wrap1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "d467bdee",
   "metadata": {},
   "outputs": [],
   "source": [
    "wrap_a = make_repaired_net(model_a)\n",
    "# Iterate through corresponding triples of (TrackLayer, TrackLayer, ResetLayer)\n",
    "# around conv layers in (model0, model1, model_a).\n",
    "for (name0, track0), (name1, track1), (namea, reset_a) in zip(\n",
    "    wrap0.named_modules(), wrap1.named_modules(), wrap_a.named_modules()\n",
    "): \n",
    "    if not isinstance(track0, TrackLayer):\n",
    "        continue  \n",
    "    assert (isinstance(track0, TrackLayer)\n",
    "            and isinstance(track1, TrackLayer)\n",
    "            and isinstance(reset_a, ResetLayer))\n",
    "\n",
    "    # get neuronal statistics of original networks\n",
    "    mu0, std0 = track0.get_stats()\n",
    "    mu1, std1 = track1.get_stats()\n",
    "    # set the goal neuronal statistics for the merged network \n",
    "    goal_mean = (1 - alpha) * mu0 + alpha * mu1\n",
    "    goal_std = (1 - alpha) * std0 + alpha * std1\n",
    "    if name0 in key2cond:\n",
    "        cond = key2cond[name0]\n",
    "        goal_mean[cond] = mu0[cond]\n",
    "        goal_std[cond] = std0[cond]\n",
    "    reset_a.set_stats(goal_mean, goal_std)\n",
    "\n",
    "# Estimate mean/vars such that when added BNs are set to eval mode,\n",
    "# neuronal stats will be goal_mean and goal_std.\n",
    "reset_bn_stats(wrap_a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "db005b88",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([-1.1200e-01, -2.9725e-01, -3.3837e-01, -1.0683e-01,  1.1715e-01,\n",
       "         -8.1634e-02, -1.6073e-01,  1.3370e-03, -1.0036e-01, -3.2025e-01,\n",
       "          7.0103e-02, -1.7607e-01, -3.9152e-02, -2.9311e-01, -6.4997e-02,\n",
       "         -3.1805e-02, -3.6700e-01, -3.2028e-01,  3.4334e-01, -1.2581e-01,\n",
       "          6.9968e-04, -1.0635e-01, -5.6365e-02, -2.9894e-01, -2.8805e-01,\n",
       "         -1.2322e-02, -1.3870e-01, -2.2542e-01, -8.2708e-02, -3.2506e-01,\n",
       "         -5.4113e-01,  2.2096e-01, -8.4765e-02, -3.9836e-02, -7.0918e-02,\n",
       "          2.0940e-01, -3.2710e-01, -4.6151e-01, -1.2840e-01,  3.8923e-01,\n",
       "         -2.2121e-02, -4.0536e-01, -2.0058e-01, -1.4678e-01, -6.2533e-02,\n",
       "         -5.8112e-01, -2.8919e-01, -3.4973e-01,  1.6151e-01, -3.5382e-01,\n",
       "         -1.6087e-01, -2.5695e-01, -9.4491e-02, -4.5885e-01, -3.7647e-01,\n",
       "         -1.5788e-01, -1.8804e-01, -1.6771e-01, -3.2800e-01, -7.4287e-01,\n",
       "         -2.0859e-01, -4.2982e-02,  1.1481e-01, -1.4776e-01], device='cuda:0'),\n",
       " 'features.0')"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(wrap0.named_modules())[2][1].get_stats()[0], list(wrap0.named_modules())[2][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "78c3da76",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['features.0', 'features.3', 'features.6', 'features.8', 'features.11', 'features.13', 'features.16', 'features.18'])"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "key2cond.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3313ef52",
   "metadata": {},
   "source": [
    "### Fuse added BNs so that weights are compatible with original architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "693446dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fuse_conv_bn(conv, bn):\n",
    "    fused_conv = torch.nn.Conv2d(conv.in_channels,\n",
    "                                 conv.out_channels,\n",
    "                                 kernel_size=conv.kernel_size,\n",
    "                                 stride=conv.stride,\n",
    "                                 padding=conv.padding,\n",
    "                                 bias=True)\n",
    "\n",
    "    # set weights\n",
    "    w_conv = conv.weight.clone()\n",
    "    bn_std = (bn.eps + bn.running_var).sqrt()\n",
    "    gamma = bn.weight / bn_std\n",
    "    fused_conv.weight.data = (w_conv * gamma.reshape(-1, 1, 1, 1))\n",
    "\n",
    "    # set bias\n",
    "    beta = bn.bias + gamma * (-bn.running_mean + conv.bias)\n",
    "    fused_conv.bias.data = beta\n",
    "    \n",
    "    return fused_conv\n",
    "\n",
    "def fuse_tracked_net(net):\n",
    "    net1 = vgg11()\n",
    "    for i, rlayer in enumerate(net.features):\n",
    "        if isinstance(rlayer, ResetLayer):\n",
    "            fused_conv = fuse_conv_bn(rlayer.layer, rlayer.bn)\n",
    "            net1.features[i].load_state_dict(fused_conv.state_dict())\n",
    "    net1.classifier.load_state_dict(net.classifier.state_dict())\n",
    "    return net1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "d5d81c46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fuse the rescaling+shift coefficients back into conv layers\n",
    "model_b = fuse_tracked_net(wrap_a)\n",
    "assert abs(evaluate(model_b) - evaluate(wrap_a)) < 0.005"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "2d899359",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(α=0): 89.6% \t\t<-- Model A\n",
      "(α=1): 90.2% \t\t<-- Model B\n",
      "(α=0.5): 83.0% \t\t<-- Merged model with REPAIR\n"
     ]
    }
   ],
   "source": [
    "print('(α=0): %.1f%% \\t\\t<-- Model A' % (100*evaluate(model0)))\n",
    "print('(α=1): %.1f%% \\t\\t<-- Model B' % (100*evaluate(model1)))\n",
    "print('(α=0.5): %.1f%% \\t\\t<-- Merged model with REPAIR' % (100*evaluate(model_b)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e531656",
   "metadata": {},
   "source": [
    "### Reweighting before pruning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "74f25e60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(α=0): 89.6% \t\t<-- Model A\n",
      "(α=1): 90.2% \t\t<-- Model B\n",
      "(α=0.5): 58.2% \t\t<-- Merged model\n"
     ]
    }
   ],
   "source": [
    "k0 = 'vgg11_v1'\n",
    "k1 = 'vgg11_v2_perm1'\n",
    "model0 = vgg11()\n",
    "model1 = vgg11()\n",
    "model_a = vgg11()\n",
    "mix_weights(model0, 0.0, k0, k1)\n",
    "mix_weights(model1, 1.0, k0, k1)\n",
    "\n",
    "alpha = 0.5\n",
    "mix_weights(model_a, alpha, k0, k1)\n",
    "print('(α=0): %.1f%% \\t\\t<-- Model A' % (100*evaluate(model0)))\n",
    "print('(α=1): %.1f%% \\t\\t<-- Model B' % (100*evaluate(model1)))\n",
    "print('(α=0.5): %.1f%% \\t\\t<-- Merged model' % (100*evaluate(model_a)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "a06c9367",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Calculate all neuronal statistics in the endpoint networks\n",
    "wrap0 = make_tracked_net(model0)\n",
    "wrap1 = make_tracked_net(model1)\n",
    "reset_bn_stats(wrap0)\n",
    "reset_bn_stats(wrap1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "d56a921e",
   "metadata": {},
   "outputs": [],
   "source": [
    "wrap_a = make_repaired_net(model_a)\n",
    "# Iterate through corresponding triples of (TrackLayer, TrackLayer, ResetLayer)\n",
    "# around conv layers in (model0, model1, model_a).\n",
    "for (name0, track0), (name1, track1), (namea, reset_a) in zip(\n",
    "    wrap0.named_modules(), wrap1.named_modules(), wrap_a.named_modules()\n",
    "): \n",
    "    if not isinstance(track0, TrackLayer):\n",
    "        continue  \n",
    "    assert (isinstance(track0, TrackLayer)\n",
    "            and isinstance(track1, TrackLayer)\n",
    "            and isinstance(reset_a, ResetLayer))\n",
    "\n",
    "    # get neuronal statistics of original networks\n",
    "    mu0, std0 = track0.get_stats()\n",
    "    mu1, std1 = track1.get_stats()\n",
    "    # set the goal neuronal statistics for the merged network \n",
    "    goal_mean = (1 - alpha) * mu0 + alpha * mu1\n",
    "    goal_std = (1 - alpha) * std0 + alpha * std1\n",
    "    reset_a.set_stats(goal_mean, goal_std)\n",
    "\n",
    "# Estimate mean/vars such that when added BNs are set to eval mode,\n",
    "# neuronal stats will be goal_mean and goal_std.\n",
    "reset_bn_stats(wrap_a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "a71f5e05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fuse the rescaling+shift coefficients back into conv layers\n",
    "model_b = fuse_tracked_net(wrap_a)\n",
    "assert abs(evaluate(model_b) - evaluate(wrap_a)) < 0.005"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "b35aafb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(α=0): 89.6% \t\t<-- Model A\n",
      "(α=1): 90.2% \t\t<-- Model B\n",
      "(α=0.5): 83.8% \t\t<-- Merged model with REPAIR\n"
     ]
    }
   ],
   "source": [
    "print('(α=0): %.1f%% \\t\\t<-- Model A' % (100*evaluate(model0)))\n",
    "print('(α=1): %.1f%% \\t\\t<-- Model B' % (100*evaluate(model1)))\n",
    "print('(α=0.5): %.1f%% \\t\\t<-- Merged model with REPAIR' % (100*evaluate(model_b)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ba19ec0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
