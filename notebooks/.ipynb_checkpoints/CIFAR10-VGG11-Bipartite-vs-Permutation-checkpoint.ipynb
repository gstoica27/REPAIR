{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eb5f2243",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/srv/share/gstoica3/miniconda3/envs/open-mmlab/lib/python3.7/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import scipy.optimize\n",
    "import os\n",
    "import pdb\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from torch.optim import SGD, lr_scheduler\n",
    "import torchvision\n",
    "import torchvision.transforms as T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7f84d11",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3ae74142",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(model, i):\n",
    "    sd = model.state_dict()\n",
    "    torch.save(model.state_dict(), os.path.join('/srv/share4/gstoica3/checkpoints/REPAIR/', '%s.pt' % i))\n",
    "\n",
    "def load_model(model, i):\n",
    "    sd = torch.load(os.path.join('/srv/share4/gstoica3/checkpoints/REPAIR', '%s.pt' % i))\n",
    "    model.load_state_dict(sd)\n",
    "    \n",
    "def evaluate(model):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    with torch.no_grad(), autocast():\n",
    "        for inputs, labels in test_loader:\n",
    "            outputs = model(inputs.cuda())\n",
    "            pred = outputs.argmax(dim=1)\n",
    "            correct += (labels.cuda() == pred).sum().item()\n",
    "    return correct / len(test_loader.dataset)\n",
    "\n",
    "# Given two networks net0, net1 which each output a feature map of shape NxCxWxH,\n",
    "# this will reshape both outputs to (N*W*H)xC\n",
    "# and then compute a CxC correlation matrix between the two\n",
    "def run_corr_matrix(net0, net1):\n",
    "    n = len(train_aug_loader)\n",
    "    with torch.no_grad():\n",
    "        net0.eval()\n",
    "        net1.eval()\n",
    "        for i, (images, _) in enumerate(tqdm(train_aug_loader)):\n",
    "            \n",
    "            img_t = images.float().cuda()\n",
    "            out0 = net0(img_t).double()\n",
    "            out0 = out0.permute(0, 2, 3, 1).reshape(-1, out0.shape[1])\n",
    "            out1 = net1(img_t).double()\n",
    "            out1 = out1.permute(0, 2, 3, 1).reshape(-1, out1.shape[1])\n",
    "\n",
    "            # save batchwise first+second moments and outer product\n",
    "            mean0_b = out0.mean(dim=0)\n",
    "            mean1_b = out1.mean(dim=0)\n",
    "            sqmean0_b = out0.square().mean(dim=0)\n",
    "            sqmean1_b = out1.square().mean(dim=0)\n",
    "            outer_b = (out0.T @ out1) / out0.shape[0]\n",
    "            if i == 0:\n",
    "                mean0 = torch.zeros_like(mean0_b)\n",
    "                mean1 = torch.zeros_like(mean1_b)\n",
    "                sqmean0 = torch.zeros_like(sqmean0_b)\n",
    "                sqmean1 = torch.zeros_like(sqmean1_b)\n",
    "                outer = torch.zeros_like(outer_b)\n",
    "            mean0 += mean0_b / n\n",
    "            mean1 += mean1_b / n\n",
    "            sqmean0 += sqmean0_b / n\n",
    "            sqmean1 += sqmean1_b / n\n",
    "            outer += outer_b / n\n",
    "\n",
    "    cov = outer - torch.outer(mean0, mean1)\n",
    "    std0 = (sqmean0 - mean0**2).sqrt()\n",
    "    std1 = (sqmean1 - mean1**2).sqrt()\n",
    "    outer = (torch.outer(std0, std1) + 1e-4)\n",
    "    outer[outer.isnan()] = 1e-4\n",
    "    corr = cov / outer\n",
    "    assert corr.isnan().sum() == 0, 'corr has nans...'\n",
    "    return corr\n",
    "\n",
    "def get_layer_perm1(corr_mtx):\n",
    "    corr_mtx_a = corr_mtx.cpu().numpy()\n",
    "    row_ind, col_ind = scipy.optimize.linear_sum_assignment(corr_mtx_a, maximize=True)\n",
    "    assert (row_ind == np.arange(len(corr_mtx_a))).all()\n",
    "    perm_map = torch.tensor(col_ind).long()\n",
    "    perm_map = torch.eye(corr_mtx.shape[0], device=corr_mtx.device)[perm_map]\n",
    "    return perm_map\n",
    "\n",
    "def get_bipartite_perm(corr, threshold=-torch.inf):\n",
    "    scores, idx = corr.max(0)\n",
    "    valid_elements = scores >= threshold\n",
    "    idx = torch.where(valid_elements, idx, corr.shape[0])\n",
    "    location_lookup = torch.eye(corr.shape[0]+1, corr.shape[0], device=corr.device)\n",
    "    matches = location_lookup[idx]\n",
    "    totals = matches.sum(0, keepdim=True)\n",
    "    matches = matches / (totals + 1)\n",
    "    return matches.t(), totals\n",
    "\n",
    "# returns the channel-permutation to make layer1's activations most closely\n",
    "# match layer0's.\n",
    "def get_layer_perm(net0, net1):\n",
    "    corr_mtx = run_corr_matrix(net0, net1)\n",
    "    return get_layer_perm1(corr_mtx)\n",
    "\n",
    "def get_layer_bipartite_transform(net0, net1, threshold=-torch.inf):\n",
    "    corr_mtx = run_corr_matrix(net0, net1)\n",
    "    return get_bipartite_perm(corr_mtx, threshold=threshold)\n",
    "\n",
    "# modifies the weight matrices of a convolution and batchnorm\n",
    "# layer given a permutation of the output channels\n",
    "def permute_output(perm_map, layer):\n",
    "    pre_weights = [layer.weight,\n",
    "                   layer.bias]\n",
    "    for w in pre_weights:\n",
    "        if len(w.shape) == 4:\n",
    "            transform = torch.einsum('ab,bcde->acde', perm_map, w)\n",
    "        elif len(w.shape) == 2:\n",
    "            transform = perm_map @ w\n",
    "        else:\n",
    "            transform = w @ perm_map.t()\n",
    "#         assert torch.allclose(w[perm_map.argmax(-1)], transform)\n",
    "        w.data = transform\n",
    "\n",
    "# modifies the weight matrix of a layer for a given permutation of the input channels\n",
    "# works for both conv2d and linear\n",
    "def permute_input(perm_map, layer):\n",
    "    w = layer.weight\n",
    "    if len(w.shape) == 4:\n",
    "        transform = torch.einsum('abcd,be->aecd', w, perm_map.t())\n",
    "    elif len(w.shape) == 2:\n",
    "        transform = w @ perm_map.t()\n",
    "#     assert torch.allclose(w[:, perm_map.argmax(-1)], transform)\n",
    "    w.data = transform\n",
    "\n",
    "def subnet(model, n_layers):\n",
    "    return model.features[:n_layers]\n",
    "\n",
    "def permute_model(source, target):\n",
    "    source_feats = source.features\n",
    "    target_feats = target.features\n",
    "    n = len(source_feats)\n",
    "    for i in range(n):\n",
    "        if not isinstance(target_feats[i], nn.Conv2d): continue\n",
    "        assert isinstance(target_feats[i+1], nn.ReLU)\n",
    "        perm_map = get_layer_perm(\n",
    "            subnet(source, i+2), subnet(target, i+2)\n",
    "        )\n",
    "        permute_output(perm_map, target_feats[i])\n",
    "        \n",
    "        next_layer = None\n",
    "        for j in range(i+1, n):\n",
    "            if isinstance(target_feats[j], nn.Conv2d):\n",
    "                next_layer = target_feats[j]\n",
    "                break\n",
    "        if next_layer is None:\n",
    "            next_layer = target.classifier\n",
    "        permute_input(perm_map, next_layer)\n",
    "\n",
    "def strip_param_suffix(name):\n",
    "    return name.replace('.weight', '').replace('.bias', '')\n",
    "\n",
    "def get_empty_module_dict(net):\n",
    "    module2Dict = dict()\n",
    "    module_list = []\n",
    "    for key in net.state_dict().keys():\n",
    "        base_name = strip_param_suffix(key)\n",
    "        module2Dict[base_name] = dict()\n",
    "        if base_name not in module_list:\n",
    "            module_list += [base_name]\n",
    "    return module2Dict, module_list\n",
    "\n",
    "def apply_bipartite_transform(source, target, threshold=-torch.inf):\n",
    "    source_feats = source.features\n",
    "    target_feats = target.features\n",
    "    module2Dict, module_list = get_empty_module_dict(target)\n",
    "    k = 0\n",
    "    n = len(source_feats)\n",
    "    for i in range(n):\n",
    "        if not isinstance(target_feats[i], nn.Conv2d): continue\n",
    "        assert isinstance(target_feats[i+1], nn.ReLU)\n",
    "        bipartite_map, layer_totals = get_layer_bipartite_transform(\n",
    "            subnet(source, i+2), subnet(target, i+2), threshold=threshold\n",
    "        )\n",
    "        permute_output(bipartite_map, target_feats[i])\n",
    "        module2Dict[module_list[k]]['output'] = layer_totals\n",
    "#         pdb.set_trace()\n",
    "        next_layer = None\n",
    "        for j in range(i+1, n):\n",
    "            if isinstance(target_feats[j], nn.Conv2d):\n",
    "                next_layer = target_feats[j]\n",
    "                break\n",
    "        if next_layer is None:\n",
    "            next_layer = target.classifier\n",
    "        permute_input(bipartite_map, next_layer)\n",
    "        module2Dict[module_list[k+1]]['input'] = layer_totals\n",
    "        k += 1\n",
    "    return module2Dict\n",
    "\n",
    "def combine_io_masks(io, param):\n",
    "    mask = torch.zeros_like(param, device=param.device)\n",
    "    try:\n",
    "        if 'output' in io:\n",
    "            mask[io['output'].view(-1) == 0] = 1.\n",
    "        if 'input' in io and len(mask.shape) > 1:\n",
    "            mask[:, io['input'].view(-1) == 0] = 1.\n",
    "    except:\n",
    "        pdb.set_trace()\n",
    "    return mask\n",
    "\n",
    "def mix_weights(net, alpha, key0, key1, module2io=None):\n",
    "    sd0 = torch.load(\n",
    "        os.path.join(\n",
    "            '/srv/share4/gstoica3/checkpoints/REPAIR', \n",
    "            '%s.pt' % key0\n",
    "        )\n",
    "    )\n",
    "    sd1 = torch.load(\n",
    "        os.path.join(\n",
    "            '/srv/share4/gstoica3/checkpoints/REPAIR', \n",
    "            '%s.pt' % key1\n",
    "        )\n",
    "    )\n",
    "    sd_alpha = {}\n",
    "    for k in sd0.keys():\n",
    "        param0 = sd0[k].cuda()\n",
    "        param1 = sd1[k].cuda()\n",
    "        sd_alpha[k] = (1 - alpha) * param0 + alpha * param1\n",
    "        if module2io is not None:\n",
    "            param_base = strip_param_suffix(k)\n",
    "            mask = combine_io_masks(module2io[param_base], param1)\n",
    "            sd_alpha[k][mask == 1] = param0[mask == 1]\n",
    "#     sd_alpha = {k: (1 - alpha) * sd0[k].cuda() + alpha * sd1[k].cuda()\n",
    "#                 for k in sd0.keys()}\n",
    "    net.load_state_dict(sd_alpha)\n",
    "    \n",
    "def mix_weights_over_alphas(num_times, model_a, k0, k1, module2io=None):\n",
    "    step = 1. / num_times\n",
    "    alphas = np.arange(0., 1. + step, step)\n",
    "    accuracies = []\n",
    "    for alpha in tqdm(alphas):\n",
    "        mix_weights(model_a, alpha, k0, k1, module2io=module2io)\n",
    "        accuracies.append(evaluate(model_a))\n",
    "    return alphas, accuracies\n",
    "\n",
    "def mix_weights_and_repair_over_alphas(num_times, model_a, wrap0, wrap1, k0, k1, module2io=None):\n",
    "    step = 1. / num_times\n",
    "    alphas = np.arange(0., 1. + step, step)\n",
    "    accuracies = []\n",
    "    for alpha in tqdm(alphas):\n",
    "        mix_weights(model_a, alpha, k0, k1, module2io=module2io)\n",
    "        wrap_a = make_repaired_net(model_a)\n",
    "        fuse_computed_batchnorms(wrap0, wrap1, wrap_a)\n",
    "        reset_bn_stats(wrap_a)\n",
    "        model_b = fuse_tracked_net(wrap_a)\n",
    "        accuracies.append(evaluate(model_b))\n",
    "    return alphas, accuracies\n",
    "\n",
    "class TrackLayer(nn.Module):\n",
    "    def __init__(self, layer):\n",
    "        super().__init__()\n",
    "        self.layer = layer\n",
    "        self.bn = nn.BatchNorm2d(layer.out_channels)\n",
    "        \n",
    "    def get_stats(self):\n",
    "        return (self.bn.running_mean, self.bn.running_var.sqrt())\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x1 = self.layer(x)\n",
    "        self.bn(x1)\n",
    "        return x1\n",
    "\n",
    "class ResetLayer(nn.Module):\n",
    "    def __init__(self, layer):\n",
    "        super().__init__()\n",
    "        self.layer = layer\n",
    "        self.bn = nn.BatchNorm2d(layer.out_channels)\n",
    "        \n",
    "    def set_stats(self, goal_mean, goal_std):\n",
    "        self.bn.bias.data = goal_mean\n",
    "        self.bn.weight.data = goal_std\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x1 = self.layer(x)\n",
    "        return self.bn(x1)\n",
    "\n",
    "# adds TrackLayers around every conv layer\n",
    "def make_tracked_net(net):\n",
    "    net1 = vgg11()\n",
    "    net1.load_state_dict(net.state_dict())\n",
    "    for i, layer in enumerate(net1.features):\n",
    "        if isinstance(layer, nn.Conv2d):\n",
    "            net1.features[i] = TrackLayer(layer)\n",
    "    return net1.cuda().eval()\n",
    "\n",
    "# adds ResetLayers around every conv layer\n",
    "def make_repaired_net(net):\n",
    "    net1 = vgg11()\n",
    "    net1.load_state_dict(net.state_dict())\n",
    "    for i, layer in enumerate(net1.features):\n",
    "        if isinstance(layer, nn.Conv2d):\n",
    "            net1.features[i] = ResetLayer(layer)\n",
    "    return net1.cuda().eval()\n",
    "\n",
    "# reset all tracked BN stats against training data\n",
    "def reset_bn_stats(model):\n",
    "    # resetting stats to baseline first as below is necessary for stability\n",
    "    for m in model.modules():\n",
    "        if type(m) == nn.BatchNorm2d:\n",
    "            m.momentum = None # use simple average\n",
    "            m.reset_running_stats()\n",
    "    model.train()\n",
    "    with torch.no_grad(), autocast():\n",
    "        for images, _ in train_aug_loader:\n",
    "            output = model(images.cuda())\n",
    "            \n",
    "def fuse_conv_bn(conv, bn):\n",
    "    fused_conv = torch.nn.Conv2d(conv.in_channels,\n",
    "                                 conv.out_channels,\n",
    "                                 kernel_size=conv.kernel_size,\n",
    "                                 stride=conv.stride,\n",
    "                                 padding=conv.padding,\n",
    "                                 bias=True)\n",
    "\n",
    "    # set weights\n",
    "    w_conv = conv.weight.clone()\n",
    "    bn_std = (bn.eps + bn.running_var).sqrt()\n",
    "    gamma = bn.weight / bn_std\n",
    "    fused_conv.weight.data = (w_conv * gamma.reshape(-1, 1, 1, 1))\n",
    "\n",
    "    # set bias\n",
    "    beta = bn.bias + gamma * (-bn.running_mean + conv.bias)\n",
    "    fused_conv.bias.data = beta\n",
    "    \n",
    "    return fused_conv\n",
    "\n",
    "def fuse_tracked_net(net):\n",
    "    net1 = vgg11()\n",
    "    for i, rlayer in enumerate(net.features):\n",
    "        if isinstance(rlayer, ResetLayer):\n",
    "            fused_conv = fuse_conv_bn(rlayer.layer, rlayer.bn)\n",
    "            net1.features[i].load_state_dict(fused_conv.state_dict())\n",
    "    net1.classifier.load_state_dict(net.classifier.state_dict())\n",
    "    return net1\n",
    "\n",
    "def fuse_computed_batchnorms(wrap0, wrap1, wrap_a, alpha=0.5, module2io=None):\n",
    "    # Iterate through corresponding triples of (TrackLayer, TrackLayer, ResetLayer)\n",
    "    # around conv layers in (model0, model1, model_a).\n",
    "    if module2io is not None:\n",
    "        modules = list(module2io.keys())\n",
    "        idx = 0\n",
    "    for track0, track1, reset_a in zip(wrap0.modules(), wrap1.modules(), wrap_a.modules()): \n",
    "        if not isinstance(track0, TrackLayer):\n",
    "            continue  \n",
    "        assert (isinstance(track0, TrackLayer)\n",
    "                and isinstance(track1, TrackLayer)\n",
    "                and isinstance(reset_a, ResetLayer))\n",
    "\n",
    "        # get neuronal statistics of original networks\n",
    "        mu0, std0 = track0.get_stats()\n",
    "        mu1, std1 = track1.get_stats()\n",
    "        # set the goal neuronal statistics for the merged network \n",
    "        goal_mean = (1 - alpha) * mu0 + alpha * mu1\n",
    "        goal_std = (1 - alpha) * std0 + alpha * std1\n",
    "        if module2io is not None:\n",
    "            io = module2io[modules[idx]]\n",
    "            mask = io['output'].view(-1) == 0.\n",
    "            goal_mean[mask == 1] = mu0[mask == 1]\n",
    "            goal_std[mask == 1] = std0[mask == 1]\n",
    "            idx += 1\n",
    "        reset_a.set_stats(goal_mean, goal_std)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1641cab4",
   "metadata": {},
   "source": [
    "# Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b52dc2dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "CIFAR_MEAN = [125.307, 122.961, 113.8575]\n",
    "CIFAR_STD = [51.5865, 50.847, 51.255]\n",
    "normalize = T.Normalize(np.array(CIFAR_MEAN)/255, np.array(CIFAR_STD)/255)\n",
    "denormalize = T.Normalize(-np.array(CIFAR_MEAN)/np.array(CIFAR_STD), 255/np.array(CIFAR_STD))\n",
    "\n",
    "train_transform = T.Compose([\n",
    "    T.RandomHorizontalFlip(),\n",
    "    T.RandomCrop(32, padding=4),\n",
    "    T.ToTensor(),\n",
    "    normalize,\n",
    "])\n",
    "test_transform = T.Compose([\n",
    "    T.ToTensor(),\n",
    "    normalize,\n",
    "])\n",
    "train_dset = torchvision.datasets.CIFAR10(root='/tmp', train=True,\n",
    "                                        download=True, transform=train_transform)\n",
    "test_dset = torchvision.datasets.CIFAR10(root='/tmp', train=False,\n",
    "                                        download=True, transform=test_transform)\n",
    "\n",
    "train_aug_loader = torch.utils.data.DataLoader(train_dset, batch_size=500, shuffle=True, num_workers=0)\n",
    "test_loader = torch.utils.data.DataLoader(test_dset, batch_size=500, shuffle=False, num_workers=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a47c75d7",
   "metadata": {},
   "source": [
    "# Initialize Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "625c5548",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/kuangliu/pytorch-cifar/blob/master/models/vgg.py\n",
    "cfg = {\n",
    "    'VGG11': [64, 'M', 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512, 'M'],\n",
    "    'VGG13': [64, 64, 'M', 128, 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512, 'M'],\n",
    "    'VGG16': [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 'M', 512, 512, 512, 'M', 512, 512, 512, 'M'],\n",
    "    'VGG19': [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 256, 'M', 512, 512, 512, 512, 'M', 512, 512, 512, 512, 'M'],\n",
    "}\n",
    "\n",
    "class VGG(nn.Module):\n",
    "    def __init__(self, vgg_name, w=1):\n",
    "        super(VGG, self).__init__()\n",
    "        self.w = w\n",
    "        self.features = self._make_layers(cfg[vgg_name])\n",
    "        self.classifier = nn.Linear(self.w*512, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.features(x)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.classifier(out)\n",
    "        return out\n",
    "\n",
    "    def _make_layers(self, cfg):\n",
    "        layers = []\n",
    "        in_channels = 3\n",
    "        for x in cfg:\n",
    "            if x == 'M':\n",
    "                layers += [nn.MaxPool2d(kernel_size=2, stride=2)]\n",
    "            else:\n",
    "                layers.append(nn.Conv2d(in_channels if in_channels == 3 else self.w*in_channels,\n",
    "                                     self.w*x, kernel_size=3, padding=1))\n",
    "                layers.append(nn.ReLU(inplace=True))\n",
    "                in_channels = x\n",
    "        layers += [nn.AvgPool2d(kernel_size=1, stride=1)]\n",
    "        return nn.Sequential(*layers)\n",
    "    \n",
    "def vgg11(w=1):\n",
    "    return VGG('VGG11', w).cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dfdabde",
   "metadata": {},
   "source": [
    "# Specify Variants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "eabc8b1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "m0 = '1'\n",
    "m1 = '4'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "47358f37",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.8982, 0.8931)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model0 = vgg11()\n",
    "model1 = vgg11()\n",
    "load_model(model0, f'vgg11_v{m0}')\n",
    "load_model(model1, f'vgg11_v{m1}')\n",
    "\n",
    "evaluate(model0), evaluate(model1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fbbf876",
   "metadata": {},
   "source": [
    "# Apply Permutation with REPAIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "cfa385f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████| 100/100 [00:16<00:00,  6.21it/s]\n",
      "100%|███████████████████████████████████████████████| 100/100 [00:15<00:00,  6.46it/s]\n",
      "100%|███████████████████████████████████████████████| 100/100 [00:15<00:00,  6.28it/s]\n",
      "100%|███████████████████████████████████████████████| 100/100 [00:15<00:00,  6.50it/s]\n",
      "100%|███████████████████████████████████████████████| 100/100 [00:15<00:00,  6.36it/s]\n",
      "100%|███████████████████████████████████████████████| 100/100 [00:14<00:00,  6.90it/s]\n",
      "100%|███████████████████████████████████████████████| 100/100 [00:16<00:00,  6.12it/s]\n",
      "100%|███████████████████████████████████████████████| 100/100 [00:16<00:00,  6.18it/s]\n"
     ]
    }
   ],
   "source": [
    "permute_model(model0, model1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "77a7425f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8931\n"
     ]
    }
   ],
   "source": [
    "print(evaluate(model1))\n",
    "save_model(model1, f'vgg11_v{m1}_perm1')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2f3caba",
   "metadata": {},
   "source": [
    "### Apply Only Permutation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "b94df16a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(α=0): 89.8% \t\t<-- Model A\n",
      "(α=1): 89.3% \t\t<-- Model B\n",
      "(α=0.5): 62.6% \t\t<-- Merged model\n"
     ]
    }
   ],
   "source": [
    "k0 = f'vgg11_v{m0}'\n",
    "k1 = f'vgg11_v{m1}_perm1'\n",
    "model0 = vgg11()\n",
    "model1 = vgg11()\n",
    "model_a = vgg11()\n",
    "mix_weights(model0, 0.0, k0, k1)\n",
    "mix_weights(model1, 1.0, k0, k1)\n",
    "\n",
    "alpha = 0.5\n",
    "mix_weights(model_a, alpha, k0, k1)\n",
    "print('(α=0): %.1f%% \\t\\t<-- Model A' % (100*evaluate(model0)))\n",
    "print('(α=1): %.1f%% \\t\\t<-- Model B' % (100*evaluate(model1)))\n",
    "print('(α=0.5): %.1f%% \\t\\t<-- Merged model' % (100*evaluate(model_a)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "67c77524",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████| 11/11 [00:26<00:00,  2.38s/it]\n"
     ]
    }
   ],
   "source": [
    "alphas, permute_accuracies = mix_weights_over_alphas(10, model_a, k0, k1)\n",
    "alpha = 0.5\n",
    "mix_weights(model_a, alpha, k0, k1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4f5b143",
   "metadata": {},
   "source": [
    "### Now Apply REPAIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "5c4a19c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Calculate all neuronal statistics in the endpoint networks\n",
    "wrap0 = make_tracked_net(model0)\n",
    "wrap1 = make_tracked_net(model1)\n",
    "reset_bn_stats(wrap0)\n",
    "reset_bn_stats(wrap1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "b2977b85",
   "metadata": {},
   "outputs": [],
   "source": [
    "wrap0 = make_tracked_net(model0)\n",
    "perm_wrap1 = make_tracked_net(model1)\n",
    "reset_bn_stats(wrap0)\n",
    "reset_bn_stats(perm_wrap1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "baa538d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████| 11/11 [03:46<00:00, 20.59s/it]\n"
     ]
    }
   ],
   "source": [
    "alphas, permute_repairs = mix_weights_and_repair_over_alphas(10, model_a, wrap0, wrap1, k0, k1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "d3ab9f9d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.8689,\n",
       " 0.8694,\n",
       " 0.8614,\n",
       " 0.8518,\n",
       " 0.8372,\n",
       " 0.8335,\n",
       " 0.8434,\n",
       " 0.8603,\n",
       " 0.8732,\n",
       " 0.8812,\n",
       " 0.881]"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "permute_repairs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac46c9e3",
   "metadata": {},
   "source": [
    "# Apply Bipartite with REPAIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "2650aabe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.8982, 0.8931)"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model0 = vgg11()\n",
    "model1 = vgg11()\n",
    "load_model(model0, f'vgg11_v{m0}')\n",
    "load_model(model1, f'vgg11_v{m1}')\n",
    "\n",
    "evaluate(model0), evaluate(model1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88c66434",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████| 100/100 [00:14<00:00,  6.68it/s]\n",
      "100%|███████████████████████████████████████████████| 100/100 [00:15<00:00,  6.64it/s]\n",
      "100%|███████████████████████████████████████████████| 100/100 [00:15<00:00,  6.37it/s]\n",
      " 52%|████████████████████████▉                       | 52/100 [00:08<00:08,  5.92it/s]"
     ]
    }
   ],
   "source": [
    "module2io = apply_bipartite_transform(model0, model1, threshold=-torch.inf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5efa6bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(evaluate(model1))\n",
    "save_model(model1, f'vgg11_v{m1}_bipartite')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6608cacd",
   "metadata": {},
   "outputs": [],
   "source": [
    "k0 = f'vgg11_v{m0}'\n",
    "k1 = f'vgg11_v{m1}_bipartite'\n",
    "model0 = vgg11()\n",
    "model1 = vgg11()\n",
    "model_a = vgg11()\n",
    "mix_weights(model0, 0.0, k0, k1)\n",
    "mix_weights(model1, 1.0, k0, k1)\n",
    "\n",
    "alpha = 0.5\n",
    "mix_weights(model_a, alpha, k0, k1, module2io=module2io)\n",
    "print('(α=0): %.1f%% \\t\\t<-- Model A' % (100*evaluate(model0)))\n",
    "print('(α=1): %.1f%% \\t\\t<-- Model B' % (100*evaluate(model1)))\n",
    "print('(α=0.5): %.1f%% \\t\\t<-- Merged model' % (100*evaluate(model_a)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42b2acfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "alphas, bipartite_accuracies = mix_weights_over_alphas(10, model_a, k0, k1, module2io)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d1ec9a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Calculate all neuronal statistics in the endpoint networks\n",
    "wrap0 = make_tracked_net(model0)\n",
    "wrap1 = make_tracked_net(model1)\n",
    "reset_bn_stats(wrap0)\n",
    "reset_bn_stats(wrap1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a17b55c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "alphas, bipartite_repairs = mix_weights_and_repair_over_alphas(\n",
    "    10, model_a, wrap0, wrap1, k0, k1, module2io=module2io\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "228f66ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "bipartite_repairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "213ae8b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = plt.gca()\n",
    "plt.axhline(.8984, label='Best Baseline Acc', color='grey')\n",
    "plt.plot(alphas, permute_accuracies, label='Permute')\n",
    "plt.plot(alphas, permute_repairs, label='Permute & REPAIR')\n",
    "plt.plot(alphas, bipartite_accuracies, label='Bipartite')\n",
    "plt.plot(alphas, bipartite_repairs, label='Bipartite & REPAIR')\n",
    "plt.legend()\n",
    "ax.set_ylim([.88, .91])\n",
    "plt.title(f'CIFAR10 & VGG11: Aligning {m1} --> {m0}')\n",
    "plt.savefig(os.path.join(os.getcwd(), f'cifar10_vgg11_{m1}to{m0}'))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88344dff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dfdf03b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
